paper_id,venue,authors,year,title,index_keys,author_keys,abstract
80B7396A,International Conference on Data Engineering,amit chandel + sunita sarawagi + pradeep nagesh,2006,Efficient Batch Top-k Search for Dictionary-based Entity Recognition,databases + data mining + knowledge based systems + graphical models + viterbi algorithm + NonControlled Keywords Not Found + warehousing + lower bound + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"We consider the problem of speeding up Entity Recognition systems that exploit existing large databases of structured entities to improve extraction accuracy. These systems require the computation of the maximum similarity scores of several overlapping segments of the input text with the entity database. We formulate a Batch-Top-K problem with the goal of sharing computations across overlapping segments. Our proposed algorithm performs a factor of three faster than independent Top-K queries and only a factor of two slower than an unachievable lower bound on total cost. We then propose a novel modification of the popular Viterbi algorithm for recognizing entities so as to work with easily computable bounds on match scores, thereby reducing the total inference time by a factor of eight compared to stateof- the-art methods."
7E09E1F3,International Conference on Data Engineering,eugene inseok chong + souripriya das + jagannathan srinivasan + george eadon,2006,Supporting Keyword Columns with Ontology-based Referential Constraints in DBMS,database system + cancer + NonControlled Keywords Not Found + Controlled Keywords Not Found + ontologies + database systems,AuthorProvided Keywords Not Found,"Keywords are typically used to qualify rows in a table. However, the fact that a keyword denotes a concept, which belongs to a specific knowledge domain, is not semantically enforced in current database systems. This paper proposes defining ontology based referential constraint for such keyword columns. A query on ontology, specified as part of the referential constraint, is used to identify the domain for the keyword column. Furthermore, since ontology may evolve causing change to the domain of the keyword column, the paper proposes use of ontology based transformation functions to either automatically evolve or to recommend refinements for the values in the keyword column. Also, queries on a keyword column can perform semantic match, that is, match a keyword to related terms based on the associated ontology. Thus, the proposed approach of semantically connecting keyword columns to ontologies 1) enhances semantic data integrity, 2) facilitates evolution of keyword columns with the referenced ontology, and 3) enables semantic match queries on keyword columns."
7F4F943E,International Conference on Data Engineering,stefan bottcher + jinghua groppe + sven groppe,2006,XPath Query Simplification with regard to the Elimination of Intersect and Except Operators,access control + computer science + database languages + xml + cost function + query language,,"XPath is widely used as an XML query language and is embedded in XQuery expressions and in XSLT stylesheets. In this paper, we propose a rule set which logically simplifies XPath queries by using a heuristic method in order to improve the processing time. Furthermore, we show how to substitute the XPath 2.0 intersect and except operators in a given XPath query with computed filter expressions. A performance evaluation comparing the execution times of the original XPath queries, which contain the intersect and except operators, and of the queries that are the result of our simplification approach shows that, depending on the used query evaluator and on the original query, performance improvements of a factor of up to 350 are possible."
7E5B5DA6,International Conference on Data Engineering,tok wang ling + min hu + changqing li,2006,Efficient Processing of Updates in Dynamic XML Data,data models + database languages + algorithm design and analysis + xml + labeling + NonControlled Keywords Not Found + Controlled Keywords Not Found + encoding + sections,AuthorProvided Keywords Not Found,"It is important to process the updates when nodes are inserted into or deleted from the XML tree. All the existing labeling schemes have high update cost, thus in this paper we propose a novel Compact Dynamic Binary String (CDBS) encoding to efficiently process the updates. CDBS has two important properties which form the foundations of this paper: (1) CDBS supports that codes can be inserted between any two consecutive CDBS codes with the orders kept and without re-encoding the existing codes; (2) CDBS is orthogonal to specific labeling schemes, thus it can be applied broadly to different labeling schemes or other applications to efficiently process the updates. We report our experimental results to show that our CDBS is superior to previous approaches to process updates in terms of the number of nodes to re-label and the time for updating."
7D6B6920,International Conference on Data Engineering,fusheng wang + j d m pearson + gerald madlmayr + frederick m azar + peiya liu,2006,Experiment Management with Metadata-based Integration for Collaborative Scientific Research,data models + data model + hierarchical data + scientific research + government + computer science + collaboration + data systems + information retrieval + NonControlled Keywords Not Found + data visualization + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Scientific research in many fields is increasingly a collaborative effort across multiple institutions and disciplines. Scientific researchers need not only an effective system to manage their data, results, and the experiments that generate the results, but also a platform to integrate, share and search these across multiple institutions. Therefore, researchers are able to reuse experiments, pool expertise and validate approaches. In this paper, we present Sci- Port, a system of experiment management and integration for collaborative scientific research. SciPortês architecture uses i) a general transformation-based data model to represent and link experiment processes; ii) hierarchical data classification across multiple institutions according to research programsê goals and organization; iii) metadatacentric representation that concisely captures the context of experiments; and iv) virtual data integration through centralized metadata integration. The system is built for open source, and the metadata-based representation and integration provides a unified framework and tool set to manage and share experiments for scientific research communities."
7ED1A8B1,International Conference on Data Engineering,ken pu + nick koudas + vagelis hristidis,2006,Syntactic Rule Based Approach toWeb Service Composition,rule based + web services + xml + state transition + dynamic programming + NonControlled Keywords Not Found + Controlled Keywords Not Found + web service + input output,AuthorProvided Keywords Not Found,"This paper studies a problem of web service composition from a syntactic approach. In contrast with other approaches on enriched semantic description such as statetransition description of web services, our focus is in the case when only the input-output type information from the WSDL specifications is available. The web service composition problem is formally formulated as deriving a given desired type from a collection of available types and web services using a prescribed set of rules with costs. We show that solving the minimal cost composition is NP-complete in general, and present a practical solution based on dynamic programming. Experiements using a mixture of synthetic and real data sets show that our approach is viable and produces good results."
7E355CBB,International Conference on Data Engineering,bee chung chen + lei chen + raghu ramakrishnan + david r musicant,2006,Learning from Aggregate Views,predictive models + data mining + labeling + decision trees + data privacy + NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In this paper, we introduce a new class of data mining problems called learning from aggregate views. In contrast to the traditional problem of learning from a single table of training examples, the new goal is to learn from multiple aggregate views of the underlying data, without access to the un-aggregated data. We motivate this new problem, present a general problem framework, develop learning methods for RFA (Restriction-Free Aggregate) views defined using COUNT, SUM, AVG and STDEV, and offer theoretical and experimental results that characterize the proposed methods."
7E6457E4,International Conference on Data Engineering,murali mani + elke a rundensteiner + ling wang,2006,U-Filter: A Lightweight XML View Update Checker,side effect + data storage + memory + xml + xml document + tcpip + NonControlled Keywords Not Found + relational database + Controlled Keywords Not Found + relational databases + data engineering + error correction,AuthorProvided Keywords Not Found,"Both XML-relational systems and native XML systems support creating XML wrapper views and querying against them. However, update operations against such virtual XML views in most cases are not supported yet."
80BE3207,International Conference on Data Engineering,larry pryor + eduard hovy + dennis mcleod + hyun woong shin,2006,Measuring Generality of Documents,information retrieval + data engineering + computer science + information science + word frequency + web search engine + frequency + ontologies + search engines + testing,,"ion to retrieve results based on desired generality appropriate for a user's knowledge and interests. We compared our generality quantification algorithm with human judges' weighting of values to show that the developed algorithm is significantly correlated."
8157BFFB,International Conference on Data Engineering,fusheng wang + xin zhou + carlo zaniolo,2006,Using XML to Build Efficient Transaction-Time Temporal Database Systems on Relational Databases,indexing + temporal database + indexation + NonControlled Keywords Not Found + relational database + history + Controlled Keywords Not Found + relational databases + data engineering + expressive power + data models + computer science + xml + data systems + database systems,AuthorProvided Keywords Not Found,"In this paper, we present the ArchIS system that achieves full-functionality transaction-time databases without requiring temporal extensions in XML or database standards. ArchISê architecture uses (a) XML to support temporally grouped (virtual) representations of the database history, (b) XQuery to express powerful temporal queries on such views, (c) temporal clustering and indexing techniques for managing the actual historical data in a relational database, and (d) SQL/XML for executing the queries on the XML views as equivalent queries on the relational database. The performance studies presented in the paper show that ArchIS is quite effective at storing and retrieving under complex query conditions the transaction-time history of relational databases."
806AB14A,International Conference on Data Engineering,felix naumann + melanie weis + jan hegewald,2006,XStruct: Efficient Schema Extraction from Multiple and Large XML Documents,xml schema + concrete + xml document + data integrity + xml + regular expression + data engineering + data mining,,"XML is the de facto standard format for data exchange on the Web. While it is fairly simple to generate XML data, it is a complex task to design a schema and then guarantee that the generated data is valid according to that schema. As a consequence much XML data does not have a schema or is not accompanied by its schema. In order to gain the benefits of having a schema-efficient querying and storage of XML data, semantic verification, data integration, etc.-This schema must be extracted. In this paper we present an automatic technique, XStruct, for XML Schema extraction. Based on ideas of [5], XStruct extracts a schema for XML data by applying several heuristics to deduce regular expressions that are 1-unambiguous and describe each elements contents correctly but generalized to a reasonable degree. Our approach features several advantages over known techniques: XStruct scales to very large documents (beyond 1GB) both in time and memory consumption; it is able to extract a general, complete, correct, minimal, and understandable schema for multiple documents; it detects datatypes and attributes. Experiments confirm these features and properties."
7E6ACB59,International Conference on Data Engineering,ganesh ramakrishnan + sumit negi + sachindra joshi + sreeram balakrishnan + raghu krishnapuram,2006,Automatic Sales Lead Generation from Web Data,data mining + training data + direct marketing + world wide web + decision making process + NonControlled Keywords Not Found + advertising + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Speed to market is critical to companies that are driven by sales in a competitive market. The earlier a potential customer can be approached in the decision making process of a purchase, the higher are the chances of converting that prospect into a customer. Traditional methods to identify sales leads such as company surveys and direct marketing are manual, expensive and not scalable. Over the past decade the World Wide Web has grown into an information-mesh, with most important facts being reported through Web sites. Several news papers, press releases, trade journals, business magazines and other related sources are on-line. These sources could be used to identify prospective buyers automatically. In this paper, we present a system called ETAP (Electronic Trigger Alert Program) that extracts trigger events from Web data that help in identifying prospective buyers. Trigger events are events of corporate relevance and indicative of the propensity of companies to purchase new products associated with these events. Examples of trigger events are change in management, revenue growth and mergers & acquisitions. The unstructured nature of information makes the extraction task of trigger events difficult. We pose the problem of trigger events extraction as a classification problem and develop methods for learning trigger event classifiers using existing classification methods. We present methods to automatically generate the training data required to learn the classifiers. We also propose a method of feature abstraction that uses named entity recognition to solve the problem of data sparsity. We score and rank the trigger events extracted from ETAP for easy browsing. Our experiments show the effectiveness of the method and thus establish the feasibility of automatic sales lead generation using the Web data."
7EE12F57,International Conference on Data Engineering,christopher re + jerome simeon + mary fernandez,2006,A Complete and Efficient Algebraic Compiler for XQuery,databases + navigation + xml + engines + sorting + standardization + algebra + NonControlled Keywords Not Found + user interfaces + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"As XQuery nears standardization, more sophisticated XQuery applications are emerging, which often exploit the entire language and are applied to non-trivial XML sources. We propose an algebra and optimization techniques that are suitable for building an XQuery compiler that is complete, correct, and efficient. We describe the compilation rules for the complete language into that algebra and present novel optimization techniques that address the needs of complex queries. These techniques include new query unnesting rewritings and specialized join algorithms that account for XQueryês complex predicate semantics. The algebra and optimizations are implemented in the Galax XQuery engine, and yield execution plans that are up to three orders of magnitude faster than earlier versions of Galax."
7E31DD39,International Conference on Data Engineering,gautam das + vana kalogeraki + dimitrios gunopulos + benjamin arai,2006,Approximating Aggregation Queries in Peer-to-Peer Networks,sampling methods + random sampling + NonControlled Keywords Not Found + Controlled Keywords Not Found + distributed computing + p2p + random walk + digital media + exact solution + distributed databases + intelligent networks + sampling technique + internet,AuthorProvided Keywords Not Found,"Peer-to-peer databases are becoming prevalent on the Internet for distribution and sharing of documents, applications, and other digital media. The problem of answering large scale, ad-hoc analysis queries _ e.g., aggregation queries _ on these databases poses unique challenges. Exact solutions can be time consuming and difficult to implement given the distributed and dynamic nature of peer-to-peer databases. In this paper we present novel sampling-based techniques for approximate answering of ad-hoc aggregation queries in such databases. Computing a high-quality random sample of the database efficiently in the P2P environment is complicated due to several factors _ the data is distributed (usually in uneven quantities) across many peers, within each peer the data is often highly correlated, and moreover, even collecting a random sample of the peers is difficult to accomplish. To counter these problems, we have developed an adaptive two-phase sampling approach, based on random walks of the P2P graph as well as block-level sampling techniques. We present extensive experimental evaluations to demonstrate the feasibility of our proposed solutio"
7DE64582,International Conference on Data Engineering,wangchien lee + hong va leong + ken c k lee,2006,Nearest Surrounder Queries,nearest neighbor + object oriented + indexation + neural networks + NonControlled Keywords Not Found + information systems + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In this paper, we study a new type of spatial query, Nearest Surrounder (NS), which searches the nearest surrounding spatial objects around a query point. NS query can be more useful than conventional nearest neighbor (NN) query as NS query takes the object orientation into consideration. To address this new type of query, we identify angle-based bounding properties and distance-bound properties of Rtree index. The former has not been explored for conventional spatial queries. With these identified properties, we propose two algorithms, namely, Sweep and Ripple. Sweep searches surrounders according to their orientation, while Ripple searches surrounders ordered by their distances to the query point. Both algorithms can deliver result incrementally with a single dataset lookup. We also consider the multiple-tier NS (mNS) query that searches multiple layers of NSs. We evaluate the algorithms and report their performance on both synthetic and real datasets."
7FE0A128,International Conference on Data Engineering,michael j franklin + matthew denny,2006,Operators for Expensive Functions in Continuous Queries,weather forecasting + design optimization + engines + supply chain management + process design + economic indicators + NonControlled Keywords Not Found + Controlled Keywords Not Found + pricing + finance,AuthorProvided Keywords Not Found,"Many analysis and monitoring applications require the repeated execution of expensive functions over streams of rapidly changing data. These applications appear in fields as varied as finance, supply chain management, and power utility monitoring.While many of these applications can be expressed declaratively, current continuous query processors are not designed to optimize queries with expensive user-defined functions. Such optimizations are hindered by ""black box"" function interfaces, where the operator has no control over the processing inside each invocation. We are currently developing VAOs (Variable Accuracy Operators), a new class of operators that allow the query processor to speed up individual function calls. VAOs use a new function interface that exposes the trade-off between work and accuracy inherent inmany functions. Using this new interface, VAOs can eliminate unneeded work by running each function call to only the accuracy needed by the query. VAOs play a key role in our larger research agenda of optimizing queries with expensive functions, and we briefly describe this larger agenda as well."
7F3D8D37,International Conference on Data Engineering,wensyan li + daniel c zilio + vishal s batra + mahadevan subramanian + calisto zuzarte + inderpal narang,2006,Load Balancing for Multi-tiered Database Systems through Autonomic Placement of Materialized Views,data models + database system + prototypes + information integration + distributed databases + materialized views + load balance + NonControlled Keywords Not Found + Controlled Keywords Not Found + statistics + database systems,AuthorProvided Keywords Not Found,"A materialized view or Materialized Query Table (MQT) is an auxiliary table with precomputed data that can be used to significantly improve the performance of a database query. AMaterialized Query Table Advisor (MQTA) is often used to recommend and create MQTs. The state-of-the-art MQTA works in a standalone database server where MQTs are placed on the same server as that in which the base tables are located. The MQTA does not apply to a federated or scaleout scenario in which MQTs need to be placed on other servers close to applications (i.e. a frontend database server) for offloading the workload on the backend database server. In this paper, we propose a Data Placement Advisor (DPA) and load balancing strategies for multi-tiered database systems. Built on top of the MQTA, DPA recommends MQTs and advises placement strategies for minimizing the response time for a query workload. To demonstrate the benefit of the data placement advising, we implemented a prototype of DPA that works with theMQTA in the IBM‘ DB2‘ Universal Database^TM (DB2 UDB) and the IBM WebSphere‘ Information Integrator (WebSphere II). The evaluation results showed substantial improvements of workload response times when MQTs are intelligently recommended and placed at a frontend database server subject to space and load characteristics for TPC-H and OLAP type workloads."
7F724556,International Conference on Data Engineering,jennifer widom + michael j franklin + wei hong + gustavo alonso + shawn r jeffery,2006,A Pipelined Framework for Online Cleaning of Sensor Data Streams,data warehouse + pipelines + wireless sensor networks + data cleaning + stream processing + supply chain management + data processing + writing + data capture + NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Data captured from the physical world through sensor devices tends to be noisy and unreliable. The data cleaning process for such data is not easily handled by standard data warehouse-oriented techniques, which do not take into account the strong temporal and spatial components of receptor data. We present Extensible receptor Stream Processing (ESP), a declarative query-based framework designed to clean the data streams produced by sensor devices."
811C1DC1,International Conference on Data Engineering,xuemin lin + flip korn + wei wang + jian xu + ying zhang,2006,Space-efficient Relative Error Order Sketch over Data Streams,relative error + data analysis + data mining + computer networks + NonControlled Keywords Not Found + Controlled Keywords Not Found + statistical analysis + statistical distributions + distributed computing,AuthorProvided Keywords Not Found,"We consider the problem of continuously maintaining order sketches over data streams with a relative rank error guarantee _. Novel space-efficient and one-scan randomised techniques are developed. Our first randomised algorithm can guarantee such a relative error precision _ with confidence 1 - delta using O( 1_ in frac{1} {2}2 log 1d log _^2N) space, where N is the number of data elements seen so far in a data stream. Then, a new one-scan space compression technique is developed. Combined with the first randomised algorithm, the one-scan space compression technique yields another one-scan randomised algorithm that guarantees the space requirement is O( 1frac{1} { in } log(1frac{1}{ in } log 1begin{gathered} frac{1}{delta } hfill  hfill  end{gathered} )frac{{log ^{2 + alpha } in N}} {{1 - 1/2^alpha }} (foralpha gt 0) on average while the worst case space remains O( frac{1}{{ in ^2 }}log frac{1} {delta }log in ^2 N). These results are immediately applicable to approximately computing quantiles over data streams with a relative error guarantee in and significantly improve the previous best space bound O( frac{1} {{ in ^3 }}log frac{1}{delta }log N). Our extensive experiment results demonstrate that both techniques can support an on-line computation against high speed data streams."
7E60D353,International Conference on Data Engineering,dina q goldin,2006,Faster In-Network Evaluation of Spatial Aggregationin Sensor Networks,routing + sensor network + data security + computer networks + NonControlled Keywords Not Found + Controlled Keywords Not Found + filtering,AuthorProvided Keywords Not Found,"Spatial aggregation is an important class of queries for geoaware spatial sensor database applications. Given a set of spatial regions, it involves the aggregation of dynamic sensor readings over each of these regions simultaneously. Nested spatial aggregation involves one more level of aggregation, combining these aggregates into a single aggregate value. We show that spatial aggregate values can often be computed in-network, rather than waiting until the partial aggregate records reach the root as is now the case. This decreases the amount of communication involved in query evaluation, thereby reducing the network's power consumption. We describe an algorithm that allows us to determine when an aggregate record for any spatial region is ready to be evaluated in-network, based on decorating the routing tree with region leader lists. We also identify several important scenarios, such as nested spatial aggregation and filtering predicates, when the savings from our approach are expected to be particularly great."
7FEE1906,International Conference on Data Engineering,mong li lee + gao cong + wynne hsu + hanyu li,2006,An Estimation System for XPath Expressions,feedback + query optimization + xml + dna + data structure + intrusion detection + data structures + NonControlled Keywords Not Found + Controlled Keywords Not Found + sequences + storms,AuthorProvided Keywords Not Found,"Estimating the result sizes of XML queries is important in query optimization and is useful in providing a quick feedback about the queries. Existing works have focused on the selectivity estimation of XML queries without order-based axes. In this work, we develop a framework to estimate the result sizes of XPath expressions with order-based axes. We describe how the path and order information of XML elements can be captured and summarized in compact data structures. We also describe methods to estimate the selectivity of XPath queries. The results of extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness and accuracy of the proposed approach."
8093362D,International Conference on Data Engineering,nisheeth shrivastava + subhash suri + john hershberger,2006,Cluster Hull: A Technique for Summarizing Spatial Data Streams,spatial data + pattern analysis + computer graphics + data analysis + computer science + shape + clustering algorithms + computer vision + NonControlled Keywords Not Found + Controlled Keywords Not Found + statistical distributions,AuthorProvided Keywords Not Found,"Recently there has been a growing interest in detecting patterns and analyzing trends in data that are generated continuously, often delivered in some fixed order and at a rapid rate, in the form of a data stream [5, 6]. When the stream consists of spatial data, its geometric ""shape"" can convey important qualitative aspects of the data set more effectively than many numerical statistics. In a stream setting, where the data must be constantly discarded and compressed, special care must be taken to ensure that the compressed summary faithfully captures the overall shape of the point distribution. We propose a novel scheme, ClusterHulls, to represent the shape of a stream of two-dimensional points. Our scheme is particularly useful when the input contains clusters with widely varying shapes and sizes, and the boundary shape, orientation, or volume of those clusters may be important in the analysis."
7F300E61,International Conference on Data Engineering,sriram mohan + yuqing wu + jonathan klinginsmith + a sengupta,2006,ACXESS - Access Control for XML with Enhanced Security Specifications,computer security + computer science + data security + xml + information security + access control + NonControlled Keywords Not Found + security policy + synthetic aperture sonar + informatics + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"We present ACXESS (Access Control for XML with Enhanced Security Specifications), a system for specifying and enforcing enhanced security constraints on XML via virtual ""security views"" and query rewrites. ACXESS is the first system that bears the capability to specify and enforce complicated security policies on both subtrees and structural relationships."
7E968B79,International Conference on Data Engineering,hanan samet + houman alborzi + jagan sankaranarayanan,2006,Enabling Query Processing on Spatial Networks,automation + spatial database + nearest neighbor search + computer science + computer networks + NonControlled Keywords Not Found + Controlled Keywords Not Found + real time systems + real time + database systems,AuthorProvided Keywords Not Found,A system that enables real time query processing on large spatial networks is demonstrated. The system provides functionality for processing a wide range of spatial queries such as nearest neighbor searches and spatial joins on spatial networks of sufficiently large sizes.
7D0D2F5C,International Conference on Data Engineering,eduard c dragut + weiyi meng + wensheng wu + prasad sistla + clement yu,2006,Merging Source Query Interfaces onWeb Databases,databases + interfaces + merging + search engines + web pages + search engine + data mining + e commerce + NonControlled Keywords Not Found + user interfaces + Controlled Keywords Not Found + user interface,AuthorProvided Keywords Not Found,"Recently, there are many e-commerce search engines that return information from Web databases. Unlike text search engines, these e-commerce search engines have more complicated user interfaces. Our aim is to construct automatically a natural query user interface that integrates a set of interfaces over a given domain of interest. For example, each airline company has a query interface for ticket reservation and our system can construct an integrated interface for all these companies. This will permit users to access information uniformly from multiple sources. Each query interface from an e-commerce search engine is designed so as to facilitate users to provide necessary information. Specifically, (1) related pieces of information such as first name and last name are grouped together and (2) certain hierarchical relationships are maintained. In this paper, we provide an algorithm to compute an integrated interface from query interfaces of the same domain. The integrated query interface can be proved to preserve the above two types of relationships. Experiments on five domains verify our theoretical study."
7FF00522,International Conference on Data Engineering,shouke qin + aoying zhou + weining qian,2006,Approximately Processing Multi-granularity Aggregate Queries over Data Streams,first order + application software + databases + fractal analysis + data mining + time complexity + impedance + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering + fractals + computer science + time change + search space + statistics,AuthorProvided Keywords Not Found,"Aggregate monitoring over data streams is attracting more and more attention in research community due to its broad potential applications. Existing methods suffer two problems, 1) The aggregate functions which could be monitored are restricted to be first-order statistic or monotonic with respect to the window size. 2) Only a limited number of granularity and time scales could be monitored over a stream, thus some interesting patterns might be neglected, and users might be misled by the incomplete changing profile about current data streams. These two impede the development of online mining techniques over data streams, and some kind of breakthrough is urged. In this paper, we employed the powerful tool of fractal analysis to enable the monitoring of both monotonic and non-monotonic aggregates on time-changing data streams. The monotony property of aggregate monitoring is revealed and monotonic search space is built to decrease the time overhead for accessing the synopsis from O(m) to O(logm), where m is the number of windows to be monitored. With the help of a novel inverted histogram, the statistical summary is compressed to be fit in limited main memory, so that high aggregates on windows of any length can be detected accurately and efficiently on-line. Theoretical analysis show the space and time complexity bound of this method are relatively low, while experimental results prove the applicability and efficiency of the proposed algorithm in different application settings."
7DA775A5,International Conference on Data Engineering,imran r mansuri + sunita sarawagi,2006,Integrating Unstructured Data into Relational Databases,conditional random field + web mining + data mining + training data + statistical model + NonControlled Keywords Not Found + relational database + Controlled Keywords Not Found + machine learning + relational databases + database systems,AuthorProvided Keywords Not Found,"In this paper we present a system for automatically integrating unstructured text into a multi-relational database using state-of-the-art statistical models for structure extraction and matching. We show how to extend current highperforming models, Conditional Random Fields and their semi-markov counterparts, to effectively exploit a variety of recognition clues available in a database of entities, thereby significantly reducing the dependence on manually labeled training data. Our system is designed to load unstructured records into columns spread across multiple tables in the database while resolving the relationship of the extracted text with existing column values, and preserving the cardinality and link constraints of the database. We show how to combine the inference algorithms of statistical models with the database imposed constraints for optimal data integration."
7D09DD0D,International Conference on Data Engineering,dong xin + jiawei han + hongyan liu + zheng shao,2006,C-Cubing: Efficient Computation of Closed Cubes by Aggregation-Based Checking,usability + computer science + NonControlled Keywords Not Found + Controlled Keywords Not Found + data cube + relational databases + data engineering,AuthorProvided Keywords Not Found,"It is well recognized that data cubing often produces huge outputs. Two popular efforts devoted to this problem are (1) iceberg cube, where only significant cells are kept, and (2) closed cube, where a group of cells which preserve roll-up/drill-down semantics are losslessly compressed to one cell. Due to its usability and importance, efficient computation of closed cubes still warrants a thorough study. In this paper, we propose a new measure, called closedness, for efficient closed data cubing. We show that closedness is an algebraic measure and can be computed efficiently and incrementally. Based on closedness measure, we develop an an aggregation-based approach, called C-Cubing (i.e., Closed-Cubing), and integrate it into two successful iceberg cubing algorithms: MM-Cubing and Star-Cubing. Our performance study shows that C-Cubing runs almost one order of magnitude faster than the previous approaches. We further study how the performance of the alternative algorithms of C-Cubing varies w.r.t the properties of the data sets."
81440B3C,International Conference on Data Engineering,wei hong + amol deshpande + joseph m hellerstein + david chu,2006,Approximate Data Collection in Sensor Networks using Probabilistic Models,databases + wireless sensor networks + data mining + wireless sensor network + robustness + biosensors + NonControlled Keywords Not Found + Controlled Keywords Not Found + data collection + probabilistic model + greedy heuristic + sensor network + intelligent networks + intelligent sensors + base station,AuthorProvided Keywords Not Found,"Wireless sensor networks are proving to be useful in a variety of settings. A core challenge in these networks is to minimize energy consumption. Prior database research has proposed to achieve this by pushing data-reducing operators like aggregation and selection down into the network. This approach has proven unpopular with early adopters of sensor network technology, who typically want to extract complete ""dumps"" of the sensor readings, i.e., to run ""SELECT *"" queries. Unfortunately, because these queries do no data reduction, they consume significant energy in current sensornet query processors. In this paper we attack the ""SELECT "" problem for sensor networks. We propose a robust approximate technique called Ken that uses replicated dynamic probabilistic models to minimize communication from sensor nodes to the networkês PC base station. In addition to data collection, we show that Ken is well suited to anomaly- and event-detection applications. A key challenge in this work is to intelligently exploit spatial correlations across sensor nodes without imposing undue sensor-to-sensor communication burdens to maintain the models. Using traces from two real-world sensor network deployments, we demonstrate that relatively simple models can provide significant communication (and hence energy) savings without undue sacrifice in result quality or frequency. Choosing optimally among even our simple models is NPhard, but our experiments show that a greedy heuristic performs nearly as well as an exhaustive algorithm."
816A27D2,International Conference on Data Engineering,mong li lee + chang sheng + wynne hsu + junmei wang,2006,A Partition-Based Approach to Graph Mining,databases + memory management + algorithm design and analysis + data structures + NonControlled Keywords Not Found + Controlled Keywords Not Found + tree graphs + data engineering,AuthorProvided Keywords Not Found,"Existing graph mining algorithms typically assume that databases are relatively static and can fit into the main memory. Mining of subgraphs in a dynamic environment is currently beyond the scope of these algorithms. To bridge this gap, we first introduce a partition-based approach called PartMiner for mining graphs. The PartMiner algorithm finds the frequent subgraphs by dividing the database into smaller and more manageable units, mining frequent subgraphs on these smaller units and finally combining the results of these units to losslessly recover the complete set of subgraphs in the database. Next, we extend PartMiner to handle updates in the dynamic environment. Experimental results indicate that PartMiner is effective and scalable in finding frequent subgraphs, and outperforms existing algorithms in the presence of updates."
59479173,International Conference on Data Engineering,daniel kifer + johannes gehrke + ashwin machanavajjhala + joseph y halpern + david m martin,2006,Worst-Case Background Knowledge in Privacy, + computer science + technical report,,"Recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. However, in practice, the data publisher does not know what background knowledge the attacker possesses. Thus, it is important to consider the worst-case. In this paper, we initiate a formal study of worst-case background knowledge. We propose a language that can express any background knowledge about the data. We provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most k pieces of information in this language. We also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold."
7ED47E75,International Conference on Data Engineering,jirong wen + fei wu + weiying ma + zaiqing nie,2006,Extracting Objects from the Web,design methodology + information extraction + search engines + web pages + data management + data mining + process design + html + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"Extracting and integrating object information from the Web is of great significance for Web data management. The existing Web information extraction techniques cannot provide satisfactory solution to the Web object extraction task since objects of the same type are distributed in diverse Web sources, whose structures are highly heterogeneous. In this paper, we propose a novel approach called Object-Level Information Extraction (OLIE) to extract Web objects. This approach extends a classic information extraction algorithm, Conditional Random Fields (CRF), by adding Web-specific information. The experimental results show OLIE can significantly improve the Web object extraction accuracy."
7EC6FA17,International Conference on Data Engineering,erik zeitler + tore risch,2006,Processing High-Volume Stream Queries on a Supercomputer,technology management + computational modeling + stream processing + radio telescope + sensor network + computer science + radio astronomy + antenna array + concurrent computing + space technology + hardware,,"Scientific instruments, such as radio telescopes, colliders, sensor networks, and simulators generate very high volumes of data streams that scientists analyze to detect and understand physical phenomena. The high data volume and the need for advanced computations on the streams require substantial hardware resources and scalable stream processing. We address these challenges by developing data stream management technology to support high-volume stream queries utilizing massively parallel computer hardware. We have developed a data stream management system prototype for state-of-the-art parallel hardware. The performance evaluation uses real measurement data from LOFAR, a radio telescope antenna array being developed in the Netherlands."
7DE90CE4,International Conference on Data Engineering,lin qiao + balakrishna r iyer + divyakant agrawal + amr el abbadi,2006,Automated Storage Management with QoS Guarantees,silicon + cost function + degradation + NonControlled Keywords Not Found + data migration + throughput + quality of service + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Automated storage management is critical for most dataintensive applications running on DBMSs. In large-scale storage subsystems, the workload is expected to vary with time. In order to ensure both QoS and efficient usage of storage resources, variation in the actual physical disks is allowed to support a single virtual disk. Such data migration generates extra IOs and consumes storage resources. Not only does data migration need to be scheduled ahead but it must also be scheduled in such a way that QoS violations do not occur because of the extra migration IOs. In this paper, we present a novel analytic framework, PULSTORE, for autonomically managing the storage to provide performance guarantee during migration."
7E998A14,International Conference on Data Engineering,kien a hua + ning yu + danzhou liu,2006,Query Decomposition: A Multiple Neighborhood Approach to Relevance Feedback Processing in Content-based Image Retrieval,feedback + feature space + k nearest neighbor + prototypes + computer science + shape + image retrieval + semantic similarity + NonControlled Keywords Not Found + Controlled Keywords Not Found + multidimensional systems + feature vector,AuthorProvided Keywords Not Found,"Todayês Content-Based Image Retrieval (CBIR) techniques are based on the ""k-nearest neighbors"" (k- NN) model. They retrieve images from a single neighborhood using low-level visual features. In this model, semantically similar images are assumed to be clustered in the high-dimensional feature space. Unfortunately, no visual-based feature vector is sufficient to facilitate perfect semantic clustering; and semantically similar images with different appearances are always clustered into distinct neighborhoods in the feature space. Confinement of the search results to a single neighborhood is an inherent limitation of the k-NN techniques. In this paper we consider a new image retrieval paradigm Ê the Query Decomposition model - that facilitates retrieval of semantically similar images from multiple neighborhoods in the feature space. The retrieval results are the k most similar images from different relevant clusters. We introduce a prototype, and present experimental results to illustrate the effectiveness and efficiency of this new approach to content-based image retrieval."
8125EDB4,International Conference on Data Engineering,s parthasarathy + hui yang,2006,Mining Spatial and Spatio-Temporal Patterns in Scientific Data,protein folding + data mining + protein structure + astronomy + data analysis + web personalization + fluid flow + proteins + bioinformatics + scientific data + computational fluid dynamics + molecular dynamic + intrusion detection + shape,,""
7FBED135,International Conference on Data Engineering,le gruenwald + prasanna pabmanabhan,2006,DREAM: A Data Replication Technique for Real-Time Mobile Ad-hoc Network Databases,mobile ad hoc network + mobile computing + prototypes + computer science + data replication + mobile ad hoc networks + computer architecture + NonControlled Keywords Not Found + Controlled Keywords Not Found + ad hoc networks + mobile database + real time,AuthorProvided Keywords Not Found,"In a Mobile Ad-hoc Network (MANET), due to the mobility and energy limitations of nodes, disconnection and network partitioning occur frequently. In addition, transactions in many MANET database applications have time constraints. In this paper, a Data REplication technique for real-time Ad-hoc Mobile databases (DREAM) that addresses all these issues is proposed. DREAM is prototyped on laptops and PDAs and compared with two existing replication techniques using a military database application."
7EDB3FA5,International Conference on Data Engineering,guido moerkotte + carlchristian kanne + sven helmer + matthias brantner,2006,Algebraic Optimization of Nested XPath Expressions,computer science + xml + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"The XPath language incorporates powerful primitives for formulating queries containing nested subexpressions which are existentially or universally quantified. However, even the best published approaches for evaluating XPath have unsatisfactory performance when applied to nested queries. We examine optimization techniques that unnest complex XPath queries. For this purpose, we classify XPath expressions particularly with regard to properties that are relevant for unnesting. We present algebraic equivalences that transform nested expressions into unnested expressions. In our experiments we compare the evaluation times with existing XPath evaluators and the naive evaluation."
7CFC9201,International Conference on Data Engineering,mariano p consens + john w s liu + bill ofarrell,2006,XPlainer: An XPath Debugging Framework,information technology + polynomials + information retrieval + xml document + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering + simple api for xml + navigation + xml + document object model + debugging + data visualization,AuthorProvided Keywords Not Found,"XML is an important practical paradigm in information technology and has a broad range of applications. How to access and retrieve the XML data is crucial to these applications. There are two standard ways for accessing and manipulating XML data, the Simple API for XML (SAX) and the Document Object Model (DOM). However, when an application needs to traverse through XML data, it is not easy to retrieve the required data with these two standard ways. XML data is impossible to be retrieved back and forth by SAX, and the graph-oriented DOM notation is not easy to work with. With such limitation, the W3C supervises the development of three important languages: XPath [3], XQuery and XSLT for exploring and querying XML. Among these three languages, XPath is the key and cornerstone language for the other two. XPath defines expressions for traversing an XML document and specifies the set of nodes (XPath 1.0) or the sequence of nodes (XPath 2.0) in the XML document."
7E5DC56F,International Conference on Data Engineering,paul brown + peter j haas,2006,Techniques for Warehousing of Sample Data,data warehouse + merging + sampling methods + xml + random sampling + NonControlled Keywords Not Found + warehousing + Controlled Keywords Not Found + parallel processing + data warehouses + scalability + database systems,AuthorProvided Keywords Not Found,"We consider the problem of maintaining a warehouse of sampled data that ""shadows"" a full-scale data warehouse, in order to support quick approximate analytics and metadata discovery. The full-scale warehouse comprises many ""data sets,"" where a data set is a bag of values; the data sets can vary enormously in size. The values constituting a data set can arrive in batch or stream form. We provide and compare several new algorithms for independent and parallel uniform random sampling of data-set partitions, where the partitions are created by dividing the batch or splitting the stream. We also provide novel methods for merging samples to create a uniform sample from an arbitrary union of data-set partitions. Our sampling/merge methods are the first to simultaneously support statistical uniformity, a priori bounds on the sample footprint, and concise sample storage. As partitions are rolled in and out of the warehouse, the corresponding samples are rolled in and out of the sample warehouse. In this manner our sampling methods approximate the behavior of more sophisticated stream-sampling methods, while also supporting parallel processing. Experiments indicate that our methods are efficient and scalable, and provide guidance for their application."
7D4F2BDE,International Conference on Data Engineering,zeeshan sardar + bettina kemme,2006,Don&#146;t be a Pessimist: Use Snapshot based Concurrency Control for XML,database languages + col + snapshot isolation + concurrency control + xml + NonControlled Keywords Not Found + Controlled Keywords Not Found + concurrent computing + optimistic concurrency control + database systems,AuthorProvided Keywords Not Found,"As native XML database systems (e.g., [3, 7, 8]) get increasingly popular, fine-granularity concurrency control becomes imperative in order to allow different clients to concurrently access the same documents. Existing concurrency control approaches for XML are mainly based on locking [2, 3, 4, 6, 5]. However, the experiments of [5] have shown that the locking overhead, especially for read operations, can be tremendous. In this paper, we present two snapshot based concurrency control mechanisms that avoid locking. Instead, transactions access a committed snapshot of the data."
7ECB3CFD,International Conference on Data Engineering,sandeep prakash + sourav s bhowmick,2006,"Every Click You Make, IWill Be Fetching It: Efficient XML Query Processing in RDMS Using GUI-driven Prefetching",data retrieval + database languages + graphical user interfaces + xml + time measurement + information retrieval + path expressions + NonControlled Keywords Not Found + Controlled Keywords Not Found + query language + relational databases + data engineering,AuthorProvided Keywords Not Found,"formulation and efficient processing of the formulated query. However, due to the nature of XML data, formulating an XML query using an XML query language such as XQuery requires considerable effort. A user must be completely familiar with the syntax of the query language, and must be able to express his/her needs accurately in a syntactically correct form. In many real life applications it is not realistic to assume that users are proficient in expressing such textual queries. Hence, there is a need for a user-friendly visual querying schemes to replace data retrieval aspects of XQuery. In this paper, we address the problem of efficient processing of XQueries in the relational environment where the queries are formulated using a user-friendly GUI. We take a novel and non-traditional approach to improving query performance by prefetching data during the formulation of a query in a single-user environment. The latency offered by the GUI-based query formulation is utilized to prefetch portions of the query results. The basic idea we employ for prefetching is that we prefetch constituent path expressions, store the intermediary results, reuse them when connective is added or ""Run"" is pressed."
7F45F0EF,International Conference on Data Engineering,adam silberstein + rebecca braynard + jun yang,2006,Energy-Efficient Continuous Isoline Queries in Sensor Networks,application software + environmental monitoring + wireless sensor networks + sensor network + computer science + intelligent networks + energy efficiency + NonControlled Keywords Not Found + energy efficient + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Environmental monitoring is a promising application for sensor networks. Many scenarios produce geographically correlated readings, making them visually interesting and good targets for the isoline query. This query depicts boundaries showing how values change in the network. Temporal and spatial suppression provide opportunities for reducing the cost of maintaining the query result. We combine both techniques for maximal benefit by monitoring node and edge constraints. A monitored node triggers a report if its value changes. A monitored edge triggers a report if the difference between its nodesê values changes. The root collects reports and derives all node values, from which the query result is generated. We fully exploit this strategy in our algorithm, CONCH, which maintains the set of node and edge constraints that minimizes message traffic."
7FDAF458,International Conference on Data Engineering,jennifer l beckmann + alan halverson + rajasekar krishnamurthy + jeffrey f naughton,2006,Extending RDBMSs To Support Sparse Datasets Using An Interpreted Attribute Storage Format,ease of use + database management system + sparse data + demography + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + data engineering + scalability + statistics,AuthorProvided Keywords Not Found,"Sparse"" data, in which relations have many attributes that are null for most tuples, presents a challenge for relational database management systems. If one uses the normal ""horizontal"" schema to store such data sets in any of the three leading commercial RDBMS, the result is tables that occupy vast amounts of storage, most of which is devoted to nulls. If one attempts to avoid this storage blowup by using a ""vertical"" schema, the storage utilization is indeed better, but query performance is orders of magnitude slower for certain classes of queries. In this paper, we argue that the proper way to handle sparse data is not to use a vertical schema, but rather to extend the RDBMS tuple storage format to allow the representation of sparse attributes as interpreted fields. The addition of interpreted storage allows for efficient and transparent querying of sparse data, uniform access to all attributes, and schema scalability. We show, through an implementation in PostgreSQL, that the interpreted storage approach dominates in query efficiency and ease-of-use over the current horizontal storage and vertical schema approaches over a wide range of queries and sparse data sets."
7DD5BF57,International Conference on Data Engineering,evangelos kanoulas + donghui zhang + tian xia + yang du,2006,Finding Fastest Paths on A Road Network with Speed Patterns,discrete time + geographic information systems + degradation + NonControlled Keywords Not Found + lower bound + Controlled Keywords Not Found + information science,AuthorProvided Keywords Not Found,"This paper proposes and solves the Time-Interval All Fastest Path (allFP) query. Given a user-defined leaving or arrival time interval I, a source node s and an end node e, allFP asks for a set of all fastest paths from s to e, one for each sub-interval of I. Note that the query algorithm should find a partitioning of I into sub-intervals. Existing methods can only be used to solve a very special case of the problem, when the leaving time is a single time instant. A straightforward solution to the allFP query is to run existing methods many times, once for every time instant in I. This paper proposes a solution based on novel extensions to the A* algorithm. Instead of expanding the network many times, we expand once. The travel time on a path is kept as a function of leaving time. Methods to combine travel-time functions are provided to expand a path. A novel lower-bound estimator for travel time is proposed. Performance results reveal that our method is more efficient and more accurate than the discrete-time approach."
7F6F2160,International Conference on Data Engineering,christopher jermaine + subramanian arumugam,2006,Closest-Point-of-Approach Join for Moving Object Histories,application software + computer science + computational geometry + computer simulation + NonControlled Keywords Not Found + history + Controlled Keywords Not Found + computational modeling + data engineering,AuthorProvided Keywords Not Found,"In applications that produce a large amount of data describing the paths of moving objects, there is a need to ask questions about the interaction of objects over a long recorded history. In this paper, we consider the problem of computing joins over massive moving object histories. The particular join that we study is the ""Closest-Point-Of- Approach"" join, which asks: Given a massive moving object history, which objects approached within a distance ‚dê of one another? We carefully consider several relatively obvious strategies for computing the answer to such a join, and then propose a novel, adaptive join algorithm which naturally alters the way in which it computes the join in response to the characteristics of the underlying data."
7E452AC7,International Conference on Data Engineering,matthias schubert + alexey pryakhin + christian bohm,2006,The Gauss-Tree: Efficient Object Identification in Databases of Probabilistic Feature Vectors,probability distribution function + biometrics + gaussian processes + information retrieval + face recognition + NonControlled Keywords Not Found + informatics + Controlled Keywords Not Found + uncertainty + fingerprint recognition + feature vector,AuthorProvided Keywords Not Found,"In applications of biometric databases the typical task is to identify individuals according to features which are not exactly known. Reasons for this inexactness are varying measuring techniques or environmental circumstances. Since these circumstances are not necessarily the same when determining the features for different individuals, the exactness might strongly vary between the individuals as well as between the features. To identify individuals, similarity search on feature vectors is applicable, but even the use of adaptable distance measures is not capable to handle objects having an individual level of exactness. Therefore, we develop a comprehensive probabilistic theory in which uncertain observations are modeled by probabilistic feature vectors (pfv), i.e. feature vectors where the conventional feature values are replaced by Gaussian probability distribution functions. Each feature value of each object is complemented by a variance value indicating its uncertainty. We define two types of identification queries, k-mostlikely identification and threshold identification. For efficient query processing, we propose a novel index structure, the Gauss-tree. Our experimental evaluation demonstrates that pfv stored in a Gauss-tree significantly improve the result quality compared to traditional feature vectors. Additionally, we show that the Gauss-tree significantly speeds up query times compared to competitive methods."
7E9C3CBE,International Conference on Data Engineering,paolo atzeni,2006,Schema and Data Translation,data models + erbium + xml + writing + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases,AuthorProvided Keywords Not Found,"The need to transform, integrate and exchange data is common to many application contexts. In databases, we often use different systems to handle data, with different models, and we therefore need to translate data and their description from one to another. The problem has been considered for decades, but definitive solutions are not yet available. The problem is relevant at the schema level, during the specification or design phase, and at the data level, when we have databases, and we want to translate them into some other system, which may be similar (for example, relational to relational) or completely different (for example, XML to relational or viceversa). In current practice, translation problems are often tackled by means of ad-hoc solutions, for example by writing code for each specific application, but this is clearly very heavy and hard to maintain."
8095C4E3,International Conference on Data Engineering,mong li lee + wynne hsu + chang sheng,2006,Mining Dense Periodic Patterns in Time Series Data,databases + algorithm design and analysis + data mining + time series + NonControlled Keywords Not Found + lower bound + time series data + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Existing techniques to mine periodic patterns in time series data are focused on discovering full-cycle periodic patterns from an entire time series. However, many useful partial periodic patterns are hidden in long and complex time series data. In this paper, we aim to discover the partial periodicity in local segments of the time series data. We introduce the notion of character density to partition the time series into variable-length fragments and to determine the lower bound of each characterês period. We propose a novel algorithm, called DPMiner, to find the dense periodic patterns in time series data. Experimental results on both synthetic and real-life datasets demonstrate that the proposed algorithm is effective and efficient to reveal interesting dense periodic patterns."
805A5E75,International Conference on Data Engineering,cristian estan + jeffrey f naughton,2006,End-biased Samples for Join Cardinality Estimation,sampling methods + histograms + probability distribution + random sampling + NonControlled Keywords Not Found + Controlled Keywords Not Found + multidimensional systems + data engineering + sample size + biased sampling + frequency,AuthorProvided Keywords Not Found,"We present a new technique for using samples to estimate join cardinalities. This technique, which we term ""end-biased samples,"" is inspired by recent work in network traffic measurement. It improves on random samples by using coordinated pseudo-random samples and retaining the sampled values in proportion to their frequency. We show that end-biased samples always provide more accurate estimates than random samples with the same sample size. The comparison with histograms is more interesting _ while end-biased histograms are somewhat better than end-biased samples for uncorrelated data sets, end-biased samples dominate by a large margin when the data is correlated. Finally, we compare end-biased samples to the recently proposed ""skimmed sketches"" and show that neither dominates the other, that each has different and compelling strengths and weaknesses. These results suggest that endbiased samples may be a useful addition to the repertoire of techniques used for data summarization."
7E0F0516,International Conference on Data Engineering,khuzaima daudjee + kenneth salem,2006,Inferring a Serialization Order for Distributed Transactions,data mining + NonControlled Keywords Not Found + Controlled Keywords Not Found + scalability + database system + computer science + concurrency control + distributed databases + control systems + distributed transactions + protocols + concurrent computing + database systems,AuthorProvided Keywords Not Found,"Data partitioning is often used to scale-up a database system. In a centralized database system, the serialization order of commited update transactions can be inferred from the database log. To achieve this in a shared-nothing distributed database, the serialization order of update transactions must be inferred from multiple database logs. We describe a technique to generate a single stream of updates from logs of multiple database systems. This single stream represents a valid serialization order of update transactions at the sites over which the database is partitioned."
7FB0E8B0,International Conference on Data Engineering,yuelong jiang + alexander tuzhilin + ke wang,2006,Mining Actionable Patterns by Role Models,domain knowledge + data mining + NonControlled Keywords Not Found + search space + Controlled Keywords Not Found + role models + data engineering,AuthorProvided Keywords Not Found,"Data mining promises to discover valid and potentially useful patterns in data. Often, discovered patterns are not useful to the user.""Actionability"" addresses this problem in that a pattern is deemed actionable if the user can act upon it in her favor. We introduce the notion of ""action"" as a domain-independent way to model the domain knowledge. Given a data set about actionable features and an utility measure, a pattern is actionable if it summarizes a population that can be acted upon towards a more promising population observed with a higher utility. We present several pruning strategies taking into account the actionability requirement to reduce the search space, and algorithms for mining all actionable patterns as well as mining the top k actionable patterns. We evaluate the usefulness of patterns and the focus of search on a real-world application domain."
7D18D6D9,International Conference on Data Engineering,johannes gehrke,2006,Models and Methods for Privacy-Preserving Data Analysis and Publishing,government + data analysis + computer science + data privacy + NonControlled Keywords Not Found + Controlled Keywords Not Found + publishing,AuthorProvided Keywords Not Found,"The digitization of our daily lives has led to an explosion in the collection of data by governments, corporations, and individuals. Protection of confidentiality of this data is of utmost importance. However, knowledge of statistical properties of this private data can have significant societal benefit, for example, in decisions about the allocation of public funds based on Census data, or in the analysis of medical data from different hospitals to understand the interaction of drugs. This tutorial will survey recent research that builds bridges between the two seemingly conflicting goals of sharing data while preserving data privacy and confidentiality. The tutorial will cover definitions of privacy and disclosure, and associated methods how to enforce them."
7D8CF05A,International Conference on Data Engineering,hiroyuki kitagawa + yoshiharu ishikawa + sophoin khy,2006,Novelty-based Incremental Document Clustering for On-line Documents,clustering algorithms + data mining + data engineering + engineering management + k means + radio broadcasting + document clustering + information gain,,"Document clustering has been used as a core technique in managing vast amount of data and providing needed information. In on-line environments, generally new information gains more interests than old one. Traditional clustering focuses on grouping similar documents into clusters by treating each document with equal weight. We proposed a novelty-based incremental clustering method for on-line documents that has biases on recent documents. In the clustering method, the notion of 'novelty' is incorporated into a similarity function and a clustering method, a variant of the K-means method, is proposed. We examine the efficiency and behaviors of the method by experiments."
7F39938F,International Conference on Data Engineering,daniel kifer + johannes gehrke + muthuramakrishnan venkitasubramaniam + ashwin machanavajjhala,2006,L-diversity: privacy beyond k-anonymity,insurance + computer science + information analysis + data privacy + NonControlled Keywords Not Found + Controlled Keywords Not Found + publishing,AuthorProvided Keywords Not Found,"Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called kappa-anonymity has gained popularity. In a kappa-anonymized dataset, each record is indistinguishable from at least kÊ1 other records with respect to certain ""identifying"" attributes. In this paper we show with two simple attacks that a kappa-anonymized dataset has some subtle, but severe privacy problems. First, we show that an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. Second, attackers often have background knowledge, and we show that kappa-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks and we propose a novel and powerful privacy definition called ell-diversity. In addition to building a formal foundation for ell-diversity, we show in an experimental evaluation that ell-diversity is practical and can be implemented efficiently."
80938979,International Conference on Data Engineering,christos tryfonopoulos + manolis koubarakis + stratos idreos,2006,Distributed Evaluation of Continuous Equi-join Queries over Large Structured Overlay Networks,satisfiability + computer networks + overlay network + indexation + NonControlled Keywords Not Found + Controlled Keywords Not Found + protocols + relational databases + distributed computing,AuthorProvided Keywords Not Found,"We study the problem of continuous relational query processing in Internet-scale overlay networks realized by distributed hash tables. We concentrate on the case of continuous two-way equi-join queries. Joins are hard to evaluate in a distributed continuous query environment because data from more than one relations is needed, and this data is inserted in the network asynchronously. Each time a new tuple is inserted, the network nodes have to cooperate to check if this tuple can contribute to the satisfaction of a query when combined with previously inserted tuples. We propose a series of algorithms that initially index queries at network nodes using hashing. Then, they exploit the values of join attributes in incoming tuples to rewrite the given queries into simpler ones, and reindex them in the network where they might be satisfied by existing or future tuples. We present a detailed experimental evaluation in a simulated environment and we show that our algorithms are scalable, balance the storage and query processing load and keep the network traffic low."
7F70479E,International Conference on Data Engineering,adam silberstein + rebecca braynard + jun yang + clarence ellis + kamesh munagala,2006,A Sampling-Based Approach to Optimizing Top-k Queries in Sensor Networks,constraint optimization + wireless sensor networks + wireless sensor network + linear program + NonControlled Keywords Not Found + Controlled Keywords Not Found + network topology + gaussian processes + predictive models + sensor network + computer science + intelligent networks + data acquisition,AuthorProvided Keywords Not Found,"Wireless sensor networks generate a vast amount of data. This data, however, must be sparingly extracted to conserve energy, usually the most precious resource in battery-powered sensors. When approximation is acceptable, a model-driven approach to query processing is effective in saving energy by avoiding contacting nodes whose values can be predicted or are unlikely to be in the result set. To optimize queries such as top-k, however, reasoning directly with models of joint probability distributions can be prohibitively expensive. Instead of using models explicitly, we propose to use samples of past sensor readings. Not only are such samples simple to maintain, but they are also computationally efficient to use in query optimization. With these samples, we can formulate the problem of optimizing approximate top-k queries under an energy constraint as a linear program. We demonstrate the power and flexibility of our sampling-based approach by developing a series of topk query planning algorithms with linear programming, which are capable of efficiently producing plans with better performance and novel features. We show that our approach is both theoretically sound and practically effective on simulated and real-world datasets."
80DBD6BC,International Conference on Data Engineering,basit shafiq + elisa bertino + arjmand samuel + arif ghafoor,2006,Technique for Optimal Adaptation of Time-Dependent Workflows with Security Constraints,application software + dynamic programming + e commerce + domains + NonControlled Keywords Not Found + Controlled Keywords Not Found + distributed application + distributed applications + data engineering + authorization + software architecture + security + access control + environmental management,AuthorProvided Keywords Not Found,"Distributed workflow based systems are widely used in various application domains including e-commerce, digital government, healthcare, manufacturing and many others. Workflows in these application domains are not restricted to the administrative boundaries of a single organization [1]. The tasks in a workflow need to be performed in a certain order and often times are subject to temporal constraints and dependencies [1, 2]. A key requirement for such workflow applications is to provide the right data to the right person at the right time. This requirement motivates for dynamic adaptations of workflows for dealing with changing environmental conditions and exceptions."
810ED11F,International Conference on Data Engineering,dennis shasha + xin zhang,2006,Better Burst Detection,binary trees + application software + binary tree + computer science + data structure + gamma ray bursts + data structures + NonControlled Keywords Not Found + tree data structures + Controlled Keywords Not Found + data engineering + heuristic search,AuthorProvided Keywords Not Found,"A burst is a large number of events occurring within a certain time window. Many data stream applications require the detection of bursts across a variety of window sizes. For example, stock traders may be interested in bursts having to do with institutional purchases or sales that are spread out over minutes or hours. In this paper, we present a new algorithmic framework for elastic burst detection [1]: a family of data structures that generalizes the Shifted Binary Tree, and a heuristic search algorithm to find an efficient structure given the input. We study how different inputs affect the desired structures and the probability to trigger a detailed search. Experiments on both synthetic and real world data show a factor of up to 35 times improvement compared with the Shifted Binary Tree over a wide variety of inputs, depending on the inputs."
7E667184,International Conference on Data Engineering,ping wu + jirong wen + weiying ma + huan liu,2006,Query Selection Techniques for Efficient Crawling of Structured Web Sources,np complete problem + search engines + structured data + web services + web search engine + data acquisition + hidden web + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + web service + dominating set,AuthorProvided Keywords Not Found,"The high quality, structured data from Web structured sources is invaluable for many applications. Hidden Web databases are not directly crawlable by Web search engines and are only accessible through Web query forms or via Web service interfaces. Recent research efforts have been focusing on understanding these Web query forms. A critical but still largely unresolved question is: how to efficiently acquire the structured information inside Web databases through iteratively issuing meaningful queries? In this paper we focus on the central issue of enabling efficient Web database crawling through query selection, i.e. how to select good queries to rapidly harvest data records from Web databases. We model each structured Web database as a distinct attribute-value graph. Under this theoretical framework, the database crawling problem is transformed into a graph traversal one that follows ""relational"" links. We show that finding an optimal query selection plan is equivalent to finding a Minimum Weighted Dominating Set of the corresponding database graph, a well-known NP-Complete problem. We propose a suite of query selection techniques aiming at optimizing the query harvest rate. Extensive experimental evaluations over real Web sources and simulations over controlled database servers validate the effectiveness of our techniques and provide insights for future efforts in this"
7F6838FE,International Conference on Data Engineering,huiming qu + alexandros labrinidis + daniel mosse,2006,UNIT: User-centric Transaction Management in Web-Database Systems,application software + weather forecasting + user requirements + computer science + satisfiability + NonControlled Keywords Not Found + Controlled Keywords Not Found + information services + internet + web server,AuthorProvided Keywords Not Found,"Web-database systems are nowadays an integral part of everybodyês life, with applications ranging from monitoring/ trading stock portfolios, to personalized blog aggregation and news services, to personalized weather tracking services. For most of these services to be successful (and their users to be kept satisfied), two criteria need to be met: user requests must be answered in a timely fashion and using fresh data. This paper presents a framework to balance both requirements from the usersê perspective. Toward this, we propose a user satisfaction metric to measure the overall effectiveness of the Web-database system. We also provide a set of algorithms to dynamically optimize this metric, through query admission control and update frequency modulation. Finally, we present extensive experimental results which compare our proposed algorithms to the current state of the art and show that we outperform competitors under various workloads (generated based on real traces) and user requirements."
7D858CD1,International Conference on Data Engineering,antoni wolski + anttipekka liedes,2006,"SIREN: A Memory-Conserving, Snapshot-Consistent Checkpoint Algorithm for in-Memory Databases",transaction processing + solids + information technology + experimental analysis + shadow mapping + algorithm design and analysis + NonControlled Keywords Not Found + throughput + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Checkpoint of an in-memory database is the main source of a persistent database image surviving a software crash, or a power outage, and is, together with transactions logs, a foundation for transaction durability. Since checkpoints are created simultaneously with transaction processing, they tend to decrease database throughput and increase its memory footprint. Of the current methods, most efficient are the fuzzy checkpoint algorithms that write dirty pages to disk and require transaction logs for reconstructing a consistent state. Known consistency-preserving methods suffer from excessive memory usage or a transaction-blocking behavior. In this paper, we present a consistency-preserving and memory-efficient checkpoint method. It is based on tuple shadowing as opposed to known page shadowing methods, and rearranging of tuples between pages for minimal memory usage overhead. The methodês algorithms are introduced and both analytical and experimental analysis of the proposed algorithms show significant reduction in the memory usage overhead, and up to 30% higher transaction throughput compared with a fuzzy checkpoint method with undo/redo log."
7FB78EBB,International Conference on Data Engineering,yoshinari kanamori + masayoshi aritsugi + damdinsuren amarmend,2006,An Air Index for Data Access over Multiple Wireless Broadcast Channels,computer science + bandwidth + time measurement + information retrieval + indexation + data access + temperature + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering + broadcasting + frequency,AuthorProvided Keywords Not Found,"In this paper, we propose an index allocation method for data access over multiple wireless channels. Our method first derives external index information from the scheduled data, and then allocates it over upper channels. Moreover, local exponential indexes with different parameters are built within each channel for local data search. Experiments are performed to compare the effectiveness of our approach with an existing approach. The results show that our method outperforms the existing method."
7F12A7B7,International Conference on Data Engineering,luca cabibbo + ivan panella + riccardo torlone,2006,DaWaII: a Tool for the Integration of Autonomous Data Marts,data warehouse + databases + merging + data mining + system testing + materialized views + multidimensional database + NonControlled Keywords Not Found + Controlled Keywords Not Found + multidimensional systems + data warehouses,AuthorProvided Keywords Not Found,"DaWaII (Data Warehouse IntegratIon) is a tool supporting the various activities related to the integration of multidimensional data. This problem arises in common scenarios where there is the need to combine independently developed data warehouses. Actually, a today common practice for building a data warehouse is to develop a collection of integrated data marts, each of which provide a dimensional view of a single business process. These data marts should be based on shared dimensions but very often, even within the same company, designers develop their data marts independently and it turns out that their integration is a difficult task. Indeed, the problem arises in other common cases. For instance, when different companies get involved in a federated project or when there is the need to combine a proprietary data warehouse with external information, for instance, with multidimensional data wrapped from the Web."
7EA0F22A,International Conference on Data Engineering,raghav kaushik + nilesh dalvi + surajit chaudhuri,2006,Robust Cardinality and Cost Estimation for Skyline Operator,database languages + sampling methods + histograms + engines + information retrieval + robustness + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering + cost estimation,AuthorProvided Keywords Not Found,"Incorporating the skyline operator inside the relational engine requires solving the cardinality estimation and the cost estimation problem, hitherto unaddressed. We propose robust techniques to estimate the cardinality and the computational cost of Skyline, and through an empirical comparison, show that our technique is substantially more effective than traditional approaches. Finally, we show through an implementation in Microsoft SQL Server that skyline queries can substantially benefit from our techniques."
7D6CCCE7,International Conference on Data Engineering,jianliang xu + xueyan tang + wangchien lee + minji wu,2006,Monitoring Top-k Query inWireless Sensor Networks,routing + sampling methods + wireless sensor networks + sensor network + energy efficiency + NonControlled Keywords Not Found + Controlled Keywords Not Found + base stations,AuthorProvided Keywords Not Found,"Top-k monitoring is important to many wireless sensor applications. This paper exploits the semantics of top-k query and proposes a novel energy-efficient monitoring approach, called FILA. The basic idea is to install a filter at each sensor node to suppress unnecessary sensor updates. The correctness of the top-k result is ensured if all sensor nodes perform updates according to their filters. We show via simulation that FILA outperforms the existing TAGbased approach by an order of magnitude."
7D3AE925,International Conference on Data Engineering,heng tao shen + beng chin ooi + kianlee tan,2006,SaveRF: Towards Efficient Relevance Feedback Search,feedback + feedback loop + iterative methods + indexing + information technology + computer science + information retrieval + linear regression + random access + NonControlled Keywords Not Found + search space + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In multimedia retrieval, a query is typically interactively refined towards the ‚optimalê answers by exploiting user feedback. However, in existing work, in each iteration, the refined query is re-evaluated. This is not only inefficient but fails to exploit the answers that may be common between iterations. In this paper, we introduce a new approach called SaveRF (Save random accesses in Relevance Feedback) for iterative relevance feedback search. SaveRF predicts the potential candidates for the next iteration and maintains this small set for efficient sequential scan. By doing so, repeated candidate accesses can be saved, hence reducing the number of random accesses. In addition, efficient scan on the overlap before the search starts also tightens the search space with smaller pruning radius. We implemented SaveRF and our experimental study on real life data sets show that it can reduce the I/O cost significantly."
7EC8B495,International Conference on Data Engineering,mor harcholbalter + arun iyengar + adam wierman + erich m nahum + bianca schroeder,2006,How to Determine a Good Multi-Programming Level for External Scheduling,feedback + application software + automatic control + computer science + system analysis and design + NonControlled Keywords Not Found + quality of service + throughput + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Scheduling/prioritization of DBMS transactions is important for many applications that rely on database backends. A convenient way to achieve scheduling is to limit the number of transactions within the database, maintaining most of the transactions in an external queue, which can be ordered as desired by the application. While external scheduling has many advantages in that it doesnêt require changes to internal resources, it is also difficult to get right in that its performance depends critically on the particular multiprogramming limit used (the MPL), i.e. the number of transactions allowed into the database. If the MPL is too low, throughput will suffer, since not all DBMS resources will be utilized. On the other hand, if the MPL is too high, there is insufficient control on scheduling. The question of how to adjust theMPL to achieve both goals simultaneously is an open problem, not just for databases but in system design in general. Herein we study this problem in the context of transactional workloads, both via extensive experimentation and queueing theoretic analysis. We find that the two most critical factors in adjusting the MPL are the number of resources that the workload utilizes and the variability of the transactionsê service demands. We develop a feedback based controller, augmented by queueing theoretic models for automatically adjusting the MPL. Finally, we apply our methods to the specific problem of external prioritization of transactions. We find that external prioritization can be nearly as effective as internal prioritization, without any negative consequences, when the MPL is set appropriately."
7DF0BED4,International Conference on Data Engineering,yicheng tu + sunil prabhakar,2006,Control-Based Load Shedding in Data Stream Management Systems,quality of service + steady state + data processing + databases + feedback control + control systems + process control + quality management,,"Data stream management has attracted much enthusiasm from the database community in recent years. A very important characteristic of data stream management systems (DSMSs) is that both data arrival and query processing are continuous. As a result, the system needs to handle a large number of streams and queries concurrently. This brings the problem of maintaining Quality-of-Service (QoS) (i.e., parameters describing timeliness, reliability, and precision) in query processing and data refreshing. Important QoS parameters include: processing delay, data loss, and sampling rate, etc. Many applications of DSMS place strong realtime constraints on query processing. Hard (e.g., in stock price analysis) or soft (e.g., in network monitoring) deadlines are normally set and the value of query results drops dramatically if these deadlines are missed. However, delays in data processing in DSMSs are difficult to control due to resource sharing of multiple streams/queries and unpredictable pattern of resource consumption of these entities. System overloading, which inevitably degrades QoS, is very common in such environments. Load shedding techniques have been exploited to overcome system overloading [6, 1]. The idea is to decrease the input load by discarding data tuples from the query engine. Essentially, it targets at maintaining critical QoS (e.g., processing delay) with the price of less critical ones (e.g., data loss). The specific problem we are interested in is: how can we control processing delays to be under an appropriate level with as little data loss as possible? Current DSMSs employ simple heuristic methods in attempt to solve this problem. For example, Fig. 1 shows the load shedding algorithm utilized in Aurora [6] and STREAM [1] DSMSs."
8056E6CE,International Conference on Data Engineering,ouri wolfson + hu cao + bo xu + huabei yin,2006,Searching Local Information in Mobile Databases,mobile ad hoc network + databases + ultra wide band + bandwidth + NonControlled Keywords Not Found + Controlled Keywords Not Found + network topology + broadcasting + routing + bluetooth + satisfiability + mobile ad hoc networks + intelligent networks + ad hoc networks + mobile database,AuthorProvided Keywords Not Found,"A mobile ad-hoc network (MANET) is a set of moving objects that communicate with each other via unregulated, short-range wireless technologies such as IEEE 802.11, Bluetooth, or Ultra Wide Band (UWB). No fixed infrastructure is assumed or relied upon. An important application domain of MANETês is local resource discovery. In a local resource discovery application, a user finds local resources that satisfy specified criteria. For example, a driver finds an available parking slot in a region by receiving information generated by the parking meter, or gets the traffic conditions on a highway segment a mile ahead; a cab driver finds a near-by customer, or a participant at a convention finds another participant with a matching profile."
7DF149CB,International Conference on Data Engineering,xinqiang zuo + jianmin wang + kwokyan lam + jiaguang sun + xiaoming jin,2006,Efficient Discovery of Emerging Frequent Patterns in ArbitraryWindows on Data Streams,synthetic data + data mining + NonControlled Keywords Not Found + search space + Controlled Keywords Not Found + data engineering + sun,AuthorProvided Keywords Not Found,"This paper proposes an effective data mining technique for finding useful patterns in streaming sequences. At present, typical approaches to this problem are to search for patterns in a fixed-size window sliding through the stream of data being collected. The practical values of such approaches are limited in that, in typical application scenarios, the patterns are emerging and it is difficult, if not impossible, to determine a priori a suitable window size within which useful patterns may exist. It is therefore desirable to devise techniques that can identify useful patterns with arbitrary window sizes. Attempts to this problem are challenging, however, because it requires a highly efficient searching in a substantially bigger solution space. This paper presents a new method which includes firstly a pruning strategy to reduce the search space and secondly a mining strategy that adopts a dynamic index structure to allow efficient discovery of emerging patterns in a streaming sequence. Experimental results on real data and synthetic data show that the proposed method outperforms other existing schemes both in computational efficiency and effectiveness in finding useful patterns."
7EDDC624,International Conference on Data Engineering,naga k govindaraju + dinesh manocha + anastassia ailamaki,2006,Query Co-Processing on Commodity Hardware,memory bandwidth + chip + data management + bandwidth + computer networks + computer architecture + data mining + high performance computing + NonControlled Keywords Not Found + graphics + Controlled Keywords Not Found + hardware,AuthorProvided Keywords Not Found,"The rapid increase in the data volumes for the past few decades has intensified the need for high processing power for database and data mining applications. Researchers have actively sought to design and develop new architectures for improving the performance. Recent research shows that the performance can be significantly improved using either (a) effective utilization of architectural features and memory hierarchies used by the conventional processors, or (b) the high computational power and memory bandwidth in commodity hardware such as network processing units (NPUs), and graphics processing units (GPUs). This seminar will survey the micro-architectural and architectural differences across these processors with data management in mind, and will present previous work and future opportunities for expanding query processing algorithms to other hardware than general-purpose processors. In addition to the database community, we intend to increase awareness in the computer architecture scene about opportunities to construct heterogeneous chips (chip multiprocessors with different architectures in them)."
7D66CFCF,International Conference on Data Engineering,matthias renz + martin pfeifle + peter kunath + hanspeter kriegel,2006,ViEWNet: Visual Exploration of Region-Wide Traffic Networks,visualization + data analysis + prototypes + algorithm design and analysis + location based service + data mining + NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Location-based services and data mining algorithms analyzing objects moving on a complex traffic network are becoming increasingly important. In this paper, we introduce a new approach which effectively and efficiently detects dense areas in spatial networks. In an offline phase, we generate a hierarchical partitioning of the traffic network. Thereby, static entities like roads and buildings are likely to be in the same partitioning if they are close to each other according to their network distance. In the online phase, our prototype ViEWNet allows the effective and efficient monitoring of objects moving on a spatial network. Based on a clear visualization of the traffic intensity in each network cell, the user can easily detect hierarchies of dense areas by our powerful prototype ViEWNet."
7F77602C,International Conference on Data Engineering,gagan agrawal + leo glimcher + christopher jermaine + ruoming jin,2006,New Sampling-Based Estimators for OLAP Queries,sampling methods + histograms + data analysis + sociology + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + hardware + statistics,AuthorProvided Keywords Not Found,"One important way in which sampling for approximate query processing in a database environment differs from traditional applications of sampling is that in a database, it is feasible to collect accurate summary statistics from the data in addition to the sample. This paper describes a set of sampling-based estimators for approximate query processing that make use of simple summary statistics to to greatly increase the accuracy of sampling-based estimators. Our estimators are able to give tight probabilistic guarantees on estimation accuracy. They are suitable for low or high dimensional data, and work with categorical or numerical attributes. Furthermore, the information used by our estimators can easily be gathered in a single pass, making them suitable for use in a streaming environment."
7E047EBE,International Conference on Data Engineering,elisa bertino + alberto trombetta,2006,Private Updates to Anonymous Databases,databases + data management + data security + privacy + cryptography + history + secure + content management + secure computation + database management + Privacy + computer science + anonymity + data privacy,AuthorProvided Keywords Not Found,"Suppose that Alice, owner of a k-anonymous database, needs to determine whether her database, when adjoined with a tuple owned by Bob, is still k-anonymous. Suppose moreover that access to the database is strictly controlled, because for example data are used for experiments that need to be maintained confidential. Clearly, allowing Alice to directly read the contents of the tuple breaks the privacy of Bob; on the other hand, the confidentiality of the database managed by Alice is violated once Bob has access to the contents of the database. Thus the problem is to check whether the database adjoined with the tuple is still k-anonymous, without letting Alice and Bob know the contents of, respectively, the tuple and the database. In this paper, we propose two protocols solving this problem."
7EB7DC25,International Conference on Data Engineering,yihua wu + s muthukrishnan + flip korn,2006,Fractal Modeling of IP Network Traffic at Streaming Speeds,histograms + data analysis + data mining + information analysis + NonControlled Keywords Not Found + Controlled Keywords Not Found + fractals,AuthorProvided Keywords Not Found,"This paper describes how to fit fractal models, online, on IP traffic data streams. Our approach relies on maintaining a sketch of the data stream and fitting straight lines: it yields algorithms that are fast, space-efficient, and accurate. We implemented our methods in AT&Tês Gigascope data stream management system, to demonstrate their practicality at streaming line speeds."
7D5D554A,International Conference on Data Engineering,kanad dixit + michael j carey + sachin thatte + vinayak borkar + sujeet banerjee + sunil jigyasu + anil malkani,2006,SQL to XQuery Translation in the AquaLogic Data Services Platform,data exchange + web services + xml + information retrieval + relational database system + NonControlled Keywords Not Found + service oriented architecture + Controlled Keywords Not Found + relational databases + data engineering + digital signal processing,AuthorProvided Keywords Not Found,"SQL has long been the standard language for retrieving and manipulating data in relational database systems. XML has become the standard format for data exchange, and XQuery is on its way to becoming the standard language for querying XML data. The BEA AquaLogic Data Services Platform provides a service-oriented, XML-based view of heterogeneous enterprise data sources and allows this view to be queried using XQuery. AquaLogic DSP includes a JDBC driver that connects the old (SQL) world with the new (XML) world via a SQL-to-XQuery translator. This paper outlines the issues related to creating such a driver and details the approach used to translate SQL queries into XQuery expressions. The paper also touches on performance considerations related to handling XML query results in a context where JDBC result sets are the desired output format."
7F801D02,International Conference on Data Engineering,mohammed k jaber + andrei voronkov,2006,UNIDOOR: a Deductive Object-Oriented Database Management System,data model + database languages + database integration + object oriented + management system + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + data models + database management + database system + query language + database systems,AuthorProvided Keywords Not Found,"In this paper, we present UNIDOOR, a deductive objectoriented database system (DOOD). We demonstrate the distinctive features of UNIDOOR data model and its query language. We then show how essential object-oriented and database management features, that were missing from other DOOD implementations, are successfully supported in UNIDOOR. These features include a scalable persistent store with crash recovery, database integrity and transaction control facilities in a multi-user environment."
7911922A,International Conference on Data Engineering,juliana freire + carlos e scheidegger + claudio t silva + emanuele santos + huy t vo + steven p callahan,2006,Managing the Evolution of Dataflows with VisTrails,navigation + history + data visualization + data analysis + pipelines,,"s w f a t eh w i t"
7D6614ED,International Conference on Data Engineering,ralf rantzau + christoph mangold,2006,Laws for Rewriting Queries Containing Division Operators,query optimization + relation algebra + data mining + engines + testing + cost function + relational database system + algebra + NonControlled Keywords Not Found + Controlled Keywords Not Found + normal form + relational databases,AuthorProvided Keywords Not Found,"Relational division, also known as small divide, is derived operator of the relational algebra that realizes many-to-one set containment test, where a set is represented as a group of tuples: Small divide discovers which sets in a dividend relation contain all elements of the set stored in a divisor relation. The great divide operator extends small divide by realizing many-to-many set containment tests. It is also similar to the set containment join operator for schemas that are not in first normal form. Neither small nor great divide has been implemented in commercial relational database systems although the operators solve important problems and many efficient algorithms for them exist. We present algebraic laws that allow rewriting expressions containing small or great divide, illustrate their importance for query optimization, and discuss the use of great divide for frequent itemset discovery, an important data mining primitive. A recent theoretic result shows that small divide must be implemented by special purpose algorithms and not be simulated by pure relational algebra expressions to achieve efficiency. Consequently, an efficient implementation requires that the optimizer treats small divide as a first-class operator and possesses powerful algebraic laws for query rewriting."
7E09438A,International Conference on Data Engineering,s muthukrishnan + wei zhuang + graham cormode,2006,"What&#146;s Different: Distributed, Continuous Monitoring of Duplicate-Resilient Aggregates on Data Streams",resource management + capacitive sensors + synthetic data + energy management + distributed environment + NonControlled Keywords Not Found + Controlled Keywords Not Found + wild animal,AuthorProvided Keywords Not Found,"Emerging applications in sensor systems and network-wide IP traffic analysis present many technical challenges. They need distributed monitoring and continuous tracking of events. They have severe resource constraints not only at each site in terms of per-update processing time and archival space for highspeed streams of observations, but also crucially, communication constraints for collaborating on the monitoring task. These elements have been addressed in a series of recent works. A fundamental issue that arises is that one cannot make the ""uniqueness"" assumption on observed events which is present in previous works, since widescale monitoring invariably encounters the same events at different points. For example, within the network of an Internet Service Provider packets of the same flow will be observed in different routers; similarly, the same individual will be observed by multiple mobile sensors in monitoring wild animals. Aggregates of interest on such distributed environments must be resilient to duplicate observations. We study such duplicate-resilient aggregates that measure the extent of the duplication_how many unique observations are there, how many observations are unique_as well as standard holistic aggregates such as quantiles and heavy hitters over the unique items. We present accuracy guaranteed, highly communication-efficient algorithms for these aggregates that work within the time and space constraints of high speed streams. We also present results of a detailed experimental study on both real-life and synthetic data."
7F4EDF6B,International Conference on Data Engineering,anthony k h tung + ying lu + wei wang + xin xu,2006,Mining Shifting-and-Scaling Co-Regulation Patterns on Gene Expression Profiles,coherence + clustering algorithms + satisfiability + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering + gene expression,AuthorProvided Keywords Not Found,"In this paper, we propose a new model for coherent clustering of gene expression data called reg-cluster. The proposed model allows (1) the expression profiles of genes in a cluster to follow any shifting-and-scaling patterns in subspace, where the scaling can be either positive or negative, and (2) the expression value changes across any two conditions of the cluster to be significant. No previous work measures up to the task that we have set: the density-based subspace clustering algorithms require genes to have similar expression levels to each other in subspace; the pattern-based biclustering algorithms only allow pure shifting or pure scaling patterns; and the tendency-based biclustering algorithms have no coherence guarantees. We also develop a novel patternbased biclustering algorithm for identifying shifting-andscaling co-regulation patterns, satisfying both coherence constraint and regulation constraint. Our experimental results show that the reg-cluster algorithm is able to detect a significant amount of clusters missed by previous models, and these clusters are potentially of high biological significance."
80D81202,International Conference on Data Engineering,roland h c yap + kenny q zhu + joxan jaffar,2006,Indexing for Dynamic Abstract Regions,minimum bounding rectangle + indexing + shape + life span + indexation + economic indicators + geographic information systems + NonControlled Keywords Not Found + Controlled Keywords Not Found + multidimensional systems,AuthorProvided Keywords Not Found,"We propose a new main memory index structure for abstract regions (objects) which may heavily overlap, the RCtree. These objects are ""dynamic"" and may have short life spans. The novelty is that rather than representing an object by its minimum bounding rectangle (MBR), possibly with pre-processed segmentation into many small MBRs, we use the actual shape of the object to maintain the index. This saves significant space for objects with large spatial extents since pre-segmentation is not needed. We show that the query performance of RC-tree is much better than many indexing schemes on synthetic overlapping data sets. The performance is also competitive on real-life GIS nonoverlapping data sets."
7F931841,International Conference on Data Engineering,shuang hou + hansarno jacobsen,2006,Predicate-based Filtering of XPath Expressions,routing + algorithm design and analysis + engines + xml + xml document + NonControlled Keywords Not Found + Controlled Keywords Not Found + automata + scalability,AuthorProvided Keywords Not Found,"The XML/XPath filtering problem has found wide-spread interest. In this paper, we propose a novel algorithm for solving it. Our approach encodes XPath expressions (XPEs) as ordered sets of predicates and translates XML documents into sets of tuples, which are evaluated over these predicates. Predicates representing overlapping portions of XPEs are stored and processed once, thus fully exploiting potential overlap in XPEs. We experimentally evaluate the performance of our algorithm, demonstrating its scalability to millions of XPEs, with matching performance in the millisecond range. We show interesting trade-offs to alternative approaches."
7E84E234,International Conference on Data Engineering,zhaohui wu + yuxin mao + heng wang + huajun chen,2006,RDF/RDFS-based Relational Database Integration,algorithm design and analysis + semantic web + NonControlled Keywords Not Found + relational database + Controlled Keywords Not Found + relational databases + traditional chinese medicine + data engineering + resource description framework + relational data + computer science + xml + natural languages + relational model + ontologies,AuthorProvided Keywords Not Found,"We study the problem of answering queries through a RDF/RDFS ontology, given a set of view-based mappings between one or more relational schemas and this target ontology. Particularly, we consider a set of RDFS semantic constraints such as rdfs:subClassof, rdfs:subPropertyof, rdfs:domain, and rdfs:range, which are present in RDF model but neither XML nor relational models. We formally define the query semantics in such an integration scenario, and design a novel query rewriting algorithm to implement the semantics. On our approach, we highlight the important role played by RDF Blank Node in representing incomplete semantics of relational data. A set of semantic tools supporting relational data integration by RDF are also introduced. The approach have been used to integrate 70 relational databases at China Academy of Traditional Chinese Medicine."
7FB72BF0,International Conference on Data Engineering,mohamed f mokbel,2006,Towards Privacy-Aware Location-Based Database Servers,trusted third party +  + databases + computer science + handheld device + location based service + private information + privacy + point location + web server,,"The wide spread of location-based services results in a strong market for location-detection devices (e.g., GPS-like devices, RFIDs, handheld devices, and cellular phones). Examples of location-based services include location-aware emergency service, location-based advertisement, live tra c reports, and location-based store nder. However, location-detection devices pose a major privacy threat on its users where it transmits private information (i.e., the location) to the server who may be untrustworthy. The existing model of location-based applications trades service with privacy where if a user wants to keep her private location information, she has to turn o her location-detection device, i.e., unsubscribe from the service. This paper tackles this model in a way that protects the user privacy while keeping the functionality of location-based services. The main idea is to employ a trusted third party, the Location Anonymizer, that expands the user location into a spatial region such that: (1) The exact user location can lie anywhere in the spatial region, and (2) There are k other users within the expanded spatial region so that each user is k-anonymous. The location-based database server is equipped with additional functionalities that support spatio-temporal queries based on the spatial region received from the location anonymizer rather than the exact point location received from the user."
7FC64B48,International Conference on Data Engineering,pavan k chowdary + jayant r haritsa + a kumaran,2006,On Pushing Multilingual Query Operators into Relational Engines,search engines + prototypes + indexation + algebra + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + associative algebra + database system + query optimization + natural languages + internet + natural language + database systems,AuthorProvided Keywords Not Found,"To effectively support todayês global economy, database systems need to manage data in multiple languages simultaneously. While current database systems do support the storage and management of multilingual data, they are not capable of querying across different natural languages. To address this lacuna, we have recently proposed two cross-lingual functionalities, LexEQUAL[13] and SemEQUAL[14], for matching multilingual names and concepts, respectively. In this paper, we investigate the native implementation of these multilingual functionalities as first-class operators on relational engines. Specifically, we propose a new multilingual storage datatype, and an associated algebra of the multilingual operators on this datatype. These components have been successfully implemented in the PostgreSQL database system, including integration of the algebra with the query optimizer and inclusion of a metric index in the access layer. Our experiments demonstrate that the performance of the native implementation is up to two orders-of-magnitude faster than the corresponding outsidethe- server implementation. Further, these multilingual additions do not adversely impact the existing functionality and performance. To the best of our knowledge, our prototype represents the first practical implementation of a crosslingual database query engine."
80D0AC20,International Conference on Data Engineering,xiaopeng xiong + walid g aref,2006,R-trees with Update Memos,application software + indexing + indexation + degradation + NonControlled Keywords Not Found + Controlled Keywords Not Found + r tree + data engineering,AuthorProvided Keywords Not Found,"The problem of frequently updating multi-dimensional indexes arises in many location-dependent applications. While the R-tree and its variants are one of the dominant choices for indexing multi-dimensional objects, the R-tree exhibits inferior performance in the presence of frequent updates. In this paper, we present an R-tree variant, termed the RUM-tree (stands for R-tree with Update Memo) that minimizes the cost of object updates. The RUM-tree processes updates in a memo-based approach that avoids disk accesses for purging old entries during an update process. Therefore, the cost of an update operation in the RUM-tree reduces to the cost of only an insert operation. The removal of old object entries is carried out by a garbage cleaner inside the RUM-tree. In this paper, we present the details of the RUM-tree and study its properties. Theoretical analysis and experimental evaluation demonstrate that the RUMtree outperforms other R-tree variants by up to a factor of eight in scenarios with frequent updates."
7E84D5C5,International Conference on Data Engineering,zhiping zeng + l zhou + jianyong wang,2006,CLAN: An Algorithm for Mining Closed Cliques from Large Dense Graph Databases,databases + protein engineering + computer science + data mining + topology + computer applications + testing + coherent structures + NonControlled Keywords Not Found + search space + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Most previously proposed frequent graph mining algorithms are intended to find the complete set of all frequent, closed subgraphs. However, in many cases only a subset of the frequent subgraphs with a certain topology is of special interest. Thus, the method of mining the complete set of all frequent subgraphs is not suitable for mining these frequent subgraphs of special interest as it wastes considerable computing power and space on uninteresting subgraphs. In this paper we develop a new algorithm, CLAN, to mine the frequent closed cliques, the most coherent structures in the graph setting. By exploring some properties of the clique pattern, we can simplify the canonical label design and the corresponding clique (or subclique) isomorphism testing. Several effective pruning methods are proposed to prune the search space, while the clique closure checking scheme is used to remove the non-closed clique patterns. Our empirical results show that CLAN is very efficient for large dense graph databases with which the traditional graph mining algorithms fail. The novelty of our method is further demonstrated by the application of CLAN in mining highly correlated stocks from large stock market data."
7ECD4DA0,International Conference on Data Engineering,fatih emekci + aziz gulbeden + amr el abbadi + divyakant agrawal,2006,Privacy Preserving Query Processing Using Third Parties,potential well + databases + law + data integrity + data privacy + NonControlled Keywords Not Found + Controlled Keywords Not Found + protocols,AuthorProvided Keywords Not Found,"Data integration from multiple autonomous data sources has emerged as an important practical problem. The key requirement for such data integration is that owners of such data need to cooperate in a competitive landscape in most of the cases. The research challenge in developing a query processing solution is that the answers to the queries need to be provided while preserving the privacy of the data sources. In general, allowing unrestricted read access to the whole data may give rise to potential vulnerabilities as well as may have legal implications. Therefore, there is a need for privacy preserving database operations for querying data residing at different parties. In this paper, we propose a new query processing technique using third parties in a peer-to-peer system. We propose and evaluate two different protocols for various database operations. Our scheme is able to answer queries without revealing any useful information to the data sources or to the third parties. Analytical comparison of the proposed approach with other recent proposals for privacy-preserving data integration establishes the superiority of the proposed approach in terms of query response time"
7DB51579,International Conference on Data Engineering,pinar senkul,2006,CompositeWeb Service Construction by Using a Logical Formalism,web service + web services + data engineering + service model,,""
7D2D6E0E,International Conference on Data Engineering,tian xia + donghui zhang,2006,Continuous Reverse Nearest Neighbor Monitoring,application software + virtual reality + a priori knowledge + recurrent neural networks + wireless networks + NonControlled Keywords Not Found + Controlled Keywords Not Found + information science + range query,AuthorProvided Keywords Not Found,"Continuous spatio-temporal queries have recently received increasing attention due to the abundance of location-aware applications. This paper addresses the Continuous Reverse Nearest Neighbor (CRNN) Query. Given a set of objects O and a query set Q, the CRNN query monitors the exact reverse nearest neighbors of each query point, under the model that both the objects and the query points may move unpredictably. Existing methods for the reverse nearest neighbor (RNN) query either are static or assume a priori knowledge of the trajectory information, and thus do not apply. Related recent work on continuous range query and continuous nearest neighbor query relies on the fact that a simple monitoring region exists. Due to the unique features of the RNN problem, it is non-trivial to even define a monitoring region for the CRNN query. This paper defines the monitoring region for the CRNN query, discusses how to perform initial computation, and then focuses on incremental CRNN monitoring upon updates. The monitoring region according to one query point consists of two types of regions. We argue that the two types should be handled separately. In continuous monitoring, two optimization techniques are proposed. Experimental results prove that our proposed approach is both efficient and scalable."
7DE07E8C,International Conference on Data Engineering,dongdong zhang + weiping wang + jianzhong li + kimutai kimeli,2006,SlidingWindow based Multi-Join Algorithms over Distributed Data Streams,computer science + NonControlled Keywords Not Found + Controlled Keywords Not Found + sliding window,AuthorProvided Keywords Not Found,"This paper focuses on multi-way sliding window join (SWJoin) processing over distributed data streams. A novel Join algorithm is proposed based on two distributed data stream transfer models. To reduce the communication cost and lighten the workload on the central processor node, the algorithm filters out tuples that canêt contribute to multiway SWJoin results by transforming the join conditions of SWJoin into filtering conditions during data stream transfer. Furthermore, the algorithm guarantees that all necessary data for generating exact multi-way SWJoin results can be transmitted to the central processor node."
8129B46F,International Conference on Data Engineering,renee j miller + periklis andritsos + ariel fuxman,2006,Clean Answers over Dirty Databases: A Probabilistic Approach,databases + project management + merging + NonControlled Keywords Not Found + Controlled Keywords Not Found + customer relationship management,AuthorProvided Keywords Not Found,"The detection of duplicate tuples, corresponding to the same real-world entity, is an important task in data integration and cleaning. While many techniques exist to identify such tuples, the merging or elimination of duplicates can be a difficult task that relies on ad-hoc and often manual solutions. We propose a complementary approach that permits declarative query answering over duplicated data, where each duplicate is associated with a probability of being in the clean database. We rewrite queries over a database containing duplicates to return each answer with the probability that the answer is in the clean database. Our rewritten queries are sensitive to the semantics of duplication and help a user understand which query answers are most likely to be present in the clean database. The semantics that we adopt is independent of the way the probabilities are produced, but is able to effectively exploit them during query answering. In the absence of external knowledge that associates each database tuple with a probability, we offer a technique, based on tuple summaries, that automates this task. We experimentally study the performance of our rewritten queries. Our studies show that the rewriting does not introduce a significant overhead in query execution time. This work is done in the context of the ConQuer project at the University of Toronto, which focuses on the efficient management of inconsistent and dirty databases."
803AC68E,International Conference on Data Engineering,minos garofalakis + neoklis polyzotis,2006,XCluster Synopses for Structured XML Content,data models + numerical range + databases + xml + cost function + xml document + NonControlled Keywords Not Found + Controlled Keywords Not Found + xml database + data engineering + internet,AuthorProvided Keywords Not Found,"We tackle the difficult problem of summarizing the path/branching structure and value content of an XML database that comprises both numeric and textual values. We introduce a novel XML-summarization model, termed XCLUSTERs, that enables accurate selectivity estimates for the class of twig queries with numeric-range, substring, and textual IR predicates over the content of XML elements. In a nutshell, an XCLUSTER synopsis represents an effective clustering of XML elements based on both their structural and value-based characteristics. By leveraging techniques for summarizing XML-document structure as well as numeric and textual data distributions, our XCLUSTER model provides the first known unified framework for handling path/branching structure and different types of element values. We detail the XCLUSTER model, and develop a systematic framework for the construction of effective XCLUSTER summaries within a specified storage budget. Experimental results on synthetic and real-life data verify the effectiveness of our XCLUSTER synopses, clearly demonstrating their ability to accurately summarize XML databases with mixed-value content. To the best of our knowledge, ours is the first work to address the summarization problem for structured XML content in its full generality."
7ED2D310,International Conference on Data Engineering,nicole alexander + siva ravada,2006,RDF Object Type and Reification in the Database,data model + uniform resource identifier + web pages + capacitive sensors + data structure + information retrieval + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + resource description framework + data models + digital library + directed graph + data structures,AuthorProvided Keywords Not Found,"The Resource Description Framework (RDF) is a standard for representing information that can be identified using a Uniform Resource Identifier. In particular, it is intended for representing metadata about Web resources. RDF is being used in numerous application areas, including Life Sciences, Digital Libraries, and Intelligence. The RDF data structure is a directed graph or network. Current solutions to managing RDF data utilize flat relational tables for database storage. This paper presents an alternative approach to managing RDF data in the database. We introduce a new object type for storing RDF data. The object type is built on top of the Oracle Spatial Network Data Model (NDM), which is Oracleês network solution in the database. This exposes the NDM functionality to RDF data, allowing RDF data to be managed as objects and analyzed as networks. Reification, a means of providing metadata for the RDF data, puts a strain on storage. We present a streamlined approach to representing reified RDF data for faster retrievals. An RDF object type and reification in the database provide the basic infrastructure for effective metadata management."
7E5258F5,International Conference on Data Engineering,bin cui + john shepherd + kianlee tan + jialie shen,2006,HSI: A Novel Framework for Efficient Automated Singer Identification in Large Music Database,information system + recommender systems + recommender system + database system + indexing + multiple signal classification + NonControlled Keywords Not Found + Controlled Keywords Not Found + scalability + database systems,AuthorProvided Keywords Not Found,"The singerês information is essential in organising, browsing and exploring music data. As an important component of music database systems, the automated artist identification is gaining considerable momentum due to numerous potential applications including music indexing and retrieval, copy right management and music recommendation systems. Unfortunately, the most currently employed approaches are still in their infancy and the performance is by far less satisfactory. Indeed, they suffer from low effectiveness, less robustness and poor scalability to accommodate large scale of data. In this demo, we presents a novel system, called Hybrid Singer Identifier (HSI), for efficient and effective automated singer identification in large music databases."
7F0221FF,International Conference on Data Engineering,anastasios gounaris + j f smith + norman w paton + paul watson + alvaro a a fernandes + rizos sakellariou,2006,Practical Adaptation to Changing Resources in Grid Query Processing,resource management + bandwidth + grid computing + NonControlled Keywords Not Found + throughput + availability + Controlled Keywords Not Found + middleware,AuthorProvided Keywords Not Found,"Grid computational resources, as well as being heterogeneous, may also exhibit unpredictable, volatile behaviour. Therefore, query processing on the Grid needs to be adaptive in order to cope with evolving resource characteristics, such as machine load and availability. To address this challenge in a Grid environment, the non-adaptive OGSA-DQP1 system described in [1] has been enhanced with adaptive capabilities."
7E2346DE,International Conference on Data Engineering,traian marius truta + bindu vinay,2006,Privacy Protection: p-Sensitive k-Anonymity Property, + databases + computer science + data mining + privacy + biomedical imaging + history + statistical analysis,,"In this paper, we introduce a new privacy protection property called p-sensitive k-anonymity. The existing kanonymity property protects against identity disclosure, but it fails to protect against attribute disclosure. The new introduced privacy model avoids this shortcoming. Two necessary conditions to achieve p-sensitive kanonymity property are presented, and used in developing algorithms to create masked microdata with p-sensitive k-anonymity property using generalization and suppression."
7FB5E29D,International Conference on Data Engineering,jochen haller,2006,A Stochastic Approach for Trust Management,business process + access control + decision support + stochastic processes + collaboration + security + disaster management + stochastic modelling + service oriented architecture + predictive models,,"Business collaborations in Virtual Organisations (VOs) demand sound security infrastructures to achieve acceptance from industry. It becomes apparent that in dynamic environments requiring swift decisions as well as quickly adapted and connected applications, traditional hard security e.g. based on static access control is no longer able to cope with security alone to meet the collaboration objective. Soft security based on trust management, e.g. provided by a reputation system, is able to address those requirements dealing with previously unknown entities and hereby complement, but not replace, established static hard security. This paper introduces a novel approach and proposed architecture for a trust management system providing reputation, rooting trust in a collaborating entitys inherently characterizing trust parameters. Those parameters are stochastically modelled aiming at capturing the real parameters behaviour and aggregated with stochastic system theory. This approach is believed to retain the quality of a model with implicit semantic behaviour of trust parameters from the root of trust, the observed parameters, to the aggregated reputation provided as decision support to applications."
816EB6F5,International Conference on Data Engineering,frederick h lochovsky + jiying wang + weifeng su,2006,Holistic Query Interface Matching using Parallel Schema Matching,impedance matching + databases + pattern matching + greedy algorithms + polynomials + data mining + information retrieval + NonControlled Keywords Not Found + Controlled Keywords Not Found + greedy algorithm + data engineering,AuthorProvided Keywords Not Found,"Using query interfaces of different Web databases, we propose a new complex schema matching approach, Parallel Schema Matching (PSM). A parallel schema is formed by comparing two individual schemas and deleting common attributes. The attribute matching can be discovered from the attribute-occurrence patterns if many parallel schemas are available. A count-based greedy algorithm identifies which attributes are more likely to be matched. Experiments show that PSM can identify both simple matching and complex matching accurately and efficiently."
807B892F,International Conference on Data Engineering,simon sheu + nigel wu,2006,XCut: Indexing XML Data for Efficient Twig Evaluation,pattern matching + filtration + indexing + xml + indexation + xml document + NonControlled Keywords Not Found + information systems + tree data structures + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Efficient evaluation of XML queries entails fast finding all the occurrences of a twig pattern, a subgraph of treestructured XML documents. One approach models the pattern as a query tree directly, and associates a cursor and a stack to each tree node for coordinated sequential access and partial match tally of all the XML elements. Even if pre-built indices on the elements can facilitate skipping mismatches, this approach works well only when the pattern purely involves ancestor-descendent edges. Another strategy first transforms XML data and query trees one-to-one into sequences. Then, string indices are used for subsequence matching to achieve equivalent skips in transform domain. This strategy performs better when the twig contains ordered parent-child edges and nodes with their own selection predicates. This paper proposes to replace the stacks used in the first approach by simple pipes (double-ended queues) to queue too inner entries of index trees accessed by level-order traversal. Our design permits the cross-verification among queued entries subject to the twig pattern. Early filtration at the inner level of index trees is thus achievable to further expedite processing. Joint with the leaf entries having full specifications of query elements, our design can also suppress any subgraph that violates parent-child edges and selection predicates at query tree nodes. Intensive experimental results also demonstrate such superiority over the second strategy."
7DEA103D,International Conference on Data Engineering,jun rao + guy m lohman + chilukuri k mohan + hamid pirahesh,2006,Compiled Query Execution Engine using JVM,database system + java + support vector machines + virtual machine + prototypes + engines + NonControlled Keywords Not Found + Controlled Keywords Not Found + relational databases + dynamic compilation + database systems,AuthorProvided Keywords Not Found,"A conventional query execution engine in a database system essentially uses a SQL virtual machine (SVM) to interpret a dataflow tree in which each node is associated with a relational operator. During query evaluation, a single tuple at a time is processed and passed among the operators. Such a model is popular because of its efficiency for pipelined processing. However, since each operator is implemented statically, it has to be very generic in order to deal with all possible queries. Such generality tends to introduce significant runtime inefficiency, especially in the context of memory-resident systems, because the granularity of data commercial system, using SVM. processing (a tuple) is too small compared with the associated overhead. Another disadvantage in such an engine is that each operator code is compiled statically, so query-specific optimization cannot be applied. To improve runtime efficiency, we propose a compiled execution engine, which, for a given query, generates new query-specific code on the fly, and then dynamically compiles and executes the code. The Java platform makes our approach particularly interesting for several reasons: (1) modern Java Virtual Machines (JVM) have Just- In-Time (JIT) compilers that optimize code at runtime based on the execution pattern, a key feature that SVMs lack; (2) because of Javaês continued popularity, JVMs keep improving at a faster pace than SVMs, allowing us to exploit new advances in the Java runtime in the future; (3) Java is a dynamic language, which makes it convenient to load a piece of new code on the fly. In this paper, we develop both an interpreted and a compiled query execution engine in a relational, Java-based, in-memory database prototype, and perform an experimental study. Our experimental results on the TPC-H data set show that, despite both engines benefiting from JIT, the compiled engine runs on average about twice as fast as the interpreted one, and significantly faster than an in-memory"
7DB15F07,International Conference on Data Engineering,jiawei han + philip s yu + xifeng yan,2006,"Mining, Indexing, and Similarity Search in Graphs and Complex Structures",application software + similarity search + complex structure + data management + indexing + data mining + data structure + NonControlled Keywords Not Found + Controlled Keywords Not Found + tree graphs + biological network + database management + clustering algorithms + intelligent networks + chemical structure + lattices + protein sequence,AuthorProvided Keywords Not Found,"Scalable methods for mining, indexing, and similarity search in graphs and other complex structures, such as trees, lattices, and networks, have become increasingly important in data mining and database management. This is because a large set of emerging applications need to handle new kinds of objects with complex structures, such as trees (e.g., XML data), graphs (e.g., Web, chemical structures and biological graphs) and networks (e.g., social and biological networks). Such complicated data structures pose many new challenging research problems related to data mining, data management, and similarity search that do not exist in the traditional database and data mining studies."
7D55A677,International Conference on Data Engineering,johannes gehrke + mirek riedewald + jayavel shanmugasundaram + fan yang,2006,Hilda: A High-Level Language for Data-DrivenWeb Applications,data model + high level language + web pages + technical report + high level languages + NonControlled Keywords Not Found + proof of concept + Controlled Keywords Not Found + relational databases + data models + java + computer science + logic + database systems,AuthorProvided Keywords Not Found,"We propose Hilda, a high-level language for developing data-driven web applications. The primary benefits of Hilda over existing development platforms are: (a) it uses a unified data model for all layers of the application, (b) it is declarative, (c) it models both application queries and updates, (d) it supports structured programming for web sites, and (e) it enables conflict detection for concurrent updates. We also describe the implementation of a simple proof-ofconcept Hilda compiler, which translates a Hilda application program into Java Servlet code."
7FCC4DA7,International Conference on Data Engineering,farnoush banaeikashani + cyrus shahabi,2006,Partial Selection Query in Peer-to-Peer Databases,query optimization + indexes + satisfiability + distributed databases + information retrieval + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"In this paper, we propose DBSampler, a query execution mechanism to answer ""partial selection"" queries in peerto- peer databases. A partial selection query is an arbitrary selection query that is satisfied with a fraction in of the results; a universal operation with applications in database tuning, query optimization and approximate query processing in peer-to-peer databases. DBSampler is based on an epidemic dissemination algorithm. We model the epidemic dissemination as a percolation problem and by rigorous percolation analysis tune DBSampler per-query and on-thefly to answer partial queries correctly and efficiently. We verify the efficiency of DBSampler in terms of query cost and query time via extensive simulation."
80CA6268,International Conference on Data Engineering,m del mar roldangarcia + jose f aldanamontes,2006,A Survey on Disk Oriented Querying and Reasoning on the Semantic Web,databases + knowledge representation + artificial intelligence + data models + data model + knowledge base + management system + ontologies + computer languages + resource description framework + physical design + logic + semantic web + owl + indexation,,"This paper presents a description of seven systems, which use database technology to both represent knowledge persistently and make scalable queries on it, in the Semantic Web context. From the study of these systems we can deduce that a lot of work regarding massive storage of knowledge has already been carried out. We can observe an evolution from RDF to OWL in most up-To-date systems. The analyzed approaches carry out a mapping between the (structure of the) ontology and the data model of the persistence management system. Nevertheless, there is still considerable work to be done regarding efficient disk oriented Abox (ontology instances) querying and reasoning. We think that due to the Semantic Web features many problems related to assertional reasoning should be addressed. We also believe that the physical design of the knowledge base is a very important task, which includes the development of both specific storage structures and indexes for knowledge storage and retrieval."
8077F348,International Conference on Data Engineering,davood rafiei + reza sherkat,2006,Efficiently Evaluating Order Preserving Similarity Queries over Historical Market-Basket Data,web pages + synthetic data + data mining + time series + upper bound + NonControlled Keywords Not Found + hydrogen + warehousing + history + Controlled Keywords Not Found + internet,AuthorProvided Keywords Not Found,"We introduce a new domain-independent framework for formulating and efficiently evaluating similarity queries over historical data, where given a history as a sequence of timestamped observations and the pair-wise similarity of observations, we want to find similar histories. For instance, given a database of customer transactions and a time period, we can find customers with similar purchasing behaviors over this period. Our work is different from the work on retrieving similar time series; it addresses the general problem in which a history cannot be modeled as a time series, hence the relevant conventional approaches are not applicable. We derive a similarity measure for histories, based on an aggregation of the similarities between the observations of the two histories, and propose efficient algorithms for finding an optimal alignment between two histories. Given the non-metric nature of our measure, we develop some upper bounds and an algorithm that makes use of those bounds to prune histories that are guaranteed not to be in the answer set. Our experimental results on real and synthetic data confirm the effectiveness and efficiency of our approach. For instance, when the minimum length of a match is provided, our algorithm achieves up to an order of magnitude speed-up over alternative methods."
8123E44F,International Conference on Data Engineering,changtien lu + jinping zheng + arnold p boedihardjo,2006,AITVS: Advanced Interactive Traffic Visualization System,visual system + visualization + demand forecasting + traffic flow + spine + NonControlled Keywords Not Found + Controlled Keywords Not Found + economic forecasting,AuthorProvided Keywords Not Found,"Transportation and the highway network form the backbone of the total public infrastructure system. As such, planning and monitoring for an effective transportation system is crucial in the building and maintenance of a regionês economy and safety. However, demand for road travel continues to expand as population increases (particularly in the metropolitan areas) while new constructions have not kept pace. According to the Federal Highway Administration, it is forecasted that the volume of freight movement alone is to nearly double by 2020 [1]. Congestion and looming gridlock crises seriously threaten any regionês mobility, safety and economic vitality. A crucial component in addressing these concerns is the development of specific technologies to monitor, model, and optimize traffic flow."
8023DB90,International Conference on Data Engineering,german shegalov + roger barga + mohamed f mokbel + david lomet,2006,Transaction Time Support Inside a Database Engine,snapshot isolation + indexing + concurrency control + engines + information retrieval + NonControlled Keywords Not Found + history + Controlled Keywords Not Found + database systems + middleware,AuthorProvided Keywords Not Found,"Transaction time databases retain and provide access to prior states of a database. An update ""inserts"" a new record while preserving the old version. Immortal DB builds transaction time database support into a database engine, not in middleware. It supports as of queries returning records current at the specified time. It also supports snapshot isolation concurrency control. Versions are stamped with the ""clock times"" of their updating transactions. The timestamp order agrees with transaction serialization order. Lazy timestamping propagates timestamps to transaction updates after commit. Versions are kept in an integrated storage structure, with historical versions initially stored with current data. Time-splits of pages permit large histories to be maintained, and enable time based indexing, which is essential for high performance historical queries. Experiments show that Immortal DB introduces little overhead for accessing recent database states while providing access to past states."
7ED8BDAB,International Conference on Data Engineering,hiroshi kori + katsumi tanaka + taro tezuka,2006,Ranking of Regional Blogs by Suitability for Sonification,search engine + information systems + filtering + speech synthesis + geographic information systems + search engines + user interfaces + informatics,,"Media content presented to a vehicle driver is mainly auditory, since visual content is distracting and viewing it increases the risk of an accident. Music and radio are thus commonly listened to while driving. However, these types of content rarely reflect regional characteristics and are therefore not well suited for tourists who want to get information about the region they are visiting. We have developed the Blog Car Radio system that presents blog entries in auditory style using sonification (speech synthesis). Blog entries are obtained from blog search engines, selected by distances from the vehicles current location, and ranked based on their suitability for sonification and relevance to the userspecified category. By using Blog Car Radio, a driver can obtain local information with only a small amount of distraction. In this paper, we particularly discussed the method to rank text contents which is suitable for sonification."
8014021D,International Conference on Data Engineering,atsuyuki morishima + shigeo sugimoto + kenichi ishikawa,2006,New Functions of File Systems to Manage Information Shared by Communities,content management + computer integrated manufacturing + information management + data engineering + file servers + information retrieval,,"Today, more and more people in knowledge communities, like research laboratories, use shared file servers to store and share their information. People in such communities often work together and their files stored in a file server have relationships with each other. Information on the relationships is usually exchanged offline and used implicitly to facilitate the management and sharing of the files. This paper proposes new functions to manage and use the relationships to make various views on the file servers. The functions provide a high-level support and are compatible with the operational framework of existing file systems."
80306819,International Conference on Data Engineering,bertram ludascher + terence critchlow + anne h h ngu + shawn bowers,2006,Enabling ScientificWorkflow Reuse through Structured Composition of Dataflow and Control-Flow,layered architecture +  + scientific computing + data engineering + control flow + computer science + fault tolerant + bioinformatics + mathematical model + robust control + genomics + data structures + data flow,,""
7FC537F1,International Conference on Data Engineering,matthias bender + gerhard weikum + sebastian michel,2006,"P2P Directories for Distributed Web Search: From Each According to His Ability, to Each According to His Needs",scalability + web crawling + routing + distributed hash table + search engine + service oriented architecture + search engines + p2p + collaboration + noise reduction + prototypes,,"A compelling application of peer-To-peer (P2P) system technology would be distributed Web search, where each peer autonomously runs a search engine on a personalized local corpus (e.g., built from a thematically focused Web crawl) and peers collaborate by routing queries to remote peers that can contribute many or particularly good results for these specific queries. Such systems typically rely on a decentralized directory, e.g., built on top of a distributed hash table (DHT), that holds compact, aggregated statistical metadata about the peers which is used to identify promising peers for a particular query. To support an a-priori unlimited number of peers, it is crucial to keep the load on the distributed directory low. Moreover, each peer should ideally tailor its postings to the directory to reflect its particular strengths, such as rich information about specialized topics that no or only few other peers would also cover. This paper addresses this problem by proposing strategies for peers that identify suitable subsets of the most beneficial statistical metadata. We argue that posting a carefully selected subset of metadata can achieve almost the same result quality as a complete metadata directory, for only the most relevant peers are eventually involved in the execution of a given query. Additionally, asking only relevant peers will result in higher precision, as the noise introduced by poor peers is reduced. We have implemented these strategies in our fully operational P2P Web search prototype Minerva, and present experimental results on real-world Web data that show the viability of the strategies and their gains in terms of high search result quality at low networking costs."
815B15AE,International Conference on Data Engineering,steven a conrad + c perez de laborda,2006,Bringing Relational Data into the SemanticWeb using SPARQL and Relational.OWL,application software + owl + database languages + ontologies + algebra + resource description framework + computer science + semantic web + data mining + relational database + relational databases + relational data + query language,,"Despite all the efforts to build up a Semantic Web, where each machine can understand and interpret the data it processes, information is usually still stored in ordinary relational databases. Semantic Web applications needing access to such semantically unexploited data, have to create their own manual relational database to Semantic Web mappings. In this paper we analyze, whether the combination of Relational.OWL as a Semantic Web representation of relational databases and a semantic query language like SPARQL could be an alternative. The benefits of such an approach are clear, since it enables Semantic Web applications to access and query data actually stored in relational databases using their own built-in functionality."
800B0266,International Conference on Data Engineering,anand gupta + vikram goyal + s k gupta + indira meshram,2006,PRINDA: Architecture and Design of Non-Disclosure Agreements in Privacy Policy Framework,databases + law + data engineering + management information systems + data privacy + privacy policy + control systems,,"Non Disclosure Agreements(NDAs) in real life are typically used whenever there is transfer of private or confidential information from one organization to another. The provider organization can not have any control over privacy mechanisms of the receiving organization. The privacy policy work so far has addressed itself for preventing privacy violation within an organization. We aim to give an architecture of PRINDA(PRIvacy NDA) system which incorporates NDAs in privacy policy framework. Advantages of PRINDA system will be the following: i) As there can be traces of malicious activity(invasion of privacy) either at provider-end or at recipient-end, if detected/reported, NDA can help to relegate the responsibility to the organization violating the agreement, and will strengthen control in the privacy arena. ii) detailed information of usage of data can be provided to the owner of data e.g. information of all accesses at every level, and hence strengthening the principle of providing the individual data usage information."
7E228AB3,International Conference on Data Engineering,michael i dekhtyar + ionut e iacob + alex dekhtyar,2006,On Potential Validity of Document-Centric XML Documents,computer science + encoding + xml + data engineering + xml document + databases,,"""Document-centric XML document creation is a process of marking up textual content rather than typing text in a predefined structure. It turns out that, although the final document has to be valid with respect to the DTD/Schema used for the encoding, the """"in-progress"""" document is almost never valid. At the same time, it is important to ensure that at each moment of time, the editor is working with an XML document that can be enriched with further markup to become valid. In this paper we explain the notion of potential validity of XML documents, which allows us to distinguish between XML documents that are invalid because the encoding is incomplete and XML documents that are invalid and no further encoding will make the document valid. We show that the set of potentially valid XML documents with respect to any DTD is context-free and we give a linear-Time algorithm for checking potential validity for documents and document updates."""
81258C57,International Conference on Data Engineering,maria esther vidal + louiqa raschid + zoe lacroix,2006,Semantic Model to Integrate Biological Resources,protocols + physical layer + mediation + biological data + data analysis + data model + information retrieval + semantic web + semantic model + data models + ontologies + bioinformatics + ontology,,"We present a framework that uses semantic modeling to represent biological data sources, the multiple links that capture relationships among them, as well as the various applications that transform or analyze biological data. We introduce a data model that encompass three layers: The ontological layer composed of an ontology to represent the scientific concepts and their relationships, the physical layer of the physical resources made available to the scientists, and the data layer composed of the entries accessible through the different resources. Keywords: Ontology; Semantic Web; Integration; Mediation; Bioinformatics."
80F93915,International Conference on Data Engineering,kenji takeda + s j cox + d a nicole + a paventhan,2006,Leveraging Windows Workflow Foundation for Scientific Workflows in Wind Tunnel Applications,high performance computing + data engineering + specific activity + databases + wind tunnel + application software + job scheduling + computer science + data acquisition + data visualization + data processing,,"Scientific and engineering experiments often produce large volumes of data that must be processed and visualised in near-realtime. An example of this, described in this paper, is microphone array processing of data from wind tunnels for aeroacoustic measurements. The overall turnaround time from data acquisition and movement, to data processing and visualization is often inhibited by factors such as manual data movement, system interoperability issues, manual resource discovery for job scheduling, and disparate physical locality between the experiment and scientist or engineer post-event. Workflow frameworks and runtimes can enable rapid composition and execution of complex scientific workflows. In this paper we explore two approaches based on Windows Workflow Foundation, a component of Microsoft WinFX. In our first approach, we present a framework for users to compose sequential workflows and access Globus grid services seamlessly using a .NET-based Commodity Grid Toolkit (MyCoG.NET). We demonstrate how application specific activity sets can be developed and extended by users. In our second approach we highlight how it can be advantageous to keep databases as central to the complete workflow enactment. These two approaches are demonstrated in the context of a wind tunnel Grid system being developed to help experimental aerodynamicists orchestrate such workflows."
80A0B44A,International Conference on Data Engineering,shinichi wakabayashi + yoko kamidoi + noriyoshi yoshida + tomotake nakamura,2006,A Decision Method of Attribute Importance for Classification by Outlier Detection,outlier detection + grouped data + feature extraction + high dimensional data + agriculture + support vector machines + data mining + data privacy,,"Our aim is to group data objects, to which the same class labels could be assigned, using a clustering method for high dimensional data sets. In order to group data objects by clustering, we compute the degree of the influence of each attribute for class labels. To find important attributes having large influence on class labels, we use the feature extraction method which we have developed. We can construct a set of data objects which have single class labels with high accuracy by finding sensitive attributes for the class label. Next, we group data objects which have unique class labels by clustering methods. From experimental simulation, we show the effectiveness of the important attribute detection by performing clustering of transformed benchmark data sets as two class classification problems."
7EE85434,International Conference on Data Engineering,boanerges alemanmeza,2006,Searching and Ranking Documents based on Semantic Relationships,ontologies + web pages + data mining + computer science + industrial relations,,"Just as the link structure of the web is a critical component in todays web search, complex relationships (i.e., the different ways the dots are connected) will be an important component in tomorrows web search technologies. In this paper, I summarize my research on answering the question of: How we can exploit semantic relationships of named-entities to improve relevance in search and ranking of documents? The intuition of my approach is to first analyze the relationships of namedentities with respect to a query. Second, relevance weights, which are assigned by human experts, can then be used to guarantee results within a relevance threshold. These relevance measures can be applied both for searching and ranking of documents."
7D0D5FDF,International Conference on Data Engineering,yogesh simmhan + beth plale + dennis gannon,2006,Towards a Quality Model for Effective Data Selection in Collaboratories,wireless sensor networks + computer science + scientific computing + data quality + predictive models + meteorology + collaboration + q factor + weather forecasting + application software,,"Data-driven scientific applications utilize workflow frameworks to execute complex dataflows, resulting in derived data products of unknown quality. We discuss our on-going research on a quality model that provides users with an integrated estimate of the data quality that is tuned to their application needs and is available as a numerical quality score that enables uniform comparison of datasets, providing a way for the community to trust derived data."
80D67207,International Conference on Data Engineering,jianliang xu + wangchien lee + g a mitchell + yingqi xu,2006,ProcessingWindow Queries in Wireless Sensor Networks,bandwidth + sensor network + databases + wireless sensor networks + wireless sensor network + robustness + information retrieval + network topology + data collection + intelligent networks,,"The existing query processing techniques for sensor networks rely on a network infrastructure for query propagation and data collection. However, such an infrastructure is very susceptible to network topology transients that widely exist in sensor networks. In this paper, we propose an infrastructure-free window query processing technique for sensor networks, called itinerary-based window query execution (IWQE), in which query propagation and data collection are combined into one single stage and executed along a well-designed itinerary inside a query window. We study the parameters for setting up an itinerary (e.g., width and route) and incorporate into IWQE three data collection schemes based on different performance trade-offs. Finally we demonstrate, by extensive simulations, the superior energy-time efficiency, robustness, and accuracy of IWQE over the current state-of-the-art techniques in supporting window queries under various network conditions. © 2006 IEEE."
7D1E3F1D,International Conference on Data Engineering,joseph m hellerstein + frederick r reiss,2006,Declarative Network Monitoring with an Underprovisioned Query Processor,steady state + network monitoring + engines + testing + NonControlled Keywords Not Found + Controlled Keywords Not Found + protocols + hardware,AuthorProvided Keywords Not Found,"Many of the data sources used in stream query processing are known to exhibit bursty behavior. We focus here on passive network monitoring, an application in which the data rates typically exhibit a large peak-to-average ratio. Provisioning a stream query processor to handle peak rates in such a setting can be prohibitively expensive. In this paper, we propose to solve this problem by provisioning the query processor for typical data rates instead of much higher peak data rates. To enable this strategy, we present mechanisms and policies for managing the tradeoffs between the latency and accuracy of query results when bursts exceed the steady-state capacity of the query processor. We describe the current status of our implementation and present experimental results on a testbed network monitoring application to demonstrate the utility of our approach"
8000D0C9,International Conference on Data Engineering,yongluan zhou,2006,Scalable and Adaptable Distributed Stream Processing,process design + scalability + layered architecture + stream processing + user interfaces + data models + engines + financial management,,"In this paper we introduce a new architectural design of a large scale distributed stream processing system. The system adopts a two layer architecture. Based on the locality and the natural administrative dependency of the processors, the processors are naturally partitioned into multiple independent entities. Processors within each entity compose the first layer while all the entities comprise the second one. Tightly coupled cooperation is employed within each entity due to the availability of central administration and their close locality. On the contrary, the entities cooperate with each other in a loosely coupled way. Challenges are identified in each layer of the architecture and techniques are proposed to solve them."
7E6E472C,International Conference on Data Engineering,yao wu + m cardenas + natalia marquez + louiqa raschid + maria esther vidal,2006,Query Rewriting in the Semantic Web7,database languages + query language + mediation + web accessibility + data models + approximation algorithms + navigation + search algorithm + semantic web + ontologies + data model,,"The widespread explosion of Web accessible resources has lead to a new challenge of locating all relevant resources and identifying the best ones to answer a query. This challenge has to address the difficult task of ranking the resources based on user needs, as well as the more expensive computational task of determining all the solutions to answer a query. In this paper, we define a Top K problem for query rewriting on the Semantic Web. We first introduce a three level data model composed of the ontology level, the physical level of the physical resources, and the data level composed of the entries in the different resources. We present a query language for Top K navigational queries over ontology concepts. We then sketch the outline of an efficient search algorithm to compute an approximation of the Top K rewriting options to produce source paths among the physical resources. We briefly discuss the results of an experimental study."
80324B76,International Conference on Data Engineering,utkarsh srivastava + volker markl + marcel kutsch + t m tran + peter j haas,2006,ISOMER: Consistent Histogram Construction Using Query Feedback,feedback + databases + col + maximum entropy principle + automatic control + histograms + engines + NonControlled Keywords Not Found + Controlled Keywords Not Found + multidimensional systems + statistical distributions,AuthorProvided Keywords Not Found,"Database columns are often correlated, so that cardinality estimates computed by assuming independence often lead to a poor choice of query plan by the optimizer. Multidimensional histograms can help solve this problem, but the traditional approach of building such histograms using a data scan often scales poorly and does not always yield the best histogram for a given workload. An attractive alternative is to gather feedback from the query execution engine about the observed cardinality of predicates and use this feedback as the basis for a histogram. In this paper we describe ISOMER, a new feedback-based algorithm for collecting optimizer statistics by constructing and maintaining multidimensional histograms. ISOMER uses the maximumentropy principle to approximate the true data distribution by a histogram distribution that is as ""simple""as possible while being consistent with the observed predicate cardinalities. ISOMER adapts readily to changes in the underlying data, automatically detecting and eliminating inconsistent feedback information in an efficient manner. The algorithm controls the size of the histogram by retaining only the most ""important"" feedback. Our experiments indicate that, unlike previous methods for feedback-driven histogram maintenance, ISOMER imposes little overhead, is extremely scalable, and yields highly accurate cardinality estimates while using only a modest amount of storage."
7D44A30E,International Conference on Data Engineering,samrat ganguly + rauf izmailov + sudeept bhatnagar,2006,Impact of Network-Awareness on Profile Migration,distributed algorithm + throughput + matched filters + high throughput + design optimization + overlay network + bandwidth + data dissemination + network routing + national electric code + algorithm design and analysis,,"A content-based data dissemination network routes messages to users based on their profiles. Our focus is on the design of a overlay network based filtering architecture capable of disseminating high bandwidth and high volume data streams to a large user population. This paper shows that networkaware profile assignment significantly improves the overall system throughput by jointly considering node-bound and link-bound constraints of the overlay network. Our main contribution is the design of a fully-distributed algorithm for dynamic profile movement in order to adapt to the changing network conditions. Each node in the system executes the proposed algorithm independently so that the system as a whole adapts and converges to a high-Throughput operating point by efficiently utilizing the network resources. We identify the critical requirements to meet the automatic reconfiguration goal and use the insights in designing the algorithm. Based on extensive simulation study, we show that we are able to improve the system throughput by more than 80% in most of the cases and by as much as 200% in some scenarios."
814B140F,International Conference on Data Engineering,thomas kabisch + dirk rother + ronald padur,2006,UsingWeb Knowledge to Improve the Wrapping of Web Sources,ontologies + labeling + web interface + search engines + databases + wikipedia + encyclopedias + search engine + web pages,,"During the wrapping of web interfaces ontological know-ledge is important in order to support an automated interpretation of information. The development of ontologies is a time consuming issue and not realistic in global contexts. On the other hand, the web provides a huge amount of knowledge, which can be used instead of ontologies. Three common classes of web knowledge sources are: Web Thesauri, search engines and Web encyclopedias. The paper investigates how Web knowledge can be utilized to solve the three semantic problems Parameter Finding for Query Interfaces, Labeling of Values and Relabeling after interface evolution. For the solution of the parameter finding problem an algorithm has been implemented using the web encyclopedia WikiPedia for the initial identification of parameter value candidates and the search engine Google for a validation of label-value relationships. The approach has been integrated into a wrapper definition framework."
8134ED27,International Conference on Data Engineering,antonio badia + bin cao,2006,Redundancy Awareness in SQL Queries,cost accounting + data engineering + decorrelation + computer science,,"In this paper, we study SQL queries with aggregate subqueries that share common tables and conditions with the outer query. While several approaches can deal with such queries, they have limited applicability. We propose the redundancy awareness method to detect the largest common part shared by query and subquery, compute it once, and determine what operations are needed to finish evaluation of the original query. Our approach can deal with redundancy in all types of subqueries. We offer the possibility for the optimizer to choose the most efficient plan for a given query. We have implemented our approach on top of a commercial DBMS; our experiments show that our approach compares favorably to existing optimization techniques."
7D8DBF16,International Conference on Data Engineering,lars e olson + gianluca tonti + andrzej uszok + nathan seeley + jeffrey m bradshaw + marianne winslett,2006,Trust Negotiation as an Authorization Service forWeb Services,web services + open systems + web service + authentication + data engineering + data security + authorization + satisfiability + cognition + availability,,"Like other open computing environments, web services need a scalable method of determining authorized users. We present desiderata for authorization facilities for web services, and analyze potential ways of satisfying them. We propose a third-party authorization system for web services based on trust negotiation, discuss its implementation using the TrustBuilder runtime system for trust negotiation, and present performance results from a stock trading application."
7F7FA667,International Conference on Data Engineering,sanghyuk lim + kyongi ku + yoosung kim + kichang kim,2006,A Node Split Algorithm Reducing Overlapped Index Spaces in M-tree Index,routing + indexation + multidimensional systems + space technology + indexing + euclidean distance,,"Even though the ideal split policy for M-Tree should determine which two routing objects are promoted so that the two partitioned regions would have minimum volume and minimum overlapped space, the previously proposed split algorithms must fail to achieve the goal since they select the actual entries from overflow entries as the routing objects to be promoted. Hence, in this paper, we propose an optimal node split algorithm for M-Tree which uses the virtual center points of two partitioned regions as the routing objects and which results in tighter volume and finally reduce the overlap index spaces between nodes. Also, we show that the proposed algorithm outperforms the previous ones for similarity-based query processing with respect to the average response time, although there is a slight increase in insertion time. We also discuss the implementation of a prototype of contentbased music retrieval system in which the proposed scheme is used for multidimensional indexing."
7F94BA05,International Conference on Data Engineering,peter boncz + sandor heman + niels j nes + marcin zukowski,2006,Super-Scalar RAM-CPU Cache Compression,data mining + storage system + throughput + database system + information retrieval system + data analysis + databases + hardware + information retrieval + scientific data + bandwidth + encoding,,"High-performance data-intensive query processing tasks like OLAP, data mining or scientific data analysis can be severely I/O bound, even when high-end RAID storage systems are used. Compression can alleviate this bottle-neck only if encoding and decoding speeds significantly exceed RAID I/O bandwidth. For this purpose, we propose three new versatile compression schemes (PDICT, PFOR, and PFOR-DELTA) that are specifically designed to extract maximum IPCfrom modern CPUs. We compare these algorithms with compression techniques used in (commercial) database and information retrieval systems. Our experiments on the MonetDB/X100 database system, using both DSM and PAX disk storage, show that these techniques strongly accelerate TPC-H performance to the point that the I/O bottleneck is eliminated. © 2006 IEEE."
7D3F04DE,International Conference on Data Engineering,thomas schwarz + witold litwin + peter tsui,2006,"An Encrypted, Content Searchable Scalable Distributed Data Structure",data privacy + bandwidth + availability + explosives + cryptography + workstations + computer networks + data structures,,"Scalable Distributed Data Structures (SDDS) store data in a file of key-based records distributed over many storage sites. The number of storage sites utilized grows and shrinks with the storage needs of applications, but transparently to them. An application can search records by key or by content in parallel at all storage sites. The need for privacy of the data at the storage sites might require the encryption of the records. However, the scheme needs to preserve the capability to search in parallel. We propose a scheme that achieves this goal. We create a collection of additional SDDS indices. We encrypt these so that we can still perform string searches performed in parallel at the storage sites. We present the scheme and evaluate its strength as well as storage and access performance."
7F94C381,International Conference on Data Engineering,weiyi meng + kinglup liu + wanjing zhang + clement yu + yiyao lu,2006,Automatic Extraction of Publication Time from News Search Results,data mining + search engines + search engine + data collection + metasearch + data engineering + testing + computer science + displays,,"The publication time of a page can have a big impact on its relevance to a query, especially for time-sensitive pages such as news items. For news search engines, the publication time of news items can usually be found in the returned search result records. In this paper, we introduce a method that can automatically extract the publication time for each news story returned from news search engines based on several important observations we made. We also introduce a wrapper implementation for the extraction method. The experimental results using data collected from 50 news search engine show that our method is effective and the wrapper implementation can not only improve the extraction accuracy but also the extraction efficiency."
8119AF83,International Conference on Data Engineering,b bouqata + frank s spear + boleslaw k szymanski + a b marcus + s adaly,2006,A Day in the Life of a Metamorphic Petrologist,working group + prototypes + chemical analysis + computer science + geology + navigation + geoscience,,"- In this paper, we describe the functionality of a toolkit for sharing and long-term use of different types of geological data sets across disciplines. Our tools allow users to describe the meaning of their data by attaching semantic information to it. The toolkit also makes use of the users' access patterns to learn how the data are used to further enhance the utility of data centric methods. These learned patterns are used in conjunction with the semantic data to help other users find common ways to navigate heterogenous collections and highlight interesting information. Our current prototype is being developed in close collaboration with the Metamorphic Petrology working group formed to facilitate sharing of data within this subdiscipline of geosciences as well as with other systems for sharing of geological data."
7D8763BA,International Conference on Data Engineering,toshiyuki amagasa + hiroyuki kitagawa + kentarou kido,2006,Processing XPath Queries in PC-Clusters Using XML Data Partitioning,data engineering + xml + xml database + parallel processing + degradation + clustering algorithms + relational databases,,"Recently, with the rapid spread of XML format, it has become popular that large-scale data, whose size range from several hundreds of MB to several GB, are described by XML. For the purpose of providing fast and reliable means for storage and retrieval of huge XML data, it is a reasonable choice for us to use XML databases. In fact, there are many ways to realize XML databases, but relational XML database, in that an XML data is mapped to relational tables and query processing is enabled in terms of SQL queries, is one of the most popular way to implement XML databases. However, some researchers have pointed out that the performance of relational XML databases degrades when dealing with such huge XML data. In this study, we propose a scheme for parallel processing of XML data using PC Clusters. First, we discuss how to decompose XML data so that we can perform parallel processing of XML queries. We give the definitions of vertical and horizontal decomposition of XML data based on decomposition of schema graph and XML instances, respectively. To allocate decomposed XML data to cluster nodes, we give an algorithm for computing pseudo-optimal assignment of XML fragments like greedy method in the light of XML query workload. Finally, we experimentally evaluate the effectiveness of the proposed method."
7FB5291D,International Conference on Data Engineering,chi hong cheong + man hon wong,2006,Mining Popular Paths in a Transportation Database System with Privacy Protection,database system + data engineering + data privacy + data mining + computer science + database systems + transportation,,"This paper proposes an algorithm to identify popular paths in a transportation system, while the privacy of drivers is preserved. A popular path is one of the most frequently used routes between any two points in a road map. In order to identify popular paths with privacy protection, the algorithm figures out what information is useless for identifying popular paths, and this information is not revealed to the data mining system so that privacy is preserved. In addition, the system does not record the identifications of the vehicles. Moreover, in the mining process, the database does not contain complete path information. The experimental results verify the correctness of the proposed algorithm and show that the proposed algorithm is scalable."
7D8A4937,International Conference on Data Engineering,weishinn ku + roger zimmermann,2006,Location-based Spatial Queries with Data Sharing in Mobile Environments,ad hoc networks + computer science + environmental management + wireless ad hoc network + neural networks + wireless network + information exchange + mobile computing + intelligent networks + real time,,"Mobile clients feature increasingly sophisticated wireless networking support that enables real-Time information exchange with remote databases. Location-dependent spatial queries, such as determining the proximity of stationary objects (e.g., restaurants and gas stations) are an important class of inquiries. We present novel approaches to support nearest-neighbor queries and window queries from mobile hosts by leveraging the sharing capabilities of wireless ad-hoc networks. We illustrate how previous query results cached in the local storage of neighboring mobile peers can be leveraged to either fully or partially compute and verify spatial queries at a local host. The feasibility and appeal of our technique is illustrated through extensive simulation results that indicate a considerable reduction of the query load on the remote database. Furthermore, the scalability of our approaches is excellent because a higher density of mobile hosts increases its effectiveness."
7D646136,International Conference on Data Engineering,kyoji kawagoe + y suzuki + battuguldur lkhagva,2006,New Time Series Data Representation ESAX for Financial Applications,pattern analysis + data engineering + data analysis + time series data + time series analysis + data mining + singular value decomposition + discrete fourier transform + similarity search,,"Efficient and accurate similarity searching for a large amount of time series data set is an important but non-Trivial problem. Many dimensionality reduction techniques have been proposed for effective representation of time series data in order to realize such similarity searching, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), the Adaptive Piecewise Constant Approximation (APCA), and the recently proposed Symbolic Aggregate Approximation (SAX). In this work we propose a new extended approach based on SAX, called Extended SAX in order to realize efficient and accurate discovering of important patterns, necessary for financial applications. While the original SAX approach allows a very good dimensionality reduction and distance measures to be defined on the symbolic approach, SAX is based on PAA (Piecewise Aggregate Approximation) representation for dimensionality reduction that minimizes dimensionality by the mean values of equal sized frames. This value based representation causes a high possibility to miss some important patterns in some time series data such as financial time series data. Extended SAX, proposed in the paper, uses additional two new points, that is, max and min points, in equal sized frames besides the mean value for data approximation. We show that Extended SAX can improve representation preciseness without losing symbolic nature of the original SAX representation. We empirically compare the Extended SAX with the original SAX approach and demonstrate its quality improvement."
7DA76E15,International Conference on Data Engineering,geoff skinner + song han + elizabeth chang,2006,Defining and Protecting Meta Privacy: A New Conceptual Framework Within Information Privacy,data privacy + data protection + conceptual framework + personal identifiable information + data security + information systems + transaction data + information privacy + internet + information security + information system,,"When considering information security and privacy issues most of the attention has previously focused on data protection and the privacy of personally identifiable information (PII). What is often overlooked is consideration for the operational and transactional data. Specifically, the security and privacy protection of metadata and metastructure information of computing environments has not been factored in to most methods. Metadata, or data about data, can contain many personal details about an entity. It is subject to the same risks and malicious actions personal data is exposed to. This paper presents a new perspective for information security and privacy. It is termed Meta Privacy and is concerned with the protection and privacy of information system metadata and metastructure details. We first present a formal definition for Meta Privacy, and then analyze the factors that encompass and influence Meta Privacy. In addition, we recommend some techniques for the protection of Meta Privacy within the information systems. Further, the paper highlights the importance of ensuring all informational elements of information systems are adequately protected from a privacy perspective."
80A37959,International Conference on Data Engineering,wenxin liang + haruo yokota,2006,A Path-sequence Based Discrimination for Subtree Matching in Approximate XML Joins,data engineering + xml + internet + clustering algorithms + bioinformatics + xml document + computer science,,"In this paper, we discuss the one-To-multiple matching problem in leaf-clustering based approximate XML join algorithms and propose a path-sequence based discrimination method to solve this problem. In our method, each path sequence from the top node to the matched leaf in the base and target subtree is extracted, and the most similar target subtree for the base one is determined by the pathsequence based subtree similarity degree. We conduct experiments to evaluate our method by using both real bibliography and bioinformatics XML documents. The experimental results show that our method can effectively decrease the occunence rate of one-To-multiple matching for both bibliography and bioinformatics XML data, and hence improve the precision of the leaf-clustering based approximate XML join algorithms."
7ED37D3F,International Conference on Data Engineering,gerhard weikum + surajit chaudhuri,2006,Foundations of Automated Database Tuning,resource management + management information systems + database systems + information system + cost effectiveness + relational databases + information technology + tuning + computer science + information management,,"2. Role of Self-Managing Technology 3. Goal of the Tutorial"
7F59C827,International Conference on Data Engineering,paolo merialdo + valter crescenzi,2006,Efficient Techniques for Effective Wrapper Induction,induction generators + web pages + writing + production + data mining + computer applications + algorithm design and analysis + html,,"Several studies have recently concentrated on the generation of wrappers for extracting data from Web data sources. The ROADRUNNER system aims at automating the tedious and expensive process of writing wrappers in an unsupervised, domain-independent, and scalable manner. The system is based on a grammar inference algorithm, called MATCH, which has been designed in a sound theoretical framework. However, in its original definition MATCH lacks in expressivity; that is, in many cases when MATCH runs over real-life Web pages, it is not able to produce a solution. In this paper we address the challenging issue of developing techniques that allow us to build upon MATCH an effective and efficient system, without renouncing to the original formal background. First, we analyze the main limitations of MATCH; then we illustrate the techniques we have developed to overcome such limitations. Finally we report on the results of some experiments, that show the efficacy of the introduced techniques and demonstrate the improvements of the overall system."
7ECF774F,International Conference on Data Engineering,brian gabor + bettina kemme,2006,Exp-WF:Workflow Support for Laboratory Information Systems,workflow management system + scientific research + remote monitoring + information systems + computer science + management information systems + workflow management + relational data + communications technology + asynchronous communication,,"As scientific experiments and their analysis become increasingly automated, the need for workflow management support rises. Many workflow management systems tailored to scientific workflows have been developed. However, they are mainly stand-Alone systems and ignore that many scientific research groups already store and manage experimental data in laboratory information systems, called LIMS. LIMS are typically web-based and allow researchers to insert and view experiment related data. In this paper we describe Exp-WF, a workflow management module that is designed to be incorporated into a typical LIMS such that workflow management and the traditional functionality of a LIMS co-exist. Scientists describe the execution order of experiments as a workflow model. Exp-WF then automatically selects experiments for execution and dispatches them to the people or machines responsible for performing them. Exp- WF extends existing workflow models to be able to handle the particularities of scientific experiment workflows. Exp- WF can be incorporated into an existing web-based LIMS in a nonintrusive way by using servlet filter technology to observe user actions and act upon them. Exp-WF uses agentbased technology with asynchronous communication to dispatch tasks to remote machines."
7F693F49,International Conference on Data Engineering,thanaa m ghanem,2006,Supporting Predicate-Window Queries in Data Stream Management Systems,test bed + sliding window + prototypes + system testing + disaster management + query optimization + computer science + query language + out of order + database languages,,"The window query model is widely used in data stream management systems where the focus of a continuous query is limited to a set of the most recent tuples. In this dissertation, we show that an interesting and important class of continuous queries can not be answered by the existing sliding-window query models. Thus, we introduce a new model for continuous queries, termed the predicate-window query model that limits the focus of a continuous query to the stream tuples that qualify a certain predicate. Predicate windows are characterized by the following (1) The window predicate can be defined over any attribute in the stream tuple (ordered or unordered). (2) Stream tuples qualify and disqualify the window predicate in an out-of-order manner. The goal of this dissertation is to develop an efficient framework to realize predicate windows inside data stream management systems. The predicate-window query framework enables the system to efficiently support a wide variety of streaming applications through an expressive query language and efficient query evaluation mechanisms (i.e., query execution and query optimization). As a test bed for our research, the predicate-window framework is being developed inside Nile; a prototype data stream management system developed at Purdue University."
7F39E46E,International Conference on Data Engineering,kiyoharu aizawa + toshihiko yamasaki + g c de silva,2006,"Creation of an Electronic Chronicle for a Ubiquitous Home: Sensing, Analysis and Evaluation",image resolution + informatics + information retrieval + data engineering,,"A system for continuous capture and retrieval of multimedia data from a home-like ubiquitous environment is presented. A large number of cameras and microphones are used for video and audio acquisition. Pressure based sensors mounted on the house floor are used to capture footsteps as context data. Both context and content are analyzed to create indices for efficient retrieval and summarization of the video and audio data. The system can be queried based on date and time, location, persons, sounds and basic activity in terms of footsteps. Key frame extraction adaptive to the activity of a person can be used to summarize the video. The system was evaluated by means of a real-life experiment where a family lived in the house and later used the system to retrieve experiences that they had during their stay. The results of this experiment are reported and discussed. The paper concludes with suggestions for possible future directions."
81181922,International Conference on Data Engineering,le gruenwald + leonard brown,2006,Speeding up Color-Based Retrieval in Multimedia Database Management Systems that Store Images as Sequences of Editing Operations,management system + information retrieval + similarity search + data mining + image retrieval + computer science + feature extraction,,"Typically, multimedia database management systems process content-based image retrieval queries by extracting a set of features from each data object as it is inserted into the underlying database. By expressing queries that are based upon these features, users are able to retrieve the data objects back from the database. Previous research has demonstrated that one method of improving the effectiveness of similarity searches in such systems is to augment the underlying database with a set of edited images to allow more flexible matching. Space can be saved by storing the additional images as sequences of editing operations instead of as large binary objects. This paper proposes an approach for processing retrieval queries in such an environment and presents the results of a performance evaluation demonstrating the effectiveness of the approach."
7E6D9E6A,International Conference on Data Engineering,zoe lacroix + herve menager,2006,A Workflow Engine for the Execution of Scientific Protocols,web services + information analysis + biology + web service + databases + information science + data flow + protocols + availability + semantic interoperability + engines + ontologies + communications technology,,"We present the execution engine of the SemanticBio system, an integration solution that provides scientists support to express and execute scientific protocols. In SemanticBio scientific workflows are first expressed as conceptual workflows using scientific ontologies. Conceptual workflows are then translated in a semi-Automated process into executable workflows, composed of calls to coordinated web services. Once the user has selected an executable workflow that meets the protocol needs, the SemanticBio execution engine supports the execution of data flow-coordinated tasks, i.e., the execution of a task is only based on the availability of its inputs. This engine is a lightweight, yet flexible approach to the execution of such workflows. Our approach addresses the problem of semantic interoperability of scientific resources publicly available on the web."
7DAA1D41,International Conference on Data Engineering,thomas plagemann + ellen munthekaas + tommy gagnes,2006,A Conceptual Service Discovery Architecture for Semantic Web Services in Dynamic Environments,service oriented architecture + assembly + informatics + taxonomy + web services + ontologies + semantic web + web service + service discovery + topology,,"Web Services technology is being used for increasingly different environments than it was designed for. To facilitate discovery of Web Services in dynamic environments, both service description and distribution of descriptions must be improved. Several research efforts target semantic description of services. However, Semantic Web Service discovery in peer-To-peer-like, dynamic environments where services and registries are transient cannot be based on current mechanisms for distribution of Web Service descriptions. Based on a set of generic requirements, we introduce a conceptual architecture that aims to solve many of the problems related to Semantic Web Service discovery in dynamic environments. The architecture is based on a distributed multi-registry topology and aliveness information. We present several research problems we have identified during our initial work on this architecture."
7FC7AF17,International Conference on Data Engineering,tomohiro yoshihara + dai kobayashi + haruo yokota + ryo taguchi,2006,A Concurrency Control Method for Parallel Btree Structures,concurrency control + data engineering + low frequency + tree structure,,"A new concurrency control protocol for parallel Btree structures, MARK-OPT, is proposed. MARK-OPT marks the lowest structure-modification-operation (SMO) occurrence point during optimistic latch coupling operations, to reduce the cost of SMO compared to the conventional protocols such as ARIES/IM and INC-OPT. The marking reduces the frequency of restarts for spreading the range of X latches, which will clearly improves the system throughput. Moreover, the MARK-OPT is deadlock free and satisfies the physical consistency requirement for Btrees. These indicate that the MARK-OPT is right and suitable as a concurrency control protocol for Btree structures. This paper also proposes three variations of the protocol, INC-MARKOPT, 2P-INT-MARK-OPT and 2P-REP-MARK-OPT, by focusing on tree structure changes from other transactions. We implement the proposed protocols, the INC-OPT, and the ARIES/IM for the Fat-Btree, a form of parallel Btree, and compare the performance of these protocols using a large-scale blade system. The experimental results indicate that the proposed protocols always improve the system throughput, and the 2P-REP-MARK-OPT is the most useful protocol in a high update environment. Moreover, in the experiment, the low frequency of restarts in the proposed protocols indicates that the marking in the proposed protocols is effective."
810F3B58,International Conference on Data Engineering,bin cao,2006,Optimization of Complex Nested Queries in Relational Databases,computer science + algebra + warehousing + relational database + decision support systems + relational databases,,"Due to the flexible structures of SQL, no general approach works efficiently for all kinds of queries. Some special kinds of queries can be further optimized for better performance. In this dissertation, we study two kinds of such queries: one is queries having non-Aggregate subqueries and the other is queries having redundancy. To deal uniformly with non-Aggregate subqueries in SQL, we propose the nested relational approach based on the nested relational model. To deal with redundancy in complex SQL queries, we propose the redundancy awareness method with the introduction of the for-loop operator. The main contribution of this dissertation is to provide more efficient solutions for these two kinds of queries than existing techniques."
7D01F6D1,International Conference on Data Engineering,mohamed y eltabakh + walid g aref + ramy eltarras,2006,Space-Partitioning Trees in PostgreSQL: Realization and Performance,databases + application software + hash table + prototypes + indexing + neural networks + indexation + NonControlled Keywords Not Found + Controlled Keywords Not Found + indexes + computer science + engines + software engineering + kd tree,AuthorProvided Keywords Not Found,"Many evolving database applications warrant the use of non-traditional indexing mechanisms beyond B+-trees and hash tables. SP-GiST is an extensible indexing framework that broadens the class of supported indexes to include disk-based versions of a wide variety of space-partitioning trees, e.g., disk-based trie variants, quadtree variants, and kd-trees. This paper presents a serious attempt at implementing and realizing SP-GiST-based indexes inside PostgreSQL. Several index types are realized inside PostgreSQL facilitated by rapid SP-GiST instantiations. Challenges, experiences, and performance issues are addressed in the paper. Performance comparisons are conducted from within PostgreSQL to compare update and search performances of SP-GiST-based indexes against the B+-tree and the R-tree for string, point, and line segment data sets. Interesting results that highlight the potential performance gains of SPGiST- based indexes are presented in the paper."
7D37DC81,International Conference on Data Engineering,shawn o pearce + marialuisa sapino + sibel adali,2006,On Querying and Exploring Activities on a User&#146;s Desktop,data visualization + context modeling + object oriented + computer science + user interfaces + management system + query language + snow + database languages + data mining,,"Many desktop query and management systems offer an object oriented view of a computer, where the emphasis is on describing the information relevant to an object. However, objects may be related to each other in different ways and in different contexts. In this paper, we argue that users create and modify data as a function of activities they are involved in. We develop methods to explore objects on a desktop through activities. Our methods integrate discovered relationships between data objects based on their participation in different activities as well as other properties. Our activity model is well suited to support a query language that is able to alter the context and the definition of an activity to easily visualize complex relationships in data. We show that this new organization makes many new interesting desktop functionalities a reality."
811139D9,International Conference on Data Engineering,ramesh jain + utz westermann,2006,"{m E} - A Generic Event Model for Event-Centric Multimedia Data Management in eChronicle Applications",concrete + content management + production systems + computer science + application software + information systems,,"eChronicle applications are inherently event-centric, enabling users to find and explore important events in an application domain and providing unified access to any media that document them. Todays multimedia data management components such as multimedia databases, however, are largely media-centric, considering events - if at all - just as one of many pieces of media metadata. Obfuscating event exploration and event-driven access to media, they are only of limited use for the implementation of eChronicle applications. Using a concrete eChronicle application in the defense domain, this paper motivates the need for event-centric multimedia data management components. As a foundation, the paper proposes the multimedia event model and discusses essential design considerations for the development of that model. s genericity and profound adaptability to varying application needs make the model a suitable foundation for reusable multimedia data management components that are useful not only for eChronicle applications, but for any multimedia application where event-driven access to content is of interest."
7E4A9FBC,International Conference on Data Engineering,tomoyo kage + kazutoshi sumiya,2006,A Temporal Clustering Method forWeb Archives,data mining + time series + web pages + data engineering + search engines + feature extraction + testing,,"Web pages are collected and stored in Web archives, and several methods to construct Web archives have been developed. We propose a method to retrieve time series of Web pages from Web archives by using the pages temporal characteristics. We present two processes for searching Web archives based on the temporal relation of query keywords. One is a method for determining the relation. The other is a method of inquiring Web pages based on the relation. In this paper, we discuss the two processes and an experimental result of the method."
7FBAA8BB,International Conference on Data Engineering,yati peng + chingyung lin + mingting sun,2006,Multimodality Sensors for Sleep Quality Monitoring and Logging,infrared + machine learning + sleep + privacy,,"In this paper, we investigate the possibility of using simple multimodality sensors to automatically detect a persons sleep condition. We propose a system which consists of heart-rate, video, and audio sensors, and apply machine learning methods to infer the sleep-Awake condition during the time a user spends on the bed. The sleep-Awake conditions will be useful information for inferring sleep latency and sleep efficiency, which are critical to both sleep-related diseases and sleep quality measurements. To eliminate possible privacy concerns, we further explore the feasibility of using passive infrared (PIR) sensor instead of video sensor for motion information acquisition. Our experimental results are promising and show the potential use of the proposed novel economical alternative to the traditional medical measurement equipment, with competitive performance on the sleeprelated activity monitoring and the sleep quality measurements."
7F5F7D2B,International Conference on Data Engineering,george stephanides + mihai gabroveanu + mirel cosulschi + nicolae constantinescu,2006,AHPA-Calculating Hub and Authority for Information Retrieval,html + web pages + social network analysis + computer science + information retrieval + search engines + informatics + world wide web,,"The fast development of the World Wide Web makes searching and retrieving of information to become a not easy task. Two algorithms proposed around the fall of 1996, Page Rank [3] and HITS [9], became the center of majority research efforts. They try to remedy the abundance of results, bringing order with the help of notions related to prestige in social network analysis. Some approaches use the link structure of the web to find the importance of the web pages (Page Rank method [3]) or to determine their authority related to a particular topic (Hub and Authority concept). In this paper we propose a new method for calculating the authority of a web page."
7ECE480B,International Conference on Data Engineering,weishinn ku + chingai wan + haojun wang + roger zimmermann,2006,MAPLE: A Mobile Scalable P2P Nearest Neighbor Query System for Location-based Services,frequency + visualization + tin + nearest neighbor + neural networks + scalability + location based service + p2p + spatial database,,"In this demonstration we present MAPLE, a scalable peer-to-peer nearest neighbor (NN) query system for mobile environments. MAPLE is designed for the efficient sharing of query results cached in the local storage of mobile peers. The MAPLE system is innovative in its ability to either fully or partially compute location-dependent nearest neighbor objects on each host. The demonstration illustrates how cooperative data sharing and distributed processing among mobile peers results in a considerable reduction of the load on remote spatial databases. •_ 2006 IEEE."
7DF1226C,International Conference on Data Engineering,chengcui zhang + weibang chen + xin chen,2006,A PCA-Based Vehicle Classification Framework,intelligent transportation systems + principal component analysis + intelligent sensors + face detection,,"Due to its great practical importance, Intelligent Transportation System has been an active research area in recent years. In this paper, we present a framework that incorporates various aspects of an intelligent transportation system with its ultimate goal being vehicle classification. Given a traffic video sequence, the proposed system first proceeds to segment individual vehicles. Then the extracted vehicle objects are normalized so that all vehicles are aligned along the same direction and measured at the same scale. Following the preprocessing step, two classification algorithms - Eigenvehicle and PCA-SVM, are proposed and implemented to classify vehicle objects into trucks, passenger cars, vans, and pick-ups. These two methods exploit the distinguishing power of Principal Component Analysis (PCA) at different granularities with different learning mechanisms. Experiments are conducted to compare these two methods and the results demonstrate the effectiveness of the proposed framework."
8143A02B,International Conference on Data Engineering,olivier j f martin + ioannis pitas + beno t macq + irene kotsia,2006,The eNTERFACE&#146;05 Audio-Visual Emotion Database,scientific research + protocols + signal processing + informatics,,"This paper presents an audio-visual emotion database that can be used as a reference database for testing and evaluating video, audio or joint audio-visual emotion recognition algorithms. Additional uses may include the evaluation of algorithms performing other multimodal signal processing tasks, such as multimodal person identification or audio-visual speech recognition. This paper presents the difficulties involved in the construction of such a multimodal emotion database and the different protocols that have been used to cope with these difficulties. It describes the experimental setup used for the experiments and includes a section related to the segmentation and selection of the video samples, in such a way that the database contains only video sequences carrying the desired affective information. This database is made publicly available for scientific research purposes."
7FE34EAB,International Conference on Data Engineering,stefan bottcher + rita steinmetz,2006,Finding the Leak: A Privacy Audit System for Sensitive XML Databases,data security + query language + testing + data privacy + database languages + private information + access control + xml database + static analysis + xml,,"Whenever private information that is legally used by multiple employees of a company has been illegally exposed to a third party, it is of significant importance to the concerned company to find the information leak in its staff for a variety of reasons, e.g., to keep confidence of its customers. In this paper, we present a privacy audit system for XML databases and the XPath query language which uses the concept of an audit query to describe the secret information. For a given audit query, our system returns a set of suspicious user queries that may have used the secret information. Suspicious user queries are identified in a sequence of four steps: first, a static analysis based on the time constraints; second, a comparison of the nodename tests of the audit query and the user queries; third, an analysis of the associations of the node-name tests found in the audit query and in the user queries; and finally, a test on historic data. Furthermore, we discuss privacy violation detection in case of an attacker who submits multiple queries and externally compares the results."
812E4284,International Conference on Data Engineering,arnon rosenthal + len seligman,2006,Pragmatics and Open Problems for Inter-schema Constraint Theory,ontologies + writing + business process + mediation + data exchange + business + artificial intelligence + databases + prototypes,,"We consider pragmatic issues in applying constraint-based theories (such as that developed for data exchange) to a variety of problems. We identify disconnects between theoreticians and tool developers, and propose principles for creating problem units that are appropriate for tools. Our Downstream Principle then explains why automated schema mapping is a prerequisite to transitioning schemamatching prototypes. Next, we compare concerns, strengths, and weaknesses of database and AI approaches to data exchange. Finally, we discuss how constrained update by business processes is a central difficulty in maintaining n-Tier applications, and compare the challenges with data exchange and conventional view update."
7F749D71,International Conference on Data Engineering,masashi toyoda + masaru kitsuregawa + hiroshi ono,2006,IdentifyingWeb Spam by Densely Connected Sites and its Statistics in a JapaneseWeb Snapshot,web pages + data mining + search engine + web spam + data engineering + search engines + statistics + link analysis + degradation + information analysis,,"Web spamming refers to actions intended to mislead search engines into ranking certain pages higher than they deserve. Recently, the amount of web spam has increased dramatically, leading to a degradation of search results. One of the most effective spamming techniques is link spamming. This is done by setting up an interconnected structure of pages for deceiving link-based ranking methods, such as PageRank. In this paper, we analyze distributions of link spam in our archive of Japanese web pages using link analysis techniques."
809F6D98,International Conference on Data Engineering,omar khadeer hussain + tharam s dillon + elizabeth chang,2006,Quantifying Risk in Financial Terms According to Context and Time for Decision Making,risk analysis + risk management + information technology + heart + information systems + uncertainty,,"One of the characteristic of Risk is the possible loss that could be incurred in an interaction. In a peer-Topeer financial interaction the loss incurred is usually the financial loss to the resources of the trusting peer that are involved in the interaction. Hence a way for the trusting peer to analyse the Risk in interacting with any trusted peer in order to decide wether to interact with it or not, is to determine the possible loss to its resources that are involved in the interaction. In this paper we will propose a methodology by which the trusting peer can determine the possible loss that could be incurred to it in interacting with a trusted peer."
7EA1CC45,International Conference on Data Engineering,barbara a eckman + terry gaasterland + ben snyder + maria esther vidal + louiqa raschid + zoe lacroix,2006,Implementing a Bioinformatics Pipeline (BIP) on a Mediator Platform: Comparing Cost and Quality of Alternate Choices,database systems + pipelines + protocols + costs and benefits + splicing + database management system + predictive models + bioinformatics + alternative splicing,,"We address the challenges of supporting scientific workflow design for a bioinformatics pipeline (BIP) to detect alternative splicing of an organisms genes. Scientific workflows depend significantly on the selection of the resources (data sources and applications) selected for their implementation. In addition, each implementation of a workflow may offer different costs and benefits. We present the details of a hard-coded BIP and then discuss the process of implementing a BIP on a database management system (DBMS) enabled mediator platform. Such a solution supports biologists as they customize the pipeline, choosing relevant resources and implementations for operators and applications. We discuss cost and benefit metrics to compare these alternative BIP choices."
7F589DBA,International Conference on Data Engineering,kyoji kawagoe + yu suzuki + guifang duan,2006,Grid Representation for Efficient Similarity Search in Time Series Databases,indexation + data engineering + indexes + multidimensional systems + grid computing + similarity search + databases + pervasive computing + time series + lower bound + fourier transforms,,"Widespread interest in time-series similarity search has made more in need of efficient technique, which can reduce dimensionality of the data and then to index it easily using a multidimensional structure. In this paper, we introduce a new technique, which we called grid representation, based on a grid approximation of the data. We propose a lower bounding distance measure that enables a bitmap approach for fast computation and searching. We also show how grid representation can be indexed with a multidimensional index structure, and demonstrate its superiority."
813861FB,International Conference on Data Engineering,gagan agrawal + ruoming jin,2006,Systematic Approach for Optimizing Complex Mining Tasks on Multiple Databases,data warehouse + application software + computer science + data mining + cost function + algebra + NonControlled Keywords Not Found + information systems + Controlled Keywords Not Found + relational databases + data engineering,AuthorProvided Keywords Not Found,"Many real world applications involve not just a single dataset, but a view of multiple datasets. These datasets may be collected from different sources and/or at different time instances. In such scenarios, comparing patterns or features from different datasets and understanding their relationships can be an extremely important part of the KDD process. This paper considers the problem of optimizing a mining task over multiple datasets, when it has been expressed using a highlevel interface. Specifically, we make the following contributions: 1) We present an SQL-based mechanism for querying frequent patterns across multiple datasets, and establish an algebra for these queries. 2) We develop a systematic method for enumerating query plans and present several algorithms for finding optimized query plan which reduce execution costs. 3) We evaluate our algorithms on real and synthetic datasets, and show up to an order of magnitude performance improvement"
7D5A561E,International Conference on Data Engineering,yingjiu li + xintao wu + songtao guo,2006,Deriving Private Information from Perturbed Data Using IQR Based Approach,data engineering + data mining + data privacy + databases + covariance matrix + information analysis + private information,,"Several randomized techniques have been proposed for privacy preserving data mining of continuous data. These approaches generally attempt to hide the sensitive data by randomly modifying the data values using some additive noise and aim to reconstruct the original distribution closely at an aggregate level. However, one challenge here is whether the reconstructed distribution can be exploited by attackers or snoopers to derive sensitive individual data. This paper presents one simple attack using Inter-Quantile Range on reconstructed distribution. The experimental results show that current random perturbation-based privacy preserving data mining techniques may need a careful scrutiny in order to prevent privacy breaches through this model based inference."
81316791,International Conference on Data Engineering,stefanie rinderle + manfred reichert + andreas wombacher,2006,On the Controlled Evolution of Process Choreographies,process control + web services + environmental economics + NonControlled Keywords Not Found + information systems + Controlled Keywords Not Found + automata + data engineering + process engineering,AuthorProvided Keywords Not Found,"Process-aware information systems have to be frequently adapted due to business process changes. One important challenge not adequately addressed so far concerns the evolution of process choreographies. If respective modifications are applied in an uncontrolled manner, inconsistencies or errors might occur in the sequel. In particular, modifications of private processes performed by a single party may affect the implementation of the private processes of partners as well. In this paper we sketch a framework that allows process engineers to detect how changes of private processes may affect related public views and - if so - how they can be propagated to the public and private processes of partners. Our approach exploits the semantics of the applied changes in order to automatically determine the adaptations necessary for the partner processes."
7D8ED3A2,International Conference on Data Engineering,klemens bohm + stephan schosser + bodo vogt + rainer schmidt,2006,Strategic Properties of Peer-to-Peer Data Structures and Behaviour of Human Peers in Economic Experiments,game theory + data structure + col + protocols + collaboration + data structures + predictive models + file sharing + economic forecasting,,"Structured peer-to-peer networks manage large data sets. Each peer administers a part of the data. To answer a query, several peers must collaborate. However, experience with peer-to-peer file sharing systems shows that peers tend to abandon collaboration. Such behaviour is also expected in structured peer-to-peer systems. In the recent past, protocols for such systems to counter this kind of behaviour have been proposed. Our objective is to investigate how the behaviour of peers reacts to such protocols. We use idealized game theoretic models to analyze the strategic properties of structured peer-to-peer networks. The models predict cutoff strategies. I.e., if more than a certain percentage of their queries are answered, peers also answer queries in a stationary equilibrium. Another theoretical result is that free riding may occur in equilibrium. To check whether our theoretical predictions are in line with reality, we have implemented our model in an experimental laboratory and have analyzed the behaviour of human peers. These experiments support our theoretical results."
7F3DC257,International Conference on Data Engineering,ming li + murali mani + elke a rundensteiner + mingzhu wei,2006,Processing Recursive XQuery over XML Streams: The Raindrop Approach, + application software + lifting equipment + computer science + stream processing + engines + xml + algebra + automata,,"- XML stream applications bring the challenge of efficiently processing queries on sequentially accessible tokenbased data. For efficient processing of queries, we need to ensure that memory usage stays low. This in turn requires that we avoid holding data in the query buffer, by outputting it at the earliest possible time. In this paper, we propose a new class of stream algebra operators for efficient recursive XQuery stream processing. In particular we propose two strategies for implementing structural joins: (a) the just-in-time structural join strategy efficiently processes joins as long as the input XML substreams are non-recursive and (b) the recursive structural join strategy supports structural joins over recursive XML substreams, however at an added cost of tuple-level ID-comparisons. Both structural join strategies are complemented by an automatadriven invocation mechanism that triggers the execution of the join at the first possible moment upon recognizing the end of the targeted input stream subelement. Further, we design this structural join operator itself to be context-aware. The operator is capable of at run-time switching from the efficient just-intime join strategy for elements that are recognized to be nonrecursive to the more powerful id-based structural join strategy for elements that are identified to be recursive. In addition, depending on whether the query is recursive, we will generate the plan with cheaper operators whenever possible. We incorporate the proposed techniques into the Raindrop stream engine. We also report on experimental studies we conducted using ToXgene that show that our techniques brings significant performance improvement."
7FFE3F20,International Conference on Data Engineering,steven bird + yi chen + yifeng zheng + haejoong lee + susan b davidson,2006,Designing and Evaluating an XPath Dialect for Linguistic Queries,database languages + navigation + natural language processing + data mining + xml + engines + natural languages + human computer interaction + NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Linguistic research and natural language processing employ large repositories of ordered trees. XML, a standard ordered tree model, and XPath, its associated language, are natural choices for linguistic data and queries. However, several important expressive features required for linguistic queries are missing or hard to express in XPath. In this paper, we motivate and illustrate these features with a variety of linguistic queries. Then we propose extensions to XPath to support linguistic queries, and design an efficient query engine based on a novel labeling scheme. Experiments demonstrate that our language is not only sufficiently expressive for linguistic trees but also efficient for practical usage."
7F2264D8,International Conference on Data Engineering,xin zhou + carlo zaniolo + hetal thakkar,2006,Unifying the Processing of XML Streams and Relational Data Streams,data models + relational data + database languages + memory management + xml + NonControlled Keywords Not Found + Controlled Keywords Not Found + query language + data engineering,AuthorProvided Keywords Not Found,"Relational data streams and XML streams have previously provided two separate research foci, but their unified support by a single Data Stream Management System (DSMS) is very desirable from an application viewpoint. In this paper, we propose a simple approach to extend relational DSMSs to support both kinds of streams efficiently. In our Stream Mill system, XML streams expressed as SAX events, can be easily transformed into relational streams, and vice versa. This enables a close cooperation of their query languages, resulting in great power and flexibility. For instance, XQuery can call functions defined in our SQLbased Expressive Stream Language (ESL) using the logical/ physical windows that have proved so useful on relational data streams. Many benefits are also gained at the system level, since relational DSMS techniques for load shedding, memory management, query scheduling, approximate query answering, and synopsis maintenance can now be applied to XML streams. Moreover, the many FSA-based optimization techniques developed for XPath and XQuery can be easily and efficiently incorporated in our system. Indeed, we show that YFilter, which is capable of efficiently processing multiple complex XML queries, can be easily integrated in Stream Mill via ESL user-defined and systemdefined aggregates. This approach produces a powerful and flexible system where relational and XML streams are unified and processed efficiently."
7EF554DC,International Conference on Data Engineering,man lung yiu + nikos mamoulis,2006,Reverse Nearest Neighbors Search in Ad-hoc Subspaces,resource management + multidimensional systems + recurrent neural networks + euclidean distance + nearest neighbor + databases + distributed database + synthetic data + neural networks + missing values + indexation + computer science + spatial database,,"Given an object q, modeled by a multidimensional point, a reverse nearest neighbors (RNN) query returns the set of objects in the database that have q as their nearest neighbor. In this paper, we study an interesting generalization of the RNN query, where not all dimensions are considered, but only an ad-hoc subset thereof. The rationale is that (i) the dimensionality might be too high for the result of a regular RNN query to be useful, (ii) missing values may implicitly define a meaningful subspace for RNN retrieval, and (iii) analysts may be interested in the query results only for a set of (ad-hoc) problem dimensions (i.e., object attributes). We consider a suitable storage scheme and develop appropriate algorithms for projected RNN queries, without relying on multidimensional indexes. Our methods are experimentally evaluated with real and synthetic data."
7E3109FD,International Conference on Data Engineering,vebjorn ljosa + a k singh + a bhattacharya,2006,LB-Index: A Multi-Resolution Index Structure for Images,earth mover s distance + indexing + quadratic form + gray scale + indexation + linear program + NonControlled Keywords Not Found + lower bound + Controlled Keywords Not Found + multidimensional systems + feature vector + concatenated codes + feature extraction + earth + image retrieval,AuthorProvided Keywords Not Found,"In many domains, the similarity between two images depends on the spatial locations of their features. The earth moverês distance (EMD), first proposed by Werman et al. [8], measures such similarity. It yields higher-quality image retrieval results than the Lp-norm, quadratic-form distance, and Jeffrey divergence [6], and has also been used for similarity search on contours [3], melodies [7], and graphs [2]."
80F76D05,International Conference on Data Engineering,yanif ahmad + john jannotti + ugur cetintemel + olga papaemmanouil,2006,Application-aware Overlay Networks for Data Dissemination,computer science + cost function + data dissemination + overlay network + routing + logic + data type + prototypes,,"XPORT (eXtensible Profile-driven Overlay Routing Trees) is a generic data dissemination system that supports an extensible set of data types and profiles, and an optimization framework that facilitates easy specification of a wide range of useful performance goals. XPORT implements a tree-based overlay network, which can be customized per application using a small number of methods that encapsulate application-specific data-profile matching, profile aggregation, and cost optimization logic. The clean separation between the îplumbingî and îapplicationî enables XPORT to uniformly and easily support disparate dissemination-based applications, such as content-based data dissemination and multicast-based content distribution. In this short paper, we provide a high-level overview of XPORT. We also discuss its current implementation status, applications we built using XPORT, and some preliminary experimental results from the prototype. We finalize the paper by summarizing our future directions."
7F55A029,International Conference on Data Engineering,bernhard seeger + jurgen kramer + sonny vaupel + michael cammert,2006,An Approach to Adaptive Memory Management in Data Stream Systems,memory management + sampling methods + computer science + mathematics + algebra + NonControlled Keywords Not Found + quality of service + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Adaptivity is a challenging open issue in data stream management. In this paper, we tackle the problem of memory adaptivity inside a system executing temporal sliding window queries over continuous data streams. Two different techniques to control the memory usage at runtime are proposed which refer to changes in window sizes and time granularities. Both techniques differ from standard load shedding approaches based on sampling as they ensure precise query answers for user-defined Quality of Service (QoS) specifications, even under query re-optimization."
80FE4D16,International Conference on Data Engineering,hector gonzalez + jiawei han + diego klabjan + xiaolei li,2006,Warehousing and Analyzing Massive RFID Data Sets,data analysis + indexing + algorithm design and analysis + data structure + indexation + radio frequency identification + NonControlled Keywords Not Found + warehousing + Controlled Keywords Not Found + tracking + supply chain management + point of sale + object tracking + data structures + data cube,AuthorProvided Keywords Not Found,"Radio Frequency Identification (RFID) applications are set to play an essential role in object tracking and supply chain management systems. In the near future, it is expected that every major retailer will use RFID systems to track the movement of products from suppliers to warehouses, store backrooms and eventually to points of sale. The volume of information generated by such systems can be enormous as each individual item (a pallet, a case, or an SKU) will leave a trail of data as it moves through different locations. As a departure from the traditional data cube, we propose a new warehousing model that preserves object transitions while providing significant compression and path-dependent aggregates, based on the following observations: (1) items usually move together in large groups through early stages in the system (e.g., distribution centers) and only in later stages (e.g., stores) do they move in smaller groups, and (2) although RFID data is registered at the primitive level, data analysis usually takes place at a higher abstraction level. Techniques for summarizing and indexing data, and methods for processing a variety of queries based on this framework are developed in this study. Our experiments demonstrate the utility and feasibility of our design, data structure, and algorithms."
7F8F6D96,International Conference on Data Engineering,latanya sweeney + bradley malin,2006,Composition and Disclosure of Unlinkable Distributed Databases,distributed database + couplings + law + computer science + dna + distributed databases + secure multiparty computation + cryptography + data privacy + NonControlled Keywords Not Found + Controlled Keywords Not Found + protocols,AuthorProvided Keywords Not Found,"An individualês location-visit pattern, or trail, can be leveraged to link sensitive data back to identity. We propose a secure multiparty computation protocol that enables locations to provably prevent such linkages. The protocol incorporates a controllable parameter specifying the minimum number of identities a sensitive piece of data must be linkable to via its trail."
7F49AFA7,International Conference on Data Engineering,akira maeda + zakia ferdousi,2006,Unsupervised Outlier Detection in Time Series Data,intrusion detection + data mining + electronics packaging + time series + time series data + outlier detection + data security + data engineering + time series analysis + stock exchange,Outlier Detection + Fraud Detection + Time Series Data + Data Mining + Peer Group Analysis,"Fraud detection is of great importance to financial institutions. This paper is concerned with the problem of finding outliers in time series financial data using Peer Group Analysis (PGA), which is an unsupervised technique for fraud detection. The objective of PGA is to characterize the expected pattern of behavior around the target sequence in terms of the behavior of similar objects, and then to detect any difference in evolution between the expected pattern and the target. The tool has been applied to the stock market data, which has been collected from Bangladesh Stock Exchange to assess its performance in stock fraud detection. We observed PGA can detect those brokers who suddenly start selling the stock in a different way to other brokers to whom they were previously similar. We also applied t-statistics to find the deviations effectively."
803ABD77,International Conference on Data Engineering,arbee l p chen + chiahan lin,2006,Approximate Video Search Based on Spatio-Temporal Information of Video Objects,information retrieval + indexing + data models + distributed computing + internet + computer science,spatio-temporal information + approximate match + video retrieval + video model,"The spatio-temporal information of a video object is important for content-based video retrieval. In this paper the spatio-temporal information of a video object, such as the velocity, location and orientation of a video object, is represented by an ST-string. Since the user may not be interested in all spatio-temporal information, the user query can be represented by a QST-string which contains some spatio-temporal information only, such as the velocity and orientation of a video object. Therefore, the video retrieval problem can be transformed into the problem of approximately matching a QST-string with ST-strings. A general solution is proposed in this paper, which includes a similarity measure, an index structure and the corresponding matching methodology. The experiment results show that both the approximate and exact results can be found efficiently via the proposed index structure and the matching algorithm."
8057A9EC,International Conference on Data Engineering,yiping ke + wilfred ng + james cheng,2006,MIC Framework: An Information-Theoretic Approach to Quantitative Association Rule Mining,databases + computer science + data mining + association rules + NonControlled Keywords Not Found + mutual information + aging + Controlled Keywords Not Found + association rule mining,AuthorProvided Keywords Not Found,"We propose a framework, called MIC, which adopts an information-theoretic approach to address the problem of quantitative association rule mining. In our MIC framework, we first discretize the quantitative attributes. Then, we compute the normalized mutual information between the attributes to construct a graph that indicates the strong informative-relationship between the attributes. We utilize the cliques in the graph to prune the unpromising attribute sets and hence the joined intervals between these attributes. Our experimental results show that the MIC framework significantly improves the mining speed. Importantly, we are able to obtain most of the high-confidence rules and the missing rules are shown to be less interesting."
815CF84E,International Conference on Data Engineering,jeanroch meurisse + philippe thiran + jeanluc hainaut + fabrice estievenart,2006,Semi-Automated Extraction of Targeted Data fromWeb Pages,information management + xml + software agents + tree structure + data mining + world wide web + semantic interpretation + computer science + html,,"The World Wide Web can be considered an infinite source of information for both individuals and organizations. Yet, if the main standard of publication on the Web (HTML) is quite suited to human reading, its poor semantics makes it difficult for computers to process and use embedded data in a smart and automated way. In this paper, we propose to build a bridge between HTML documents and external applications by means of socalled mapping rules. Such rules mainly record a semantic interpretation of recurring types of information in a cluster of similar Web documents and their location in those documents. Relying on these rules, HTML-embedded data can be extracted towards a more computable format. The definition of mapping rules is based on direct user input mainly for the interpretation part, and on automatic computing for the location of data in HTML tree structures. This approach is supported by a user-friendly tool called Retrozilla."
7EE67F7A,International Conference on Data Engineering,anish das sarma + jennifer widom + omar benjelloun + alon halevy,2006,Working Models for Uncertain Data,prototypes + space exploration + NonControlled Keywords Not Found + data visualization + Controlled Keywords Not Found + relational databases + uncertainty + expressive power + database systems,AuthorProvided Keywords Not Found,"This paper explores an inherent tension in modeling and querying uncertain data: simple, intuitive representations of uncertain data capture many application requirements, but these representations are generally incomplete_standard operations over the data may result in unrepresentable types of uncertainty. Complete models are theoretically attractive, but they can be nonintuitive and more complex than necessary for many applications. To address this tension, we propose a two-layer approach to managing uncertain data: an underlying logical model that is complete, and one or more working models that are easier to understand, visualize, and query, but may lose some information. We explore the space of incomplete working models, place several of them in a strict hierarchy based on expressive power, and study their closure properties. We describe how the two-layer approach is being used in our prototype DBMS for uncertain data, and we identify a number of interesting open problems to fully realize the approach."
7E89F806,International Conference on Data Engineering,sudipto guha + nick koudas + xiaohui yu + divesh srivastava,2006,Reasoning About Approximate Match Query Results,databases + pressing + data cleaning + production systems + information retrieval + parameter estimation + degradation + NonControlled Keywords Not Found + Controlled Keywords Not Found + circuits,AuthorProvided Keywords Not Found,"Join techniques deploying approximate match predicates are fundamental data cleaning operations. A variety of predicates have been utilized to quantify approximate match in such operations and some have been embedded in a declarative data cleaning framework. These techniques return pairs of tuples from both relations, tagged with a score, signifying the degree of similarity between the tuples in the pair according to the specific approximate match predicate. In this paper, we consider the problem of estimating various parameters on the output of declarative approximate join algorithms for planning purposes. Such algorithms are highly time consuming, so precise knowledge of the result size as well as its score distribution is a pressing concern. This knowledge aids decisions as to which operations are more promising for identifying highly similar tuples, which is a key operation for data cleaning. We propose solution strategies that fully comply with a declarative framework and analytically reason about the quality of the estimates we obtain as well as the performance of our strategies. We present the results of a detailed performance evaluation of all strategies proposed. Our experimental results validate our analytical expectations and shed additional light on the quality and performance of our estimation framework. Our study offers a set of simple, fully declarative techniques for this problem, which can be readily deployed in data cleaning systems."
80F43CEB,International Conference on Data Engineering,jurgen kramer + alexander zeiss + tobias riemenschneider + christoph heinz + maxim schwarzkopf + bernhard seeger + michael cammert,2006,Stream Processing in Production-to-Business Software,application software + data analysis + business + stream processing + production + NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In order to support continuous queries over data streams, a plethora of suitable techniques as well as prototypes have been developed and evaluated in recent years. In particular, it is of utmost importance to confirm their necessity and feasibility in real-world applications. For that reason, we have successfully coupled our infrastructure for data stream processing (PIPES) with an industrial Production-to-Business software (i-Plant) dedicated to highly automated manufacturing processes."
7FA59658,International Conference on Data Engineering,stavros harizopoulos + ippokratis pandis + anastassia ailamaki + vladislav shkapenyuk + kun gao,2006,Simultaneous Pipelining in QPipe: Exploiting Work Sharing Opportunities Across Queries,resource management + databases + graphical user interfaces + systems + engines + data warehousing + NonControlled Keywords Not Found + warehousing + Controlled Keywords Not Found + concurrent computing + independent set + time sharing,AuthorProvided Keywords Not Found,"Data warehousing and scientific database applications operate on massive datasets and are characterized by complex queries accessing large portions of the database. Concurrent queries often exhibit high data and computation overlap, e.g., they access the same relations on disk, compute similar aggregates, or share intermediate results. Unfortunately, run-time sharing in modern database engines is limited by the paradigm of invoking an independent set of operator instances per query, potentially missing sharing opportunities if the buffer pool evicts data early."
7FC13AAF,International Conference on Data Engineering,beng chin ooi + hua lu + christian s jensen + zhiyong huang,2006,Skyline Queries Against Mobile Lightweight Devices in MANETs,mobile device + mobile ad hoc network + mobile communication + computer science + wireless ad hoc network + mobile ad hoc networks + information retrieval + NonControlled Keywords Not Found + Controlled Keywords Not Found + computational modeling + wireless communication,AuthorProvided Keywords Not Found,"Skyline queries are well suited when retrieving data according to multiple criteria. While most previous work has assumed a centralized setting this paper considers skyline querying in a mobile and distributed setting, where each mobile device is capable of holding only a portion of the whole dataset; where devices communicate through mobile ad hoc networks; and where a query issued by a mobile user is interested only in the userês local area, although a query generally involves data stored on many mobile devices due to the storage limitations. We present techniques that aim to reduce the costs of communication among mobile devices and reduce the execution time on each single mobile device. For the former, skyline query requests are forwarded among mobile devices in a deliberate way, such that the amount of data to be transferred is reduced. For the latter, specific optimization measures are proposed for resource-constrained mobile devices. We conduct extensive experiments to show that our proposal performs efficiently in real mobile devices and simulated wireless ad hoc networks."
7D9B88AA,International Conference on Data Engineering,k s candan + ping lin,2006,Access-Private Outsourcing of Markov Chain and RandomWalk based Data Analysis Applications,outsourcing + data security + system analysis + data analysis + information security + service provider + a priori knowledge + data engineering + markov chain + probability distribution + application software + data privacy + grid computing + random walk + databases,,""
80D86752,International Conference on Data Engineering,tetsuji satoh + yutaka yanagisawa,2006,Clustering Multidimensional Trajectories based on Shape and Velocity,data mining + multidimensional systems + information analysis + information system + navigation + information systems + geographic information systems + time series data,,"Recently, the analysis of moving objects has become one of the most important technologies to be used in various applications such as GIS, navigation systems, and locationbased information systems, Existing geographic analysis approaches are based on points where each object is located at a certain time. These techniques can extract interesting motion patterns from each moving object, but they can not extract relative motion patterns from many moving objects. Therefore, to retrieve moving objects with a similar trajectory shape to another given moving object, we propose queries based on the similarity between the shapes of moving object trajectories. Our proposed technique can find trajectories whose shape is similar to a certain given trajectory. We define the shape-based similarity query trajectories as an extension of similarity queries for time series data, and then we propose a new clustering technique based on similarity by combining both velocities of moving objects and shapes of objects. Moreover, we show the effectiveness of our proposed clustering method through a performance study using moving rickshaw data."
8008E389,International Conference on Data Engineering,elke michlmayr,2006,Ant Algorithms for Search in Unstructured Peer-to-Peer Networks,ant colony optimization + intelligent networks + robustness + routing,,"in rceaSh fro s iltroghm"
7F7B6AEE,International Conference on Data Engineering,nuwee wiwatwattana + h v jagadish + divesh srivastava + laks v s lakshmanan,2006,Making Designer Schemas with Colors,design methodology + databases + data model + erbium + algorithm design and analysis + NonControlled Keywords Not Found + Controlled Keywords Not Found + normal form + data engineering + markup languages + data models + satisfiability + xml + xml schema,AuthorProvided Keywords Not Found,"XML schema design has two opposing goals: elimination of update anomalies requires that the schema be as normalized as possible; yet higher query performance and simpler query expression are often obtained through the use of schemas that permit redundancy. In this paper, we show that the recently proposed MCT data model, which extends XML by adding colors, can be used to address this dichotomy effectively. Specifically, we formalize the intuition of anomaly avoidance in MCT using notions of node normal and edge normal forms, and the goal of efficient query processing using notions of association recoverability and direct recoverability. We develop algorithms for transforming design specifications given as ER diagrams into MCT schemas that are in a node or edge normal form and satisfy association or direct recoverability. Experimental results using a wide variety of ER diagrams validate the benefits of our design methodology."
80E62792,International Conference on Data Engineering,mor harcholbalter + arun iyengar + erich m nahum + bianca schroeder,2006,Achieving Class-Based QoS for Transactional Workloads,application software + queue management + switches + e commerce + NonControlled Keywords Not Found + quality of service + Controlled Keywords Not Found + adaptive systems + computer science + control systems + throughput + level of service + database systems,AuthorProvided Keywords Not Found,"Transaction processing systems lie at the core of modern e-commerce applications such as on-line retail stores, banks and airline reservation systems. The economic success of these applications depends on the ability to achieve high user satisfaction, since a single mouse-click is all that it takes a frustrated user to switch to a competitor. Given that system resources are limited and demands are varying, it is difficult to provide optimal performance to all users at all times. However, often transactions can be divided into different classes based on how important they are to the online retailer. For example, transactions initiated by a ""big spending"" client are more important than transactions from a client that only browses the site. A natural goal then is to ensure short delays for the class of important transactions, while for the less important transactions longer delays are acceptable."
7DD4640D,International Conference on Data Engineering,subbarao kambhampati + ullas nambiar,2006,Answering Imprecise Queries over Autonomous Web Databases,databases + distance metric + computer science + robustness + functional dependency + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"Current approaches for answering queries with imprecise constraints require user-specific distance metrics and importance measures for attributes of interest - metrics that are hard to elicit from lay users. We present AIMQ, a domain and user independent approach for answering imprecise queries over autonomous Web databases. We developed methods for query relaxation that use approximate functional dependencies. We also present an approach to automatically estimate the similarity between values of categorical attributes. Experimental results demonstrating the robustness, efficiency and effectiveness of AIMQ are presented. Results of a preliminary user study demonstrating the high precision of the AIMQ system is also provided."
8019ED7C,International Conference on Data Engineering,christian mathis + theo harder,2006,Twig Query Processing Under Concurrent Updates,database systems + database languages + assembly + information systems + xml + xml database + data models + management system + memory,,"An appropriate database language characteristics leading to the success of declarative query processing-and, in turn, to the rise of relational DBMSs in general-always provides more than one way of evaluating a query. This counts for structurally different but logically equivalent query evaluation plans (QEPs) as well as for different implementations of the same logical operator. This principle surely holds for the novel XML database management systems (XDBMSs): Recently proposed operators for XML query processing can be grouped into the logical operators Structural Join [1, 22] and Holistic Twig Join [3, 6, 16]. Depending on available internal system mechanisms, a lot of opportunities exist how to implement these operators (two of which are presented in this paper)."
7F21F5FA,International Conference on Data Engineering,chihfang wang + zhewei jiang + wenchi hou + feng yan + cheng luo,2006,Estimating XML Structural Join Size Quickly and Economically,cosine transform + query optimization + computer science + wavelet transforms + xml + wavelet transform + relational model + NonControlled Keywords Not Found + Controlled Keywords Not Found + tree graphs + internet + statistics,AuthorProvided Keywords Not Found,"XML structural joins, which evaluate the containment (ancestor-descendant) relationships between XML elements, are important operations of XML query processing. Estimating structural join size accurately and quickly is thus crucial to the success of XML query plan selection and the query optimization. XML structural joins are essentially complex unequal joins, which render well-known estimation techniques, such as cosine transform, wavelet transform, and sketch, not directly applicable. In this paper, we propose a relation model to capture the structural information of XML data such that the original complex unequal joins are converted to equal joins and those well-known estimation techniques become directly applicable to structural join size estimation. Theoretical analyses and extensive experiments have been performed on these estimation methods. It is shown that the cosine transform requires the least memory and yields the best estimates."
7CF97E6F,International Conference on Data Engineering,beechung chen + vinod yegneswaran + raghu ramakrishnan + paul barford,2006,Toward a Query Language for Network Attack Data,intrusion detection system + data mining + intrusion detection + database languages + data engineering + internet + data security + computer security + computer networks + query language,,"The growing sophistication and diversity of malicious activity in the Internet presents a serious challenge for network security analysts. In this paper, we describe our efforts to develop a database and query language for network attack data from firewalls, intrusion detection systems and honeynets. Our first step toward this objective is to develop a prototype database and query interface to identify coordinated scanning activity in network attack data. We have created a set of aggregate views and templatized SQL queries that consider timing, persistence, targeted services, spatial dispersion and temporal dispersion, thereby enabling us to evaluate coordinated scanning along these dimensions. We demonstrate the utility of the interface by conducting a case study on a set of firewall and intrusion detection system logs from Dshield.org. We show that the interface is able to identify general characteristics of coordinated activity as well as instances of unusual activity that would otherwise be difficult to mine from the data. These results highlight the potential for developing a more generalized query language for a broad class of network intrusion data. The case study also exposes some of the challenges we face in extending our system to more generalized queries over potentially vast quantities of data."
7F060E0E,International Conference on Data Engineering,xiaokui xiao + yufei tao + jian pei,2006,SUBSKY: Efficient Computation of Skylines in Subspaces,indexes + computer science + data security + information retrieval + degradation + NonControlled Keywords Not Found + relational database + Controlled Keywords Not Found + relational databases + scalability,AuthorProvided Keywords Not Found,"Given a set of multi-dimensional points, the skyline contains the best points according to any preference function that is monotone on all axes. In practice, applications that require skyline analysis usually provide numerous candidate attributes, and various users depending on their interests may issue queries regarding different (small) subsets of the dimensions. Formally, given a relation with a large number (e.g.,ge 10) of attributes, a query aims at finding the skyline in an arbitrary subspace with a low dimensionality (e.g., 2). The existing algorithms do not support subspace skyline retrieval efficiently because they (i) require scanning the entire database at least once, or (ii) are optimized for one particular subspace but incur significant overhead for other subspaces. In this paper, we propose a technique SUBSKY which settles the problem using a single B-tree, and can be implemented in any relational database. The core of SUBSKY is a transformation that converts multi-dimensional data to 1D values, and enables several effective pruning heuristics. Extensive experiments with real data confirm that SUBSKY outperforms alternative approaches significantly in both efficiency and scalability."
817A82DC,International Conference on Data Engineering,yawen hsu + rahul singh + naureen moon,2006,"A Multiple-Perspective, Interactive Approach for Web Information Extraction and Exploration",data visualization + information extraction + visual cues + computer science + moon + displays + indexing + data mining + web pages + user interface + user interfaces + navigation + pattern analysis,,"While increasing amounts of complex information are becoming available on the web, there is, beyond keywordbased search and listing of results, a paucity of user interface paradigms and implementations that support interaction, exploration, and assimilation of information. This paper describes our design of a novel framework to address this deficiency. The proposed framework supports both direct search behavior as well as more exploratory search strategies through multiple-perspective visualization and interaction with search results. The approach is developed around the twin themes of supporting data context and facilitating effective interactions between users and data. The system supports data context through determination of semantic correlations between web pages and extraction of the spatio-Temporal data contained therein. A multipleperspective environment is then used to display semantic and spatio-Temporal relationships as well as to provide intuitive views of the data, specifically through web page thumbnail, map, and timeline modules. The environment supports direct interactions with the data through a reflective interface by which user selections in any one panel highlight the corresponding information in other panels. In this environment, visual cues and explicit facilities to model space and time aid in recognition, querying, and exploration of information as well as in representation and reasoning with complex relationships (such as spatio-Temporal, causal, evolutionary) in the data. Experimental studies of a quantitative and qualitative nature demonstrate the efficacy of the system in facilitating both information extraction and discovery."
7F23B013,International Conference on Data Engineering,karl aberer + andras feher + philippe cudremauroux,2006,Probabilistic Message Passing in Peer Data Management Systems,mobile computing + message passing + data management + embedded computing + data integrity + NonControlled Keywords Not Found + Controlled Keywords Not Found + transitive closure + data engineering + probabilistic model,AuthorProvided Keywords Not Found,"Until recently, most data integration techniques involved central components, e.g., global schemas, to enable transparent access to heterogeneous databases. Today, however, with the democratization of tools facilitating knowledge elicitation in machine-processable formats, one cannot rely on global, centralized schemas anymore as knowledge creation and consumption are getting more and more dynamic and decentralized. Peer Data Management Systems (PDMS) provide an answer to this problem by eliminating the central semantic component and considering instead compositions of local, pair-wise mappings to propagate queries from one database to the others. PDMS approaches proposed so far make the implicit assumption that all mappings used in this way are correct. This obviously cannot be taken as granted in typical PDMS settings where mappings can be created (semi) automatically by independent parties. In this work, we propose a totally decentralized, efficient message passing scheme to automatically detect erroneous mappings in PDMS. Our scheme is based on a probabilistic model where we take advantage of transitive closures of mapping operations to confront local belief on the correctness of a mapping against evidences gathered around the network. We show that our scheme can be efficiently embedded in any PDMS and provide a preliminary evaluation of our techniques on sets of both automatically-generated and real-world schemas."
7F81EF9C,International Conference on Data Engineering,wei zhang + clement yu + neil r smalheiser + vetle i torvik,2006,Segmentation of Publication Records of Authors from the Web,databases + structured data + computer science + computer architecture + NonControlled Keywords Not Found + psychiatry + Controlled Keywords Not Found + data engineering + distributed computing,AuthorProvided Keywords Not Found,"Publication records are often found in the authorsê personal home pages. If such a record is partitioned into a list of semantic fields of authors, title, date, etc., the unstructured texts can be converted into structured data, which can be used in other applications. In this paper, we present PEPURS, a publication record segmentation system. It adopts a novel ""Split and Merge"" strategy. A publication record is split into segments; multiple statistical classifiers compute their likelihoods of belonging to different fields; finally adjacent segments are merged if they belong to the same field. PEPURS introduces the punctuation marks and their neighboring texts as a new feature to distinguish different roles of the marks. PEPURS yields high accuracy scores in experiments."
7D4B83D7,International Conference on Data Engineering,charu c aggarwal + jiawei han + chen chen,2006,On the Inverse Classification Problem and its Applications,sensitivity analysis + data mining + Decision Support + training data + testing + Classification + decision support + classification + decision support systems,Classification + Decision Support,"In this paper, we discuss the inverse classification problem, in which we desire to define the features of an incomplete record in such a way that will result in a desired class label. Such an approach is useful in applications in which it is an objective to determine a set of actions to be taken in order to guide the data mining application towards a desired solution. This system can be used for a variety of decision support applications which have pre-determined task criteria."
8112DAAB,International Conference on Data Engineering,m kataigi + ryo taguchi + dai kobayashi + tsutomu yoshihara + hidetoshi yokota + t kobayashi,2006,Evaluation of Placement and Access Asignment for Replicated Object Striping,history + quality of service + satisfiability + storage system + high throughput + throughput,,""
7EAC2824,International Conference on Data Engineering,xifeng yan + jiawei han + feida zhu + philip s yu,2006,Searching Substructures with Superimposed Distance,information system + artificial intelligent + similarity search + indexes + computer science + indexing + indexation + NonControlled Keywords Not Found + Controlled Keywords Not Found + chemicals + data engineering + cardiology,AuthorProvided Keywords Not Found,"Efficient indexing techniques have been developed for the exact and approximate substructure search in large scale graph databases. Unfortunately, the retrieval problem of structures with categorical or geometric distance constraints is not solved yet. In this paper, we develop a method called PIS (Partition-based Graph Index and Search) to support similarity search on substructures with superimposed distance constraints. PIS selects discriminative fragments in a query graph and uses an index to prune the graphs that violate the distance constraints. We identify a criterion to distinguish the selectivity of fragments in multiple graphs and develop a partition method to obtain a set of highly selective fragments, which is able to improve the pruning performance. Experimental results show that PIS is effective in processing real graph queries."
80C2818B,International Conference on Data Engineering,h v jagadish + beng chin ooi + rong zhang + aoying zhou + quang hieu vu,2006,VBI-Tree: A Peer-to-Peer Framework for Supporting Multi-Dimensional Indexing Schemes,binary trees + databases + indexing + tree structure + indexation + robustness + load balance + NonControlled Keywords Not Found + tree data structures + Controlled Keywords Not Found + multidimensional systems + range query,AuthorProvided Keywords Not Found,"Multi-dimensional data indexing has received much attention in a centralized database. However, not so much work has been done on this topic in the context of Peerto- Peer systems. In this paper, we propose a new Peer-to- Peer framework based on a balanced tree structure overlay, which can support extensible centralized mapping methods and query processing based on a variety of multidimensional tree structures, including R-Tree, X-Tree, SSTree, and M-Tree. Specifically, in a network with N nodes, our framework guarantees that point queries and range queries can be answered within O(logN) hops. We also provide an effective load balancing strategy to allow nodes to balance their work load efficiently. An experimental assessment validates the practicality of our proposal."
8018A9CA,International Conference on Data Engineering,samir tata + abhishek swaroop + j s friedman,2006,Declarative Querying for Biological Sequences,databases + pattern matching + data analysis + proteomics + amino acids + indexation + algebra + NonControlled Keywords Not Found + Controlled Keywords Not Found + sequences + protein engineering + genetics + relation algebra + dna,AuthorProvided Keywords Not Found,"The ongoing revolution in life sciences research is producing vast amounts of genetic and proteomic sequence data. Scientists want to pose increasingly complex queries on this data, but current methods for querying biological sequences are primitive and largely procedural. This limits the ease with which complex queries can be posed, and often results in very inefficient query plans. There is a growing and urgent need for declarative and efficient methods for querying biological sequence data. In this paper, we introduce a system called Periscope/SQ which addresses this need. Queries in our system are based on a well-defined extension of relational algebra. We introduce new physical operators and support for novel indexes in the database. As part of the optimization framework, we describe a new technique for selectivity estimation of string pattern matching predicates that is more accurate than previous methods. We also describe a simple, yet highly effective algorithm to optimize sequence queries. Finally, using a real-world application in eye genetics, we show how Periscope/SQ can be used to achieve a speedup of two orders of magnitude over existing procedural methods!"
80B3A18E,International Conference on Data Engineering,anastasios kementsietsidis + diego milano + floris geerts,2006,MONDRIAN: Annotating and Querying Databases through Colors and Blocks,data models + database languages + data model + prototypes + algebra + NonControlled Keywords Not Found + informatics + Controlled Keywords Not Found + query language + astronomy + protein sequence,AuthorProvided Keywords Not Found,"Annotations play a central role in the curation of scientific databases. Despite their importance, data formats and schemas are not designed to manage the increasing variety of annotations. Moreover, DBMSês often lack support for storing and querying annotations. Furthermore, annotations and data are only loosely coupled. This paper introduces an annotation-oriented data model for the manipulation and querying of both data and annotations. In particular, the model allows for the specification of annotations on sets of values and for effectively querying the information on their association. We use the concept of block to represent an annotated set of values. Different colors applied to the blocks represent different annotations. We introduce a color query language for our model and prove it to be both complete (it can express all possible queries over the class of annotated databases), and minimal (all the algebra operators are primitive). We present MONDRIAN, a prototype implementation of our annotation mechanism, and we conduct experiments that investigate the set of parameters which influence the evaluation cost for color queries."
8169A117,International Conference on Data Engineering,masaru kitsuregawa + takayuki tamura + kulwadee somboonviwat,2006,Finding Thai Web Pages in Foreign Web Spaces,information technology + web server + space technology + web crawling + web pages,,This paper proposes language specific web crawling (LSWC) as a method of creating large-scale language specific Web archives for countries with linguistic identities such as Thailand. The LSWC strategy for selectively gathering Thai web pages from virtually anywhere on the Web is derived based on the results of static analyses of the Thai Web graph. We evaluated the performance of the LSWC strategy using a web crawling simulator.
7F946398,International Conference on Data Engineering,curtis e dyreson + richard t snodgrass + sabah currim + faiz currim + shailesh joshi,2006,"Validating Quicksand: Schema Versioning in 	auXSchema",genomics + database systems + xml document + data type + data structures + solids + data engineering + data models + bioinformatics + xml schema + xml,,"The W3C XML Schema recommendation defines the structure and data types for XML documents, but lacks explicit support for time-varying XML documents or for a time-varying schema. In previous work we introduced _XSchema which is an infrastructure and suite of tools to support the creation and validation of time-varying documents, without requiring any changes to XML Schema. In this paper we extend _XSchema to support versioning of the schema itself. We introduce the concept of a bundle, which is an XML document that references a base (nontemporal) schema, temporal annotations describing how the document can change, and physical annotations describing where timestamps are placed. When the schema is versioned, the base schema and temporal and physical schemas can themselves be time-varying documents, each with their own (possibly versioned) schemas. We describe how the validator can be extended to validate documents in this seeming precarious situation of data that changes over time, while its schema and even its representation are also changing."
80B8A724,International Conference on Data Engineering,dejing dou + shiwoong kim + peishen qi + paea lependu,2006,Integrating Databases into the Semantic Web through an Ontology-Based Framework,engines + knowledge base + information science + first order + ontologies + world wide web + owl + computer science + semantic web + application software + relational databases,,"To realize the Semantic Web, it will be necessary to make existing database content available for emerging Semantic Web applications, such as web agents and services, which use ontologies to formally define the semantics of their data. Our research in the design and implementation of an ontology-based system, OntoGrate, addresses the critical and challenging problem of supporting human experts in multiple domains to interactively integrate information that is heterogenous in both structure and semantics. Databases, knowledge bases, the World Wide Web, and the emerging Semantic Web are some of the resources for which scalable integration remains a challenge. To integrate databases into the Semantic Web, we use Semantic Web ontologies to incorporate database schemas. An expressive first order ontology language, Web-PDDL, is used to define the structure, semantics, and mappings of data resources. A powerful inference engine, OntoEngine, can be used for query answering and data translation. In this paper, besides introducing new ideas in the OntoGrate system, we will elaborate on two case studies for which our system works well."
8172F730,International Conference on Data Engineering,alper okcan + veli bicer + asuman dogac,2006,Collaborative Business Process Support in IHE XDS through ebXML Business Processes,information flow + business process + NonControlled Keywords Not Found + information systems + Controlled Keywords Not Found + health care + dicom,AuthorProvided Keywords Not Found,"Currently, clinical information is stored in all kinds of proprietary formats through a multitude of medical information systems available on the market. This results in a severe interoperability problem in sharing electronic healthcare records. To address this problem, an industry initiative, called ""Integrating Healthcare Enterprise (IHE)"" has specified the ""Cross Enterprise Document Sharing (XDS)"" Profile to store healthcare documents in an ebXML registry/ repository to facilitate their sharing. Through a separate effort, IHE has also defined interdepartmental Workflow Profiles to identify the transactions required to integrate information flow among several information systems. Although the clinical documents stored in XDS registries are obtained as a result of executing these workflows, IHE has not yet specified collaborative healthcare processes for the XDS. Hence, there is no way to track the workflows in XDS and the clinical documents produced through the workflows are manually inserted into the registry/ repository. Given that IHE XDS is using the ebXML architecture, the most natural way to integrate IHE Workflow Profiles to IHE XDS is using ebXML Business Processes (ebBP). In this paper, we describe the implementation of an enhanced IHE architecture demonstrating how ebXML Business Processes, IHE Workflow Profiles and the IHE XDS architecture can all be integrated to provide collaborative business process support in the healthcare domain."
7D39B1A5,International Conference on Data Engineering,na zhao + shuching chen + meiling shyu,2006,Video Database Modeling and Temporal Pattern Retrieval using Hierarchical Markov Model Mediator,hidden markov models + feedback + stochastic processes + information retrieval + indexation + indexing + information system + information systems,,"The dream of pervasive multimedia retrieval and reuse will not be realized without incorporating semantics in the multimedia database. As video data is penetrating many information systems, the need for database support for video data evolves. Hence, we propose an innovative database modeling mechanism called Hierarchical Markov Model Mediator (HMMM) which integrates lowlevel features, semantic concepts, and high-level user perceptions for modeling and indexing multiple-level video objects to facilitate temporal pattern retrieval. Different from the existing database modeling methods, our approach carries a stochastic and dynamic process in both search and similarity calculation. In the retrieval of semantic event patterns, HMMM always tries to traverse the right path and therefore it can assist in retrieving more accurate patterns quickly with lower computational costs. Moreover, HMMM supports feedbacks and learning strategies, which can proficiently assure the continuous improvements of the overall performance."
7E6C0BAC,International Conference on Data Engineering,michael morse + william i grosky + jignesh m patel,2006,Efficient Continuous Skyline Computation,scalability + databases + indexation,Database + Skyline + Algorithm + Index + Multidimensional data structures + Quadtree + R*-tree,"In a number of emerging streaming applications, the data values that are produced have an associated time interval for which they are valid. A useful computation over such streaming data is to produce a continuous and valid skyline summary. Previous work on skyline algorithms have only focused on evaluating skylines over static data sets, and there are no known algorithms for skyline computation in the continuous setting. In this paper, we introduce the continuous time-interval skyline operator, which continuously computes the current skyline over a data stream. We present a new algorithm called LookOut for evaluating such queries efficiently, and empirically demonstrate the scalability of this algorithm. In addition, we also examine the effect of the underlying spatial index structure when evaluating skylines. Whereas previous work on skyline computations have only considered using the R*-tree index structure, we show that for skyline computations using an underlying quadtree has significant performance benefits over an R*-tree index. 2007 Elsevier Inc. All rights reserved. The skyline operator is an elegant summary method over multidimensional datasets [14]. Given a dataset P containing data points p1; p2; . . . ; pn, the skyline of P is the set of all pi in P such that no pj dominates pi. A commonly cited example for the use of a skyline operator is assisting a tourist in choosing a set of interesting hotels from a larger set of candidate hotels. Each hotel is identified by two attributes: a distance from a specific point (such as a location on a beach), and the price for the hotel. To assist a tourist in narrowing down the choices, the skyline operator can be used to find the set of all hotels that are not dominated by another hotel. Hotel a dominates hotel b if a is at least as close as b and at least as cheap as b, and offers either a better price, or is closer, or both compared to b. Fig. 1 shows an example dataset and the corresponding skyline; the distance of the hotel from the beach is shown on the x-axis and the hotel price is plotted along the y-axis. The skyline is the set of points a, c, d, i, and j."
7E266419,International Conference on Data Engineering,jyotishman pathak + vasant honavar + robyn r lutz + samik basu,2006,MoSCoE: A Framework for Modeling Web Service Composition and Execution,computational intelligence + ontologies + software design + assembly + web service + computer science + user requirements + web services + collaborative software + propulsion,,"Development of sound approaches and software tools for specification, assembly, and deployment of composite Web services from independently developed components promises to enhance collaborative software design and reuse. In this context, the proposed research introduces a new incremental approach to service composition, MoSCoE (Modeling Web Service Composition and Execution), based on the three steps of abstraction, composition and refinement. Abstraction refers to the high-level description of the service desired (goal) by the user, which drives the identification of an appropriate composition strategy. In the event that such a composition is not realizable, MoSCoE guides the user through successive refinements of the specification towards a realizable goal service that meets the user requirements."
814962BC,International Conference on Data Engineering,peter pietzuch + mema roussopoulos + matt welsh + margo seltzer + jonathan ledlie + jeffrey shneidman,2006,Network-Aware Operator Placement for Stream-Processing Systems,simulation experiment + query optimization + stream processing + cost function + intrusion detection + NonControlled Keywords Not Found + logic + Controlled Keywords Not Found + filtering + digital signal processing,AuthorProvided Keywords Not Found,"To use their pool of resources efficiently, distributed stream-processing systems push query operators to nodes within the network. Currently, these operators, ranging from simple filters to custom business logic, are placed manually at intermediate nodes along the transmission path to meet application-specific performance goals. Determining placement locations is challenging because network and node conditions change over time and because streams may interact with each other, opening venues for reuse and repositioning of operators. This paper describes a stream-based overlay network (SBON), a layer between a stream-processing system and the physical network that manages operator placement for stream-processing systems. Our design is based on a cost space, an abstract representation of the network and on-going streams, which permits decentralized, large-scale multi-query optimization decisions. We present an evaluation of the SBON approach through simulation, experiments on PlanetLab, and an integration with Borealis, an existing stream-processing engine. Our results show that an SBON consistently improves network utilization, provides low stream latency, and enables dynamic optimization at low engineering cost."
7FBEEE64,International Conference on Data Engineering,stefan seltzsam + daniel gmach + stefan krompass + alfons kemper,2006,AutoGlobe: An Automatic Administration Concept for Service-Oriented Database Applications,databases + application software + automatic control + software systems + self organization + fuzzy logic + NonControlled Keywords Not Found + service oriented architecture + Controlled Keywords Not Found + hardware,AuthorProvided Keywords Not Found,"Future database application systems will be designed as Service Oriented Architectures (SOAs) like SAPês NetWeaver instead of monolithic software systems such as SAPês R/3. The decomposition in finer-grained services allows the usage of hardware clusters and a flexible serviceto- server allocation but also increases the complexity of administration. Thus, new administration techniques like our self-organizing infrastructure that we developed in cooperation with the SAP Adaptive Computing Infrastructure (ACI) group are necessary. For our purpose the available hardware is virtualized, pooled, and monitored. A fuzzy logic based controller module supervises all services running on the hardware platform and remedies exceptional situations automatically. With this self-organizing infrastructure we reduce the necessary hardware and administration overhead and, thus, lower the total cost of ownership (TCO). We used our prototype implementation, called Auto- Globe, for SAP-internal tests and we performed comprehensive simulation studies to demonstrate the effectiveness of our proposed concept."
7FC85C82,International Conference on Data Engineering,beng chin ooi + feng yu + kianlee tan + yongluan zhou,2006,Adaptive Reorganization of Coherency-Preserving Dissemination Tree for Streaming Data,fluctuations + financial management + xml + cost effectiveness + NonControlled Keywords Not Found + Controlled Keywords Not Found + statistics,AuthorProvided Keywords Not Found,"In many recent applications, data are continuously being disseminated from a source to a set of servers. In this paper, we propose a cost-based approach to construct dissemination trees to minimize the average loss of fidelity of the system. Our cost model takes into account both the processing cost and the communication cost. To adapt to inaccurate statistics, runtime fluctuations of data characteristics, server workloads, and network conditions etc., we propose a runtime adaptive scheme to incrementally transform a dissemination tree to a more cost-effective one. Given apriori statistics of the system, we propose two static algorithms to construct a dissemination tree for relatively static environments. The performance study shows that the adaptive mechanisms are effective in a dynamic context and the proposed static tree construction algorithms perform close to optimal in a static environment."
81214F62,International Conference on Data Engineering,ning zhang + m t ozsu + ihab f ilyas + a a aboulnaga,2006,XSEED: Accurate and Fast Cardinality Estimation for XPath Queries,histograms + computer science + kernel + xml + testing + robustness + cost function + NonControlled Keywords Not Found + Controlled Keywords Not Found + statistical distributions + relational databases + tree graphs,AuthorProvided Keywords Not Found,"We propose XSEED, a synopsis of path queries for cardinality estimation that is accurate, robust, efficient, and adaptive to memory budgets. XSEED starts from a very small kernel, and then incrementally updates information of the synopsis. With such an incremental construction, a synopsis structure can be dynamically configured to accommodate different memory budgets. Cardinality estimation based on XSEED can be performed very efficiently and accurately. Extensive experiments on both synthetic and real data sets show that even with less memory, XSEED could achieve accuracy that is an order of magnitude better than that of other synopsis structures. The cardinality estimation time is under 2% of the actual querying time for a wide range of queries in all test cases."
7D529376,International Conference on Data Engineering,haixun wang + jun yang + philip s yu + hao he + jeffrey xu yu,2006,Dual Labeling: Answering Graph Reachability Queries in Constant Time,internet routing + data analysis + indexing + owl + indexation + NonControlled Keywords Not Found + Controlled Keywords Not Found + resource description framework + routing + navigation + xml + labeling + internet + ontologies,AuthorProvided Keywords Not Found,"Graph reachability is fundamental to a wide range of applications, including XML indexing, geographic navigation, Internet routing, ontology queries based on RDF/OWL, etc. Many applications involve huge graphs and require fast answering of reachability queries. Several reachability labeling methods have been proposed for this purpose. They assign labels to the vertices, such that the reachability between any two vertices may be decided using their labels only. For sparse graphs, 2-hop based reachability labeling schemes answer reachability queries efficiently using relatively small label space. However, the labeling process itself is often too time consuming to be practical for large graphs. In this paper, we propose a novel labeling scheme for sparse graphs. Our scheme ensures that graph reachability queries can be answered in constant time. Furthermore, for sparse graphs, the complexity of the labeling process is almost linear, which makes our algorithm applicable to massive datasets. Analytical and experimental results show that our approach is much more efficient than stateof- the-art approaches. Furthermore, our labeling method also provides an alternative scheme to tradeoff query time for label space, which further benefits applications that use tree-like graphs."
8.05E+18,International Conference on Data Engineering,johannes gehrke + d riley + c d jones + mirek riedewald + james m cordes + p p dmitriev + a ryd + manuel calimlim + david lifka + g j sharp + j s deneva + v e kuznetsov + gehrke + selcuk aya + l gibbons + william y arms,2006,Three Case Studies of Large-Scale Data Flows,space technology + physics + computer aided software engineering + face + internet + arm + astronomy + computer science,,"We survey three examples of large-scale scientific workflows that we are working with at Cornell: The Arecibo sky survey, the CLEO high-energy particle physics experiment, and the Web Lab project for enabling social science studies of the Internet. All three projects face the same general challenges: massive amounts of raw data, expensive processing steps, and the requirement to make raw data or data products available to users nation- or world-wide. However, there are several differences that prevent a one-sizefits-All approach to handling their data flows. Instead, current implementations are heavily tuned by domain and data management experts. We describe the three projects, and we outline research issues and opportunities to integrate Grid technology into these workflows."
7D646D1B,International Conference on Data Engineering,feifei li + azer bestavros + ching chang + george kollios,2006,Characterizing and Exploiting Reference Locality in Data Stream Applications,application software + computer science + algorithm design and analysis + mathematical model + NonControlled Keywords Not Found + Controlled Keywords Not Found + sliding window + database systems,AuthorProvided Keywords Not Found,"In this paper, we investigate a new approach to process queries in data stream applications. We show that reference locality characteristics of data streams could be exploited in the design of superior and flexible data stream query processing techniques. We identify two different causes of reference locality: popularity over long time scales and temporal correlations over shorter time scales. An elegant mathematical model is shown to precisely quantify the degree of those sources of locality. Furthermore, we analyze the impact of locality-awareness on achievable performance gains over traditional algorithms on applications such asMAX-subset approximate sliding window join and approximate count estimation. In a comprehensive experimental study, we compare several existing algorithms against our locality-aware algorithms over a number of real datasets. The results validate the usefulness and efficiency of our approach."
810D9218,International Conference on Data Engineering,alkis simitsis + yannis ioannidis + georgia koutrika,2006,Pr&#233;cis: The Essence of a Query Answer,query language + relational database + relational databases + relational database system + network address translation + database languages + data models + system architecture + dictionaries + database system + writing + database systems,,"Wide spread use of database systems in modern society has brought the need to provide inexperienced users with the ability to easily search a database with no specific knowledge of a query language. Several recent research efforts have focused on supporting keyword-based searches over relational databases. This paper presents an alternative proposal and introduces the idea of pr_cis queries. These are free-form queries whose answer (a pr_cis) is a synthesis of results, containing not only information directly related to the query selections but also information implicitly related to them in various ways. Our approach to pr_cis queries includes two additional novelties: (a) queries do not generate individual relations but entire multi-relation databases; and (b) query results are personalized to user-specific and/or domain requirements. We develop a framework and system architecture for supporting such queries in the context of a relational database system and describe algorithms that implement the required functionality. Finally, we present a set of experimental results that evaluate the proposed algorithms and show the potential of this work."
7D52F930,International Conference on Data Engineering,george pallis + athena vakali + konstantinos stamos + dimitrios katsaros + antonis sidiropoulos,2006,Replication Based on Objects Load under a Content Distribution Network,bandwidth + terrorism + synthetic data + web services + e commerce + data management + quality of service + web server,,"Users tend to use the Internet for _resource-hungryî applications (which involve content such as video, audio on-demand and distributed data) and at the same time, more and more applications (such as e-commerce, elearning etc.) are relying on the Web. In this framework, Content Distribution Networks (CDNs) are increasingly being used to disseminate data in today's Internet, providing a delicate balance between costs for Web content providers and quality of services for Web customers. The growing interest in CDNs is motivated by a common problem across disciplines: how does one reduce the load on the origin server and the traffic on the Internet, and ultimately improve response time to users? In this direction, crucial data management issues should be addressed. A very important issue is the optimal placement of the outsourced content to CDN's servers. Taking into account that this problem is NP complete, an heuristic method should be developed. All the approaches developed so far either take as criterion the network's latency or the workload. This paper develops a novel technique to place the outsourced content to CDN's servers, integrating both the latency and the load. Through a detailed simulation environment, using both real and synthetic data, we show that the proposed method can improve significantly the response time of requests while keeping the CDNs' servers' load at a very low level."
7D9B97E7,International Conference on Data Engineering,percy bernedo + prasanna padmanabhan + le gruenwald,2006,PETRANET: a Power Efficient Transaction Management Technique for Real-Time Mobile Ad-hoc Network Databases,mobile ad hoc network + disaster management + mobile ad hoc networks + switches + energy management + voting + central processing unit + NonControlled Keywords Not Found + Controlled Keywords Not Found + ad hoc networks + real time + power efficiency,AuthorProvided Keywords Not Found,"A Mobile Ad-Hoc Network (MANET) is a collection of wireless autonomous mobile nodes with no fixed infrastructure. Since no fixed infrastructure is required, MANET fits well in military operations, emergency disaster rescue, and mobile ad-hoc voting. There are many issues that have to be addressed while designing a technique for managing real-time database transactions in MANET: 1) energy limitations; 2) client and server mobility; 3) real-time constraints imposed on transactions; and 4) frequent disconnection and network partitioning. We have designed PETRANET1: a Power-Efficient Transaction management technique for Real-time mobile Ad-hoc NETwork databases that addresses the above specified issues. In this paper, we present a system prototype that we have developed to implement PETRANET for a military database application."
816CD157,International Conference on Data Engineering,somchai chatvichienchai + mizuho iwaihara + le gruenwald + sourav s bhowmick,2006,PRIVATE-IYE: A Framework for Privacy Preserving Data Integration, + scientific research + data mining + testing + e commerce + data integrity + data engineering + homeland security + computer science + terrorism + distributed databases + data privacy,,"Data integration has been a long standing challenge to the database and data mining communities. This need has become critical in numerous contexts, including building e-commerce market places, sharing data from scientific research, and improving homeland security. However, these important activities are hampered by legitimate and widespread concerns of data privacy. It is necessary to develop solutions that enable integration of data, especially in the domains of national priorities, while effective privacy control of the data. In this paper, we present an architecture and key research issues for building such a privacy preserving data integration system called PRIVATE-IYE."
7D3D4A47,International Conference on Data Engineering,wensheng wu + anhai doan + clement yu,2006,WebIQ: Learning from the Web to Match Deep-Web Query Interfaces,databases + question answering + NonControlled Keywords Not Found + Controlled Keywords Not Found + motion pictures + deep web + artificial intelligence,AuthorProvided Keywords Not Found,"Integrating Deep Web sources requires highly accurate semantic matches between the attributes of the source query interfaces. These matches are usually established by comparing the similarities of the attributesê labels and instances. However, attributes on query interfaces often have no or very few data instances. The pervasive lack of instances seriously reduces the accuracy of current matching techniques. To address this problem, we describe WebIQ, a solution that learns from both the Surface Web and the Deep Web to automatically discover instances for interface attributes. WebIQ extends question answering techniques commonly used in the AI community for this purpose. We describe how to incorporate WebIQ into current interface matching systems. Extensive experiments over five realworld domains show the utility ofWebIQ. In particular, the results show that acquired instances help improve matching accuracy from 89.5% F-1 to 97.5%, at only a modest runtime overhead."
7E73C7B0,International Conference on Data Engineering,raghav kaushik + venkatesh ganti + surajit chaudhuri,2006,A Primitive Operator for Similarity Joins in Data Cleaning,couplings + data cleaning + information retrieval + hamming distance + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering + data warehouses + database systems,AuthorProvided Keywords Not Found,"Data cleaning based on similarities involves identification of ""close"" tuples, where closeness is evaluated using a variety of similarity functions chosen to suit the domain and application. Current approaches for efficiently implementing such similarity joins are tightly tied to the chosen similarity function. In this paper, we propose a new primitive operator which can be used as a foundation to implement similarity joins according to a variety of popular string similarity functions, and notions of similarity which go beyond textual similarity. We then propose efficient implementations for this operator. In an experimental evaluation using real datasets, we show that the implementation of similarity joins using our operator is comparable to, and often substantially better than, previous customized implementations for particular similarity functions."
7F4776C9,International Conference on Data Engineering,kensuke ohta + takashi kobayashi + dai kobayashi + haruo yokota + ryo taguchi,2006,Treatment of Rules in Individual Metadata of Flexible Contents Management,databases + frequency + satisfiability + system performance + storage system + aging + information management + broadcasting + data management + content management,,"The properties of contents stored in a computer system are very wide while the data volume treated in the system becomes very large. It is important to treat each stored object in di erent manners to re ect its properties in the data management for the large amount of stored data. To satisfy the requirement, we propose a method for the autonomous management based on ECA rules stored in metadata of the contents. We study the feasibility of treating a large number of ECA rules corresponding to the number of stored objects. Because the cost for evaluating conditions in the rules becomes dominant to the system performance when the number of objects increases, we divide the conditions into two types, previously evaluable conditions and runtime evaluable conditions, and construct a discrimination network for the previously evaluable conditions of each event to reduce the cost for processing the rules. We implement the methods in the autonomous disk system, a high functional storage system we proposed, and evaluate the e ciency of them."
5E1416D5,International Conference on Data Engineering,yang xiao + susan d urban + suzanne w dietrich,2006,A process history capture system for analysis of data dependencies in concurrent process execution,process integration,,"This paper presents a Process History Capture System (PHCS) as a logging mechanism for distributed long running business processes executing over Delta-Enabled Grid Services (DEGS). A DEGS is a Grid Service with an enhanced interface to access incremental data changes, known as deltas, associated with service execution in the context of global processes. The PHCS captures process execution context and deltas from distributed DEGSs and constructs a global schedule for multiple executing processes, integrating local schedules that are extracted from deltas at distributed sites. The global schedule forms the basis for analyzing data dependencies among concurrently executing processes. The schedule can be used for rollback and also to identify data dependencies that affect the possible recovery of other concurrent processes. This paper presents the design of the PHCS and the use of the PHCS for process failure recovery. We also outline future directions for specification of userdefined semantic correctness."
7F4F801A,International Conference on Data Engineering,beng chin ooi + dan lin + rui zhang + christian s jensen,2006,Effective Density Queries on ContinuouslyMoving Objects,communications technology + mobile communication + computer science + indexation + empirical study + object model + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"This paper assumes a setting where a population of objects move continuously in the Euclidean plane. The position of each object, modeled as a linear function from time to points, is assumed known. In this setting, the paper studies the querying for dense regions. In particular, the paper defines a particular type of density query with desirable properties and then proceeds to propose an algorithm for the efficient computation of density queries. While the algorithm may exploit any existing index for the current and near-future positions of moving objects, the Bx-tree is used. The paper reports on an extensive empirical study, which elicits the performance properties of the algorithm."
8147B5DD,International Conference on Data Engineering,nikos mamoulis + david w cheung + man lung yiu + k h cheng,2006,Efficient Aggregation of Ranked Inputs,databases + search engines + computer science + algorithm design and analysis + NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"A top-k query combines different rankings of the same set of objects and returns the k objects with the highest combined score according to an aggregate function. We bring to light some key observations, which impose two phases that any top-k algorithm, based on sorted accesses, should go through. Based on them, we propose a new algorithm, which is designed to minimize the number of object accesses, the computational cost, and the memory requirements of top-k search. Adaptations of our algorithm for search variants (exact scores, on-line and incremental search, top-k joins, other aggregate functions, etc.) are also provided. Extensive experiments with synthetic and real data show that, compared to previous techniques, our method accesses fewer objects, while being orders of magnitude faster."
80B216FF,International Conference on Data Engineering,shubha u nabar + arnd christian konig,2006,Scalable Exploration of Physical Database Design,physical design + design optimization + constraint optimization + prototypes + production systems + probability + information retrieval + cost function + NonControlled Keywords Not Found + Controlled Keywords Not Found + scalability + database systems,AuthorProvided Keywords Not Found,"Physical database design is critical to the performance of a large-scale DBMS. The corresponding automated design tuning tools need to select the best physical design from a large set of candidate designs quickly. However, for large workloads, evaluating the cost of each query in the workload for every candidate does not scale. To overcome this, we present a novel comparison primitive that only evaluates a fraction of the workload and provides an accurate estimate of the likelihood of selecting correctly. We show how to use this primitive to construct accurate and scalable selection procedures. Furthermore, we address the issue of ensuring that the estimates are conservative, even for highly skewed cost distributions. The proposed techniques are evaluated through a prototype implementation inside a commercial physical design tool."
7E13F8DB,International Conference on Data Engineering,david w embley + yihong ding,2006,Using Data-Extraction Ontologies to Foster Automating Semantic Annotation,web pages + ontologies + data mining + engines + semantic web + system performance + computer science + automation,,"Semantic annotation adds formal metadata to web pages to link web data with ontology concepts. Automated semantic annotation is a primary way of enabling the semantic web. A main drawback of existing automated semantic annotation approaches is that they need a post-extraction mapping between extraction categories and ontology concepts. This mapping requirement usually needs human intervention, which decreases automation. Our approach uses data-extraction ontologies to avoid this problem. To automate semantic annotation, the new approach uses an ontology-based data recognizer that fosters automated semantic annotation, optimizes the system performance, provides support for ontology assembly, and is compatible with semantic web standards."
7D4368A8,International Conference on Data Engineering,sangsoo sung + dennis mcleod,2006,Ontology-Driven Semantic Matches between Database Schemas,data engineering + data mining + information management + computer science + ontologies + terminology + pattern matching + database systems + statistics,,"Schema matching has been historically difficult to automate. Most previous studies have tried to find matches by exploiting information on schema and data instances. However, schema and data instances cannot fully capture the semantic information of the databases. Therefore, some attributes can be matched to improper attributes. To address this problem, we propose a sche- ma matching framework that supports identification of the correct matches by extracting the semantics from ontologies. In ontologies, two concepts share similar semantics in their common parent. In addition, the parent can be further used to quantify a similarity bet- ween them. By combining this idea with effective cont- emporary mapping algorithms, we perform an ontolo- gy-driven semantic matching in multiple data sources. Experimental results indicate that the proposed method successfully identifies higher accurate matches than those of previous works."
7ED674DF,International Conference on Data Engineering,matthias renz + peter kunath + alexey pryakhin + hanspeter kriegel + peer kroger + johannes assfalg,2006,Threshold Similarity Queries in Large Time Series Databases,databases + law + similarity search + data structure + time series + NonControlled Keywords Not Found + Controlled Keywords Not Found + pharmaceuticals + sequences + computer science + data structures + air pollution + time series data,AuthorProvided Keywords Not Found,"Similarity search in time series data is an active area of research. In this paper, we introduce the novel concept of threshold-similarity queries in time series databases which report those time series exceeding a user-defined query threshold at similar time frames compared to the query time series. In addition, we present a new data structure to support threshold similarity queries efficiently. The performance of our solution is demonstrated by an extensive experimental evaluation."
7FBB0DB9,International Conference on Data Engineering,huahai he + ambuj k singh,2006,Closure-Tree: An Index Structure for Graph Queries,data models + edit distance + structured data + indexing + proteins + indexation + biochemistry + NonControlled Keywords Not Found + helium + Controlled Keywords Not Found + range query,AuthorProvided Keywords Not Found,"Graphs have become popular for modeling structured data. As a result, graph queries are becoming common and graph indexing has come to play an essential role in query processing. We introduce the concept of a graph closure, a generalized graph that represents a number of graphs. Our indexing technique, called Closure-tree, organizes graphs hierarchically where each node summarizes its descendants by a graph closure. Closure-tree can efficiently support both subgraph queries and similarity queries. Subgraph queries find graphs that contain a specific subgraph, whereas similarity queries find graphs that are similar to a query graph. For subgraph queries, we propose a technique called pseudo subgraph isomorphism which approximates subgraph isomorphism with high accuracy. For similarity queries, we measure graph similarity through edit distance using heuristic graph mapping methods. We implement two kinds of similarity queries: K-NN query and range query. Our experiments on chemical compounds and synthetic graphs show that for subgraph queries, Closuretree outperforms existing techniques by up to two orders of magnitude in terms of candidate answer set size and index size. For similarity queries, our experiments validate the quality and efficiency of the presented algorithms."
7DF37E50,International Conference on Data Engineering,peter mork + arnon s rosenthal + ken samuel + j p korb,2006,Integration Workbench: Integrating Schema Integration Tools,data engineering + databases + prototypes + data integrity + common knowledge + graphical user interfaces,,"A key aspect of any data integration endeavor is establishing a transformation that translates instances of one or more source schemata into instances of a target schema. This schema integration task must be tackled regardless of the integration architecture or mapping formalism. In this paper we provide a task model for schema integration. We use this breakdown to motivate a workbench for schema integration in which multiple tools share a common knowledge repository. In particular, the workbench facilitates the interoperation of research prototypes for schema matching (which automatically identify likely semantic correspondences) with commercial schema mapping tools (which help produce instance-level transformations). Currently, each of these tools provides its own ad hoc representation of schemata and mappings; combining these tools requires aligning these representations. The workbench provides a common representation so that these tools can more rapidly be combined."
59EF50CA,International Conference on Data Engineering,mehmet sayal,2006,Business impact analysis using time correlations,time series data + data analysis,,"A novel method for analyzing time-series data and extracting timecorrelations (time-dependent relationships) among multiple time-series data streams is described. The application of time-correlation detection in business impact analysis (BIA) is explained on an example. The method described in this paper is the first one that can efficiently detect and report time-dependent relationships among multiple time-series data streams. Detected time-correlation rules explain how the changes in the values of one set of time-series data streams influence the values in another set of time-series data streams. Those rules can be stored digitally and fed into various data analysis tools, such as simulation, forecasting, impact analysis, etc., for further analysis of the data. Performance experiments showed that the described method is 95% accurate, and has a linear running time with respect to the amount of input data."
7CEEC0E8,International Conference on Data Engineering,marko smiljanic + m van keulen + willem jonker,2006,Using Element Clustering to Increase the Efficiency of XML Schema Matching,xml + information retrieval + clustering algorithms + data engineering + xml schema + dictionaries + structural similarity + data type + internet,,"Schema matching attempts to discover semantic mappings between elements of two schemas. Elements are cross compared using various heuristics (e.g., name, data-type, and structure similarity). Seen from a broader perspective, the schema matching problem is a combinatorial problem with an exponential complexity. This makes the naive matching algorithms for large schemas prohibitively inefficient. In this paper we propose a clustering based technique for improving the efficiency of large scale schema matching. The technique inserts clustering as an intermediate step into existing schema matching algorithms. Clustering partitions schemas and reduces the overall matching load, and creates a possibility to trade between the efficiency and effectiveness. The technique can be used in addition to other optimization techniques. In the paper we describe the technique, validate the performance of one implementation of the technique, and open directions for future research."
7E89108A,International Conference on Data Engineering,ali inan + erkay savas + ayca azgin hintoglu + albert levi + y saygyn,2006,Privacy Preserving Clustering on Horizontally Partitioned Data,record linkage + distributed databases + data privacy + association rules + dna + customer relationship management + protocols + distributed environment + bioinformatics + couplings + data mining + association rule + data engineering + spectrum,Privacy + Data Mining + Distributed Clustering + Security,"Data mining has been a popular research area for more than a decade due to its vast spectrum of applications.. However, the popularity and wide availability of data mining tools also raised concerns about the privacy of individuals. The aim of privacy preserving data mining researchers is to develop data mining techniques that could be applied on databases without violating the privacy of individuals. Privacy preserving techniques for various data mining models have been proposed, initially for classification on centralized data then for association rules in distributed environments. In this work, we propose methods for constructing the dissimilarity matrix of objects from different sites in a privacy preserving manner which can be used for privacy preserving clustering as well as database joins, record linkage and other operations that require pair-wise comparison of individual private data objects horizontally distributed to multiple sites. We show communication and computation complexity of our protocol by conducting experiments over synthetically generated and real datasets. Each experiment is also performed for a baseline protocol which has no privacy concern to show that the overhead comes with security and privacy by comparing the baseline protocol and our protocol."
7F1E57C8,International Conference on Data Engineering,vasilis vassalos + prodromos malakasiotis + vaclav lin,2006,MiniCount: Efficient Rewriting of COUNT-Queries Using Views,databases + conjunctive queries + decision support system + data warehousing + data integrity + NonControlled Keywords Not Found + informatics + warehousing + Controlled Keywords Not Found + decision support systems,AuthorProvided Keywords Not Found,"We present MiniCount, the first efficient sound and complete algorithm for finding maximally contained rewritings of conjunctive queries with count, using conjunctive views with count and conjunctive views without aggregation. An efficient and scalable solution to this problem yields significant benefits for data warehousing and decision support systems, as well as for powerful data integration systems.We first present a naive rewriting algorithm implicit in the recent theoretical results by Cohen et al. [5] and identify three independent sources of exponential complexity in the naive algorithm, including an expensive containment check. Then we present and discuss MiniCount and prove it sound and complete. We also present an experimental study that shows Mini- Count to be orders of magnitude faster than the naive algorithm, and to be able to scale to large numbers of views"
7E2BA740,International Conference on Data Engineering,felix naumann + ulf leser + jana bauckmann,2006,Efficiently Computing Inclusion Dependencies for Schema Discovery, + data analysis + computer science + writing + relational databases + data engineering,,"Large data integration projects must often cope with undocumented data sources. Schema discovery aims at automatically finding structures in such cases. An important class of relationships between attributes that can be detected automatically are inclusion dependencies (IND), which provide an excellent basis for guessing foreign key constraints. INDs can be discovered by comparing the sets of distinct values of pairs of attributes. In this paper we present efficient algorithms for finding unary INDs. We first show that (and why) SQL is not suitable for this task. We then develop two algorithms that compute inclusion dependencies outside of the database. Both are much faster than the SQL-based methods; in fact, for larger schemas they are the only feasible solution. Our experiments show that we can compute all unary INDs in a schema of 1, 680 attributes with a total database size of 3.2 GB in approximately 2.5 hours."
7DFBF41E,International Conference on Data Engineering,bongki moon + praveen rao,2006,SketchTree: Approximate Tree Pattern Counts over Streaming Labeled Trees,moon + relative error + computer science + algorithm design and analysis + xml + NonControlled Keywords Not Found + approximation algorithms + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In recent years, there has been a rising interest in developing online approximation algorithms for data streams. Some of the key challenges are posed by the fact that streaming data can be read only once in a fixed order of arrival and only a limited amount of memory is available for storage. In this paper, we address the problem of approximately counting tree patterns over a stream of labeled trees (e.g., XML documents). We propose a new approximation algorithm called SketchTree that computes a synopsis of the stream in a single pass by processing each tree only once. Using a limited amount of memory, SketchTree provides approximate answers for both ordered and unordered tree pattern counts. Furthermore, we discuss a class of count queries that can be handled by SketchTree and their utility. We provide theoretical analyses to show that our algorithm has provably strong guarantees on the error bounds. Experiments on real datasets demonstrate that SketchTree can indeed estimate tree pattern counts within 10-15% relative error with high confidence under various situations."
7D82E0F7,International Conference on Data Engineering,kaiuwe sattler + r f schmidt + manfred hauswirth + marcel karnstedt,2006,Similarity Queries on Structured Data in Structured Overlays, + routing + automation + structured data + computer science + distributed hash table + distributed databases + self organization + robustness + distributed computing + database systems,,"Structured P2P systems based on distributed hash tables are a popular choice for building large-scaled data management systems. Generally, they only support exact match queries, but data heterogeneities often demand for more complex query types, particularly similarity queries. In this work, we suggest a vertical data organization, which allows for efficient processing of similarity queries on instance as well as on schema level, and we introduce corresponding physical similarity operators. Our novel approach is shown to be suitable in conjunction with P-Grid, as an example of robust, large-scaled and self-organizing P2P systems."
7E94271A,International Conference on Data Engineering,c r johnson + frank leymann + jerry kiernan + rakesh agrawal,2006,Taming Compliance with Sarbanes-Oxley Internal Controls Using Database Technology,automation + automatic control + process control + data security + internal control + financial management + control systems + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"The Sarbanes-Oxley Act instituted a series of corporate reforms to improve the accuracy and reliability of financial reporting. Sections 302 and 404 of the Act require SEC-reporting companies to implement internal controls over financial reporting, periodically assess the effectiveness of these internal controls, and certify the accuracy of their financial statements. We suggest that database technology can play an important role in assisting compliance with the internal control provisions of the Act. The core components of our solution include: (i) modeling of required workflows, (ii) active enforcement of control activities, (iii) auditing of actual workflows to verify compliance with internal controls, and (iv) discovery-driven OLAP to identify irregularities in financial data. We illustrate how the features of our solution fulfill Sarbanes-Oxley requirements using several real-life scenarios. In the process, we identify opportunities for new database research."
7E5C0218,International Conference on Data Engineering,sethuram balaji kodeswaran + anupam joshi,2006,Content and Context Aware Networking Using Semantic Tagging,resource description framework + routing + computer science + intelligent networks + labeling + content management + best effort,,"Today's model of networking primarily concentrates intelligence at the end hosts with the network itself offering a simple_best-effortî,_data agnosticî communication medium. However, this paradigm has proven to be insufficient to meet todays needs considering the diversity of applications and devices that are networked. To offer value added services to these end users and applications, more and more intelligence needs to be migrated away from the edges and into the network in a controlled and tractable manner. In this paper, we present our approach of utilizing semantic data tagging to provide content level information for data streams flowing through a network. A policy based management mechanism is utilized within the network fabric allowing routers to reason over the content and make intelligent decisions regarding the handling of data packets. Service differentiation, in-network content adaptation, traffic monitoring and control etc. are some of the new services that can now be offered by the network in a generic and flexible manner. By deploying our proposed architecture, a network need no longer be viewed as a simple data transport medium but rather as a policy-controlled intelligent packet/stream processor that can offer specialized handling based on application needs."
7CF938E0,International Conference on Data Engineering,bertram ludaescher + manish parashar + scott klasky,2006,The Center for Plasma Edge Simulation Workflow Requirements, + predictive models + solid modeling + physics + magnetohydrodynamics + geometry + kinetics + kinetic theory,,"The Center for Plasma Edge Simulation (CPES) is a recently funded prototype Fusion Simulation Project, which is part of the DOE SciDAC program. Our center is developing a novel integrated predictive plasma edge simulation framework, which is applicable to existing magnetic fusion facilities (D3D, NSTX, CMOD) and next generation burning plasma experiments, e.g. ITER. The success of this project will be in developing and understanding new models for the plasma edge in a kinetic regime with complex geometry. Because of the multi-scale nature of the problem, we will study the neoclassical physics time scale kinetically, and the fast and larger scale MHD modes via a fluid code. Our approach is to couple these codes via a scientific workflow system, Kepler-HPC. Kepler-HPC will enhance Kepler with capabilities such as code coupling and data redistribution, high volume data transfers and interactive (and autonomic) monitoring, steering and debugging, which will be necessary for scientific progress in this project."
7FC9C091,International Conference on Data Engineering,stan zdonik + nesime tatbul,2006,Dealing with Overload in Distributed Stream Processing Systems,computer science + quality management + quality of service + degradation + dynamic system + stream processing + resource management,,"Overload management has been an important problem for large-scale dynamic systems. In this paper, we study this problem in the context of our Borealis distributed stream processing system. We show that server nodes must coordinate in their load shedding decisions to achieve global control on output quality. We describe a distributed load shedding approach which provides this coordination by upstream metadata aggregation and propagation. Metadata enables an upstream node to make fast local load shedding decisions which will influence its descendant nodes in the best possible way."
7DB6C019,International Conference on Data Engineering,nikos ntarmos + gerhard weikum + peter triantafillou,2006,Counting at Large: Efficient Cardinality Estimation in Internet-Scale Data Networks,information system + databases + bandwidth + information retrieval + robustness + load balance + NonControlled Keywords Not Found + information systems + Controlled Keywords Not Found + internet + scalability,AuthorProvided Keywords Not Found,"Counting in general, and estimating the cardinality of (multi-) sets in particular, is highly desirable for a large variety of applications, representing a foundational block for the efficient deployment and access of emerging internetscale information systems. Examples of such applications range from optimizing query access plans in internet-scale databases, to evaluating the significance (rank/score) of various data items in information retrieval applications. The key constraints that any acceptable solution must satisfy are: (i) efficiency: the number of nodes that need be contacted for counting purposes must be small in order to enjoy small latency and bandwidth requirements; (ii) scalability, seemingly contradicting the efficiency goal: arbitrarily large numbers of nodes nay need to add elements to a (multi-) set, which dictates the need for a highly distributed solution, avoiding server-based scalability, bottleneck, and availability problems; (iii) access and storage load balancing: counting and related overhead chores should be distributed fairly to the nodes of the network; (iv) accuracy: tunable, robust (in the presence of dynamics and failures) and highly accurate cardinality estimation; (v) simplicity and ease of integration: special, solution-specific indexing structures should be avoided. In this paper, first we contribute a highly-distributed, scalable, efficient, and accurate (multi-) set cardinality estimator. Subsequently, we show how to use our solution to build and maintain histograms, which have been a basic building block for query optimization for centralized databases, facilitating their porting into the realm of internet-scale data networks."
7F4C5B61,International Conference on Data Engineering,malik magdonismail + brandeis hill + sibel adali,2006,The Impact of Ranker Quality on Rank Aggregation Algorithms: Information vs. Robustness,information retrieval + robustness + web pages + markov chain + databases + ground truth + data engineering,,"The rank aggregation problem has been studied extensively in recent years with a focus on how to combine several different rankers to obtain a consensus aggregate ranker. We study the rank aggregation problem from a different perspective: how the individual input rankers impact the performance of the aggregate ranker. We develop a general statistical framework based on a model of how the individual rankers depend on the ground truth ranker. Within this framework, one can study the performance of different aggregation methods. The individual rankers, which are the inputs to the rank aggregation algorithm, are statistical perturbations of the ground truth ranker. With rigorous experimental evaluation, we study how noise level and the misinformation of the rankers affect the performance of the aggregate ranker. We introduce and study a novel Kendalltau rank aggregator and a simple aggregator called PrOpt, which we compare to some other well known rank aggregation algorithms such as average, median and Markov chain aggregators. Our results show that the relative performance of aggregators varies considerably depending on how the input rankers relate to the ground truth."
7DC80354,International Conference on Data Engineering,d salvi + michele melchiori + v de antonellis + devis bianchini,2006,Semantic-Enriched Service Discovery, + owl + information retrieval + distributed environment + service discovery + adaptive systems + context dependent + microstrip + knowledge representation + logic + information systems + ontologies,,"Automated techniques and tools are required to effectively locate services that fulfil a given user request. To this purpose, the use of semantic descriptions of services has been widely motivated and recommended for automated service discovery under highly dynamic and contextdependent requirements in distributed environments. Our aim in this work is to propose a semantic-enriched framework to describe services and an ontology-based hybrid approach where such framework is exploited combining together different kinds of comparison strategies to provide a flexible and efficient matchmaking between service descriptions. For service discovery two matchmaking strategies are proposed: a deductive strategy based on Description Logics, with a reasoning procedure exploiting ontology knowledge to assess the type of match between services; a similarity-based strategy, exploiting retrieval metrics to measure the degree of match between services."
7D3F9566,International Conference on Data Engineering,andrea wenning + thomas seidl + ira assent,2006,Approximation Techniques for Indexing the Earth Mover&#146;s Distance in Multimedia Databases,earth mover s distance + application software + indexing + interactive multimedia + data mining + indexation + NonControlled Keywords Not Found + lower bound + Controlled Keywords Not Found + satisfiability + earth + computer vision + biomedical imaging,AuthorProvided Keywords Not Found,"Todays abundance of storage coupled with digital technologies in virtually any scientific or commercial application such as medical and biological imaging or music archives deal with tremendous quantities of images, videos or audio files stored in large multimedia databases. For content-based data mining and retrieval purposes suitable similarity models are crucial. The Earth Moverês Distance was introduced in Computer Vision to better approach human perceptual similarities. Its computation, however, is too complex for usage in interactive multimedia database scenarios. In order to enable efficient query processing in large databases, we propose an index-supported multistep algorithm. We therefore develop new lower bounding approximation techniques for the Earth Moverês Distance which satisfy high quality criteria including completeness (no false drops), index-suitability and fast computation. We demonstrate the efficiency of our approach in extensive experiments on large image databases"
5894368B,International Conference on Data Engineering,manuel alvarez + alberto pan + jose losada + justo hidalgo,2006,Optimization of automatic navigation to hidden web pages by ranking-based browser preloading,hidden web,,"Web applications have become an invaluable source of information for many different vertical solutions, but their complex navigation and semistructured format make their information difficult to retrieve. Web Automation and Extraction systems are able to navigate through web links and to fill web forms automatically in order to get information not directly accessible by a URL. In these systems, the main optimization parameter is the time required to navigate through the intermediate pages which lead to the desired final pages. This paper proposes a series of techniques and algorithms that improves this parameter by basically storing historical information from previous queries, and using it to make the browser manager preload an adequate subset of the whole navigational sequence on a specific browser, before the following query is executed. These techniques also handle which sequences are the most common, thus being the ones which are preloaded more often."
7F2AF21B,International Conference on Data Engineering,marko smiljanic + m van keulen + willem jonker,2006,Effectiveness Bounds for Non-Exhaustive Schema Matching Systems,scalability + data engineering + system testing + xml + upper bound + availability + information security + xml schema,,"Semantic validation of the effectiveness of a schema matching system is traditionally performed by comparing system-generated mappings with those of human evaluators. The human effort required for validation quickly becomes huge in large scale environments. The performance of a matching system, however, is not solely determined by the quality of the mappings, but also by the efficiency with which it can produce them. Improving efficiency quickly leads to a trade-off between efficiency and effectiveness. Establishing or obtaining a large test collection for measuring this trade-off is often a severe obstacle. In this paper, we present a technique for determining lower and upper bounds for effectiveness measures for a certain class of schema matching system improvements in order to lower the required validation effort. Effectiveness bounds for a matching system improvement are solely derived from a comparison of answer sets of the improved and original matching system. The technique was developed in the context of improving efficiency in XML schema matching, but we believe it to be more generically applicable in other retrieval systems facing scalability problems."
806EF86B,International Conference on Data Engineering,felix naumann + melanie weis,2006,Detecting Duplicates in Complex XML Data,data models + information management + clustering algorithms + xml + NonControlled Keywords Not Found + Controlled Keywords Not Found + motion pictures + data warehouses + customer relationship management,AuthorProvided Keywords Not Found,"Recent work both in the relational and the XML world have shown that the efficacy and efficiency of duplicate detection is enhanced by regarding relationships between entities. However, most approaches for XML data rely on 1:n parent/child relationships, and do not apply to XML data that represents m:n relationships. We present a novel comparison strategy, which performs duplicate detection effectively for all kinds of parent/child relationships, given dependencies between different XML elements. Due to cyclic dependencies, it is possible that a pairwise classification is performed more than once, which compromises efficiency. We propose an order that reduces the number of such reclassifications and apply it to two algorithms. The first algorithm performs reclassifications, and efficiency is increased by using the order reducing the number of reclassifications. The second algorithm does not perform a comparison more than once, and the order is used to miss few reclassifications and hence few potential duplicates."
7E1B1F88,International Conference on Data Engineering,magesh jayapandian + h v jagadish,2006,Automating the Design and Construction of Query Forms,database languages + algorithm design and analysis + clustering algorithms + engines + writing + data structures + information analysis + NonControlled Keywords Not Found + Controlled Keywords Not Found + data engineering,AuthorProvided Keywords Not Found,"One of the simplest ways to query a database is through a form, where a user can fill in relevant information and obtain desired results by submitting the form. Designing good static forms is a non-trivial manual task, and the designer needs a sound understanding of both the data organization and the querying needs. Furthermore, form design has two conflicting goals: forms should be simple to understand, and at the same time must provide the broadest possible querying capability to the user. In this paper, we present a framework for generating forms in an automatic and principled way, given the database schema and a sample query workload. We design a tunable clustering algorithm for establishing form structure based on multiple ""similar""queries, which includes a mechanism for extending form structure to support other ""similar"" queries the system may see in the future. The algorithm is adaptive and can incrementally adjust the form structure to reflect the addition or removal of queries in the workload. We have implemented our form generation system on a real database and evaluated it on a comprehensive set of query loads and database schemas. We observe that our system can significantly reduce the numbers of forms needed for various query loads by exploiting similarities across queries, even after placing a strict bound on form complexity."
7E079B50,International Conference on Data Engineering,ke deng + heng tao shen + kai xu + xuemin lin,2006,Surface k-NN Query Processing,databases + k nearest neighbor + computer networks + animal movement + cost function + spatial resolution + upper bound + NonControlled Keywords Not Found + Controlled Keywords Not Found + shortest path,AuthorProvided Keywords Not Found,"A k-NN query finds the k nearest-neighbors of a given point from a point database. When it is sufficient to measure object distance using the Euclidian distance, the key to efficient k-NN query processing is to fetch and check the distances of a minimum number of points from the database. For many applications, such as vehicle movement along road networks or rover and animal movement along terrain surfaces, the distance is only meaningful when it is along a valid movement path. For this type of k-NN queries, the focus of efficient query processing is to minimize the cost of computing distances using the environment data (such as the road network data and the terrain data), which can be several orders of magnitude larger than that of the point data. Efficient processing of k-NN queries based on the Euclidian distance or the road network distance has been investigated extensively in the past. In this paper, we investigate the problem of surface k-NN query processing, where the distance is calculated from the shortest path along a terrain surface. This problem is very challenging, as the terrain data can be very large and the computational cost of finding shortest paths is very high. We propose an efficient solution based on multiresolution terrain models. Our approach eliminates the need of costly process of finding shortest paths by ranking objects using estimated lower and upper bounds of distance on multiresolution terrain models."
NE2509,International Conference on Data Engineering,Wei-Shinn Ku+R. Zimmermann+Chi-Ngai Wan+Haojun Wang,2006,MAPLE: A Mobile Scalable P2P Nearest Neighbor Query System for Location-based Services,NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In this demonstration we present MAPLE, a scalable peer-to-peer nearest neighbor (NN) query system for mobile environments. MAPLE is designed for the efficient sharing of query results cached in the local storage of mobile peers. The MAPLE system is innovative in its ability to either fully or partially compute location-dependent nearest neighbor objects on each host. The demonstration illustrates how cooperative data sharing and distributed processing among mobile peers results in a considerable reduction of the load on remote spatial databases."
7F6DCAB0,International Conference on Data Engineering,mitch cherniack + stan zdonik + anurag s maskey + esther ryvkina,2006,Revision Processing in a Stream Processing Engine: A High-Level Design,data models + stream processing + engines + process design + writing + NonControlled Keywords Not Found + Controlled Keywords Not Found + financial services,AuthorProvided Keywords Not Found,"Data stream processing systems have become ubiquitous in academic [1, 2, 5, 6] and commercial [11] sectors, with application areas that include financial services, network traffic analysis, battlefield monitoring and traffic control [3]. The append-only model of streams implies that input data is immutable and therefore always correct. But in practice, streaming data sources often contend with noise (e.g., embedded sensors) or data entry errors (e.g., financial data feeds) resulting in erroneous inputs and therefore, erroneous query results. Many data stream sources (e.g., commercial ticker feeds) issue ""revision tuples"" (revisions) that amend previously issued tuples (e.g. erroneous share prices). Ideally, any stream processing engine should process revision inputs by generating revision outputs that correct previous query results. We know of no stream processing system that presently has this capability."
NE2532,International Conference on Data Engineering,G. Koutrika+A. Simitsis+Y. Ioannidis,2006,Pr _ cis: The Essence of a Query Answer,NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"Wide spread use of database systems in modern society has brought the need to provide inexperienced users with the ability to easily search a database with no specific knowledge of a query language. Several recent research efforts have focused on supporting keyword-based searches over relational databases. This paper presents an alternative proposal and introduces the idea of pr_cis queries. These are free-form queries whose answer (a pr_cis) is a synthesis of results, containing not only information directly related to the query selections but also information implicitly related to them in various ways. Our approach to pr_cis queries includes two additional novelties: (a) queries do not generate individual relations but entire multi-relation databases; and (b) query results are personalized to user-specific and/or domain requirements. We develop a framework and system architecture for supporting such queries in the context of a relational database system and describe algorithms that implement the required functionality. Finally, we present a set of experimental results that evaluate the proposed algorithms and show the potential of this work."
7E1912C3,International Conference on Data Engineering,ahmed m ayad + jeffrey f naughton + stephen j wright + u n srivastava,2006,Approximating StreamingWindow Joins Under CPU Limitations,steady state + concrete + computer science + production + control systems + data structures + NonControlled Keywords Not Found + Controlled Keywords Not Found + computational modeling + data engineering,AuthorProvided Keywords Not Found,"Data streaming systems face the possibility of having to shed load in the case of CPU or memory resource limitations. We study the CPU limited scenario in detail. First, we propose a new model for the CPU cost. Then we formally state the problem of shedding load for the goal of obtaining the maximum possible subset of the complete answer, and propose an online strategy for semantic load shedding. Moving on to random load shedding, we discuss random load shedding strategies that decouple the window maintenance and tuple production operations of the symmetric hash join, and prove that one of them Ê Probe-No-Insert Ê always dominates the previously proposed coin flipping strategy."
816767C9,International Conference on Data Engineering,yifeng zheng + susan b davidson + yi chen,2006,An Efficient XPath Query Processor for XML Streams,pattern matching + polynomials + xml + data structure + data structures + NonControlled Keywords Not Found + Controlled Keywords Not Found + streaming algorithm + automata,AuthorProvided Keywords Not Found,"Streaming XPath evaluation algorithms must record a potentially exponential number of pattern matches when both predicates and descendant axes are present in queries, and the XML data is recursive. In this paper, we use a compact data structure to encode these pattern matches rather than storing them explicitly. We then propose a polynomial time streaming algorithm to evaluate XPath queries by probing the data structure in a lazy fashion. Extensive experiments show that our approach not only has a good theoretical complexity bound but is also efficient in practice."
7FFC460B,International Conference on Data Engineering,bei yu + mihai lupu,2006,HiWaRPP &#8213; Hierarchical Wavelet-based Retrieval on Peer-to-Peer Network,prototypes + information retrieval + testing + overlay network + discrete wavelet transform + intrusion detection + NonControlled Keywords Not Found + Controlled Keywords Not Found + feature vector + broadcasting + routing + wavelet analysis + synthetic data + multiresolution analysis + connected graph,AuthorProvided Keywords Not Found,"This paper introduces the use of wavelets for information retrieval in a peer-to-peer environment. In order to achieve our purposes, we use a new combination between broadcasting and a hierarchical overlay. Compared to previous approaches, we do not store complete information about the children of a super-peer, nor do we broadcast the queries blindly. We approximate the feature vectors using the multiresolution analysis and the discrete wavelet transform. Each peer is represented by a high-dimensional feature vector and the height of the hierarchy is logarithmic in the dimensionality of this feature vector. Leaf nodes represent real peers, while internal nodes are virtual peers used for routing. Our retrieval method has been tested with both real and synthetic data and shown to be efficient in retrieving relevant information, resulting in good precision and recall on four standard test collections."
7F4A635E,International Conference on Data Engineering,kristen lefevre + david j dewitt + raghu ramakrishnan,2006,Mondrian Multidimensional K-Anonymity,databases + greedy algorithms + data security + demography + privacy + NonControlled Keywords Not Found + approximation algorithms + Controlled Keywords Not Found + greedy algorithm + multidimensional systems + publishing,AuthorProvided Keywords Not Found,"K-Anonymity has been proposed as a mechanism for protecting privacy in microdata publishing, and numerous recoding ""models"" have been considered for achieving __anonymity. This paper proposes a new multidimensional model, which provides an additional degree of flexibility not seen in previous (single-dimensional) approaches. Often this flexibility leads to higher-quality anonymizations, as measured both by general-purpose metrics and more specific notions of query answerability. Optimal multidimensional anonymization is NP-hard (like previous optimal __-anonymity problems). However, we introduce a simple greedy approximation algorithm, and experimental results show that this greedy algorithm frequently leads to more desirable anonymizations than exhaustive optimal algorithms for two single-dimensional models."
NE2564,International Conference on Data Engineering,R. Agrawal+D. Asonov+M. Kantarcioglu+Yaping Li,2006,Sovereign Joins,NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"We present a secure network service for sovereign information sharing whose only trusted component is an off-theshelf secure coprocessor. The participating data providers send encrypted relations to the service that sends the encrypted results to the recipients. The technical challenge in implementing such a service arises from the limited capability of the secure coprocessors: they have small memory, no attached disk, and no facility for communicating directly with other machines in the network. The internal state of an ongoing computation within the secure coprocessor cannot be seen from outside, but its interactions with the server can be exploited by an adversary. We formulate the problem of computing join in this setting where the goal is to prevent information leakage through patterns in I/O while maximizing performance. We specify criteria for proving the security of a join algorithm and provide provably safe algorithms. These algorithms can be used to compute general joins involving arbitrary predicates and multiple sovereign databases. We thus enable a new class of applications requiring query processing across sovereign entities such that nothing apart from the result is revealed to the recipients."
7E7E1DBC,International Conference on Data Engineering,murali mani + elke a rundensteiner + maged elsayed,2006,Incremental Maintenance of Materialized XQuery Views,application software + databases + search engines + computer science + xml + materialized views + tcpip + data structures + NonControlled Keywords Not Found + Controlled Keywords Not Found + internet,AuthorProvided Keywords Not Found,"Materializing the contents of views has important applications including providing fast access to derived database repositories, optimizing query processing based on cached results, and increasing availability. Maintaining the consistency between materialized views and their base data in the presence of source updates is important to ensure that the materialized views are up-to-date. The straightforward solution for this problem is to recompute the view from scratch over the updated sources."
7EEF493D,International Conference on Data Engineering,dou shen + qiang yang + zheng chen + hui zhao + jiantao sun,2006,Text Classification Improved through Automatically Extracted Sequences,classification algorithms + computer science + natural language processing + probability distribution + language model + NonControlled Keywords Not Found + Controlled Keywords Not Found + knowledge management + uncertainty + machine learning + sun,AuthorProvided Keywords Not Found,"We propose to use the n-multigram model to help the automatic text classification task. This model could automatically discover the latent semantic sequences contained in the document set of each category. Based on the n-multigram model and the n-gram language model, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram model can achieve the similar classification performance compared with the one based on n-gram model. However, the model size of our algorithm is only 4.21% of the latter one. Another proposed algorithm based on the combination of nmultigram model and n-gram model improves the micro- F1 and macro-F1 values by 3.5% and 4.5% respectively which support the validity of our approach."
7F40901C,International Conference on Data Engineering,jiawei han + zheng shao + dong xin + hongyan liu,2006,Top-Down Mining of Interesting Patterns from Very High Dimensional Data,top down + application software + algorithm design and analysis + data mining + NonControlled Keywords Not Found + engineering management + Controlled Keywords Not Found + data engineering + transaction data + computer science + high dimensional data + bioinformatics + search space + statistical analysis,AuthorProvided Keywords Not Found,"Many real world applications deal with transactional data, characterized by a huge number of transactions (tuples) with a small number of dimensions (attributes). However, there are some other applications that involve rather high dimensional data with a small number of tuples. Examples of such applications include bioinformatics, survey-based statistical analysis, text processing, and so on. High dimensional data pose great challenges to most existing data mining algorithms. Although there are numerous algorithms dealing with transactional data sets, there are few algorithms oriented to very high dimensional data sets with a relatively small number of tuples."
NE2550,International Conference on Data Engineering,Shuigeng Zhou+Zheng Zhang+Weining Qian+Aoying Zhou,2006,SIPPER: Selecting Informative Peers in Structured P2P Environment for Content-Based Retrieval,NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,"In this demonstration, we present a prototype system called SIPPER, which is the abbreviation for Selecting Informative Peers in Structured P2P Environment for Content-based Retrieval. SIPPER distinguishes itself from the existing P2P-IR systems by the following two features: First, to improve retrieval efficiency, SIPPER employs a novel peer selection method to direct the query to a small fraction of relevant peers in the network for searching globally relevant documents. Second, to reduce the bandwidth cost of meta data publishing, SIPPER uses a new publishing mechanism, the term-node publishing mechanism, which is different from the traditional term-document model [2]."
80589A4A,International Conference on Data Engineering,todd eavis + david c green + andrew rauchaplin + frank dehne + elankayer sithirasenan + yenkuang chen,2006,cgmOLAP: Efficient Parallel Generation and Querying of Terabyte Size ROLAP Data Cubes,databases + project management + indexing + bandwidth + indexation + NonControlled Keywords Not Found + engineering management + Controlled Keywords Not Found + parallel processing + web server + engines + data cube + concurrent computing,AuthorProvided Keywords Not Found,"We present the cgmOLAP server, the first fully functional parallel OLAP system able to build data cubes at a rate of more than 1 Terabyte per hour. cgmOLAP incorporates a variety of novel approaches for the parallel computation of full cubes, partial cubes, and iceberg cubes as well as new parallel cube indexing schemes. The cgmOLAP system consists of an application interface, a parallel query engine, a parallel cube materialization engine, meta data and cost model repositories, and shared server components that provide uniform management of I/O, memory, communications, and disk resources."
7FB20D94,International Conference on Data Engineering,francesco bonchi + salvatore orlando + fosca giannotti + claudio lucchese + raffaele perego + roberto trasarti,2006,ConQueSt: a Constraint-based Querying System for Exploratory Pattern Discovery,database languages + navigation + graphical user interfaces + computer science + engines + data mining + NonControlled Keywords Not Found + search space + data visualization + Controlled Keywords Not Found + query language,AuthorProvided Keywords Not Found,"ConQueSt is a constraint-based querying system devised with the aim of supporting the intrinsically exploratory nature of pattern discovery. It provides users with an expressive constraint-based query language which allows the discovery process to be effectively driven toward potentially interesting patterns. Constraints are also exploited to reduce the cost of pattern mining. The system is built around an efficient constraint-based mining engine which entails several data and search space reduction techniques, and allows new user-defined constraints to be easily added."
809D9D02,International Conference on Data Engineering,yannis velegrakis + divesh srivastava + yannis kotidis,2006,Updates Through Views: A New Hope,side effect + NonControlled Keywords Not Found + Controlled Keywords Not Found + database systems,AuthorProvided Keywords Not Found,"Database views are extensively used to represent unmaterialized tables. Applications rarely distinguish between a materialized base table and a virtual view, thus, they may issue update requests on the views. Since views are virtual, update requests on them need to be translated to updates on the base tables. Existing literature has shown the difficulty of translating view updates in a side-effect free manner. To address this problem, we propose a novel approach for separating the data instance into a logical and a physical level. This separation allows us to achieve side-effect free translations of any kind of update on the view. Furthermore, deletes on a view can be translated without affecting the base tables. We describe the implementation of the framework and present our experimental results"
80B3ECB3,International Conference on Data Engineering,mauricio hernandez + h ho + l popa + paolo papotti + ariel fuxman + r j miller + toshio fukuda,2007,Creating Nested Mappings with Clio,application software + data representation + data exchange + data integrity + Clio + electronic data interchange + transformation queries + xml + engines + data structures + schema mappings + nested mappings + data integration,AuthorProvided Keywords Not Found,"Schema mappings play a central role in many data integration and data exchange scenarios. In those applications, users need to quickly and correctly specify how data represented in one format is converted into a different format. Clio (L. Popa et al., 2002) is a joint research project between IBM and the University of Toronto studying the creation, maintenance, and use of schema mappings. There have always been two goals in our work in Clio: 1) the automatic creation of logical assertions that capture the way one or more source schemas are mapped into a target schema, and 2) the generation of transformation queries or programs that transform a source data instance into a target data instance."
7E5B5342,International Conference on Data Engineering,lina peng + k s candan,2007,Data-quality Guided Load Shedding for Expensive In-Network Data Processing,load shedding + wireless sensor networks + fuses + data processing + wireless sensor network + in-network data processing + quality assessment models + sensor fusion + fusion operators + data engineering + data quality + quality management + computer science + redundancy + throughput + partial order,AuthorProvided Keywords Not Found,"In situ wireless sensor networks, not only have to route sensed data from sources to destinations, but also have to filter and fuse observations to eliminate potentially irrelevant data. If data arrive faster to such fusion nodes than the speed with which they can consume the inputs, this will result in an overflow of input buffers. In this paper, we develop load shedding mechanisms which take into consideration both data quality and expensive nature of fusion operators. In particular, we present quality assessment models for objects and fusion operators and we highlight that such quality assessments may impose partial orders on objects."
7F91C553,International Conference on Data Engineering,minsoo kim + minjae lee + jaegil lee + kihoon lee + kyuyoung whang,2007,Odysseus: a High-Performance ORDBMS Tightly-Coupled with Spatial Database Features,information technology + indexing + object-oriented databases + information retrieval + visual databases + geographic information systems + management information systems + relational databases + data type + Odysseus + object-relational database management system + spatial database + indexes + computer science + engines + computer architecture + concurrency control + geographical information system + geographic information system + tight coupling,AuthorProvided Keywords Not Found,"We have earlier proposed the tight-coupling architecture for adding new data types into the DBMS engine. In this paper, we introduce the Odysseus ORDBMS and present its tightly-coupled spatial database features. We demonstrate a geographical information system (GIS) implemented using Odysseus."
80201697,International Conference on Data Engineering,wenyuan guo + kianlee tan + wei wu,2007,Distributed Processing of Moving K-Nearest-Neighbor Query on Moving Objects,complex data + distributed processing + visual databases + cellular network + continuous k-nearest-neighbor query + global positioning system + distributed computing + distributed strategy + wireless communication + query processing + communications technology + k nearest neighbor + moving K-nearest-neighbor query + collaboration + data structures + moving objects + real time,AuthorProvided Keywords Not Found,"A moving k-nearest-neighbor (MKNN) query is a continuous k-nearest-neighbor (KNN) query issued by a moving object. As both the query owner and other mobile objects are moving, the influenced area (i.e., cells in the cellular networks), and query result of a MKNN query change with time. Existing processing techniques for MKNN queries are all centralized approaches which rely on the location update messages from moving objects. However, these approaches typically employ complex data structures and algorithms. Moreover, the server may not be able to cope with a high location report rate which is necessary to ensure accurate and correct answers. In this paper, we propose a distributed strategy to process MKNN queries in real-time. In our scheme, called disMKNN, the server and moving objects collaborate to maintain the KNN of a MKNN query. While the server keeps track of a MKNN query's influenced cells, moving objects within the cells monitor their own relationships (i.e., whether they are part of the KNN answers) to the query. Results of an extensive performance study show the effectiveness of disMKNN."
811B3FAA,International Conference on Data Engineering,r chand + minos garofalakis + pascal felber,2007,Tree-Pattern Similarity Estimation for Scalable Content-based Routing,data exchange + XML documents + xml document + content-based publish-subscribe systems + tree-pattern subscription Boolean combinations + routing + XML data streams + XML data scalable distribution + satisfiability + xml + semantic clusters + specification languages + xml documents + content-routing protocols + data publishing + protocols + filtering + publishing + content-based systems + middleware + XPath + sampling methods + network infrastructure + hash-based sampling + routing protocol + encoding + tree-pattern specification language + XML content + scalable content-based routing + routing protocols + XML + tree-pattern similarity estimation + file organisation,AuthorProvided Keywords Not Found,"With the advent of XML as the de facto language for data publishing and exchange, scalable distribution of XML data to large, dynamic populations of consumers remains an important challenge. Content-based publish/subscribe systems offer a convenient design paradigm, as most of the complexity related to addressing and routing is encapsulated within the network infrastructure. To indicate the type of content that they are interested in, data consumers typically specify their subscriptions using a tree-pattern specification language (an important subset of XPath), while producers publish XML content without prior knowledge of any potential recipients. Discovering semantic communities of consumers with similar interests is an important requirement for scalable content-based systems: such ""semantic clusters"" of consumers play a critical role in the design of effective content-routing protocols and architectures. The fundamental problem underlying the discovery of such semantic communities lay in effectively evaluating the similarity of different tree-pattern subscriptions based on the observed document stream. In this paper, we propose a general framework and algorithmic tools for estimating different tree-pattern similarity metrics over continuous streams of XML documents. In a nutshell, our approach relies on continuously maintaining a novel, concise synopsis structure over the observed document stream that allows us to accurately estimate the fraction of documents satisfying various Boolean combinations of different tree-pattern subscriptions. To effectively capture different branching and correlation patterns within a limited amount of space, our techniques use ideas from hash-based sampling in a novel manner that exploits the hierarchical structure of our document synopsis. Experimental results with various XML data streams verify the effectiveness of our approach."
7F3DD913,International Conference on Data Engineering,christof bornhovd + mariano cilia + kai sachs + pablo e guerrero + alejandro buchmann,2007,Pushing Business Data Processing Towards the Periphery,radiofrequency identification + sensing technologies + information management + data processing + RFID + scalability + business process + engines + system architecture + supply chain management + distributed databases + supply chain management applications + rfid + business data processing + electronic commerce,AuthorProvided Keywords Not Found,The usage of RFID and sensing technologies in supply chain management applications requires the automatic conversion of large amounts of raw data into manageable business process information. This has led to many performance and scalability issues in existing RFID infrastructures. We present an approach to alleviate these shortcomings based on a flexible system architecture that partially migrates business data processing towards the periphery.
7813E430,International Conference on Data Engineering,neeraj gupta + c gokhale + pankaj kumar + r ng + b a prakash + laks v s lakshmanan,2007,Complex Group-By Queries for XML,chemistry + database languages + complex group-by queries + data exchange + XML query optimization + XQuery + query languages + data integrity + relational databases + query processing + query optimization + xml + XML + query language + XML query languages,AuthorProvided Keywords Not Found,"The popularity of XML as a data exchange standard has led to the emergence of powerful XML query languages like XQuery and studies on XML query optimization. Of late, there is considerable interest in analytical processing of XML data. As pointed out by Borkar and Carey, even for data integration, there is a compelling need for performing various group-by style aggregate operations. A core operator needed for analytics is the group-by operator, which is widely used in relational as well as OLAP database applications. XQuery requires group-by operations to be simulated using nesting."
800AD48D,International Conference on Data Engineering,yannis velegrakis + divesh srivastava,2007,MMS: Using Queries As Data Values for Metadata Management,data models + query processing + meta data + metadata storage + data values + data security + metadata management + information security + relational queries + relational model + relational databases + ontologies,AuthorProvided Keywords Not Found,"We demonstrate MMS, a system for storing and managing a variety of metadata in a simple, elegant and uniform way. The system is based on two observations. First, that the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their association. Second, that relational queries with a join mechanism augmented to permit matching of query result relations, instead of only atomic values, is an elegant way to uniformly query across data and metadata."
816DDE24,International Conference on Data Engineering,jiawei han + martin ester + wen jin + zengjian hu,2007,The Multi-Relational Skyline Operator,data retrieval + multirelational databases + information retrieval + decision support + relational database + relational databases + skyline computation + query processing + recommender system + database operator + computer science + multirelational skyline operator + skyline query,AuthorProvided Keywords Not Found,"Most of the existing work on skyline query has been extensively used in decision support, recommending systems etc, and mainly focuses on the efficiency issue for a single table. However the data retrieved by users for the targeting skylines may often be stored in multiple tables, thus require to perform join operations among tables. As a result, the cost on computing skylines on the joined table will be increased dramatically due to its potentially increasing cardinality and dimensionality. In this paper, we systematically study the skyline operator on multi-relational databases, and propose solutions aiming to seamlessly integrating state-of-the-art join methods into skyline computation. Our experiments not only demonstrate that the proposed methods are efficient, but also show the promising applicability of extending skyline operator to other typical database operators such as join and aggregates."
7E6946AB,International Conference on Data Engineering,alkis simitsis + panos vassiliadis + n e frantzell + spiros skiadopoulos + neoklis polyzotis,2007,Supporting Streaming Updates in an Active Data Warehouse,active data warehouse + online warehouse refreshment + data warehouse + production systems + data mining + data warehousing + warehousing + scalability + surrogate key assignment + memory management + storage management + Mesh Join algorithm + streaming update + throughput + data warehouses + duplicate detection,AuthorProvided Keywords Not Found,"Active data warehousing has emerged as an alternative to conventional warehousing practices in order to meet the high demand of applications for up-to-date information. In a nutshell, an active warehouse is refreshed on-line and thus achieves a higher consistency between the stored information and the latest data updates. The need for on-line warehouse refreshment introduces several challenges in the implementation of data warehouse transformations, with respect to their execution time and their overhead to the warehouse processes. In this paper, we focus on a frequently encountered operation in this context, namely, the join of a fast stream S of source updates with a disk-based relation R, under the constraint of limited memory. This operation lies at the core of several common transformations, such as, surrogate key assignment, duplicate detection or identification of newly inserted tuples. We propose a specialized join algorithm, termed mesh join (MeshJoin), that compensates for the difference in the access cost of the two join inputs by (a) relying entirely on fast sequential scans of R, and (b) sharing the I/O cost of accessing R across multiple tuples of S. We detail the Mesh Join algorithm and develop a systematic cost model that enables the tuning of Mesh Join for two objectives: maximizing throughput under a specific memory budget or minimizing memory consumption for a specific throughput. We present an experimental study that validates the performance of Mesh Join on synthetic and real-life data. Our results verify the scalability of Mesh-Join to fast streams and large relations, and demonstrate its numerous advantages over existing join algorithms."
7EB2718B,International Conference on Data Engineering,leonard mcmillan + feng pan + wei wang,2007,Accelerating Profile Queries in Elevation Maps,spatial data + spatial data representation + hydrology + maximal likelihood + spatial index + visual databases + geographic information systems + flexible query + profile query problem + probabilistic model + query processing + acceleration + terminology + inverse problems + geographical information system + geographic information system + information systems + search space + elevation map,AuthorProvided Keywords Not Found,"Elevation maps are a widely used spatial data representation in geographical information systems (GIS). Paths on elevation maps can be characterized by profiles, which describe relative elevation as a function of distance. In this research, we address the inverse of this mapping - given a profile, how to efficiently find paths that could have generated it. This is called the profile query problem. Profiles have a wide variety of uses that include registering tracking information, or even other maps, to a given map. We describe a probabilistic model to characterize the maximal likelihood that a point lying on a path matches the query profile. Propagation of such probabilities to neighboring points can effectively prune the search space. This model enables us to efficiently answer queries of arbitrary profiles with user-specified error tolerances. When compared to existing spatial index methods, our approach supports more flexible queries with orders of magnitude speedup."
7FDB244F,International Conference on Data Engineering,huiming qu + alexandros labrinidis,2007,Preference-Aware Query and Update Scheduling in Web-databases,profitability + databases + update scheduling + web pages + Web database systems + dynamic Web pages + preference-aware query + quality of data + dynamic web pages + quality contracts + quality of service + dynamic scheduling + database management systems + measurement + query processing + weather forecasting + computer science + technology management + spectrum + scheduling + adaptive algorithm + read-only queries + Internet + internet,AuthorProvided Keywords Not Found,"Typical Web-database systems receive read-only queries, that generate dynamic Web pages as a response, and write-only updates, that keep information up-to-date. Users expect short response times and low staleness. However, it may be extremely hard to apply all updates on time, i.e., keep zero staleness, and also get fast response times, especially in periods of bursty traffic. In this paper, we present the concept of quality contracts (QCs) which combines the two incomparable performance metrics: response time or quality of service (QoS), and staleness or quality of data (QoD). QCs allows individual users to express their preferences for the expected QoS and QoD of their queries by assigning ""profit"" values. To maximize the total profit from submitted QCs, we propose an adaptive algorithm, called QUTS. QUTS addresses the problem of prioritizing the scheduling of updates over queries using a two-level scheduling scheme that dynamically allocates CPU resources to updates and queries according to user preferences. We present the results of an extensive experimental study using real data (taken from a stock information Web site), where we show that QUTS performs better than baseline algorithms under the entire spectrum of QCs; QUTS also adapts fast to changing workloads."
7FE44940,International Conference on Data Engineering,bugra gedik + philip s yu + kimlung wu + ling liu,2007,A Load Shedding Framework and Optimizations for M-way Windowed Stream Joins,profitability + query processing + optimization problem + approximation technique + resource allocation + data analysis + tuple dropping + load shedding framework + GrubJoin + adaptive m-way windowed stream join + greedy heuristic + statistics,AuthorProvided Keywords Not Found,"Tuple dropping, though commonly used for load shedding in most stream operations, is inadequate for m-way, windowed stream joins. The join output rate can be overly reduced because it fails to exploit the time correlations likely to exist among interrelated streams. In this paper, we introduce GrubJoin; an adaptive, m-way, windowed stream join that effectively performs time correlation-aware CPU load shedding. GrubJoin maximizes the output rate by achieving near-optimal window harvesting, which picks only the most profitable window segments for the join. Due to combinatorial explosion of possible m-way join sequences involving window segments, m-way, windowed stream joins pose several unique challenges. We focus on addressing two of them: (1) How can we quickly determine the optimal window harvesting configuration for any m-way, windowed stream join? (2) How can we monitor and learn the time correlations among the streams with high accuracy and minimal overhead? To tackle these challenges, we formalize window harvesting as an optimization problem, develop greedy heuristics to determine near-optimal window harvesting configurations and use approximation techniques to capture the time correlations. Our experimental results show that GrubJoin is vastly superior to tuple dropping when time correlations exist and is equally effective when time correlations are nonexistent."
8142F734,International Conference on Data Engineering,yun chen + jignesh m patel,2007,Efficient Evaluation of All-Nearest-Neighbor Queries,All Nearest Neighbor queries + bucket quadtree index structure + multidimensional datasets + data analysis + algorithm design and analysis + indexation + MAXMAXDIST + multidimensional systems + pervasive computing + pruning metric + query processing + NXNDIST + nearest neighbor + database indexing + k nearest neighbor + clustering algorithms + MBRQT index + tree data structures + All-k-Nearest-Neighbor queries + R-tree based methods + pattern recognition,AuthorProvided Keywords Not Found,"The All Nearest Neighbor (ANN) operation is a commonly used primitive for analyzing large multi-dimensional datasets. Since computing ANN is very expensive, in previous works R*-tree based methods have been proposed to speed up this computation. These traditional index-based methods use a pruning metric called MAXMAXDIST, which allows the algorithms to prune out nodes in the index that need not be traversed during the ANN computation. In this paper we introduce a new pruning metric called the NXNDIST, and show that this metric is far more effective than the traditional MAXMAXDIST metric. In this paper, we also challenge the common practice of using R*-tree index for speeding up the ANN computation. We propose an enhanced bucket quadtree index structure, called the MBRQT, and using extensive experimental evaluation show that the MBRQT index can significantly speed up the ANN computation. In addition, we also present the MBA algorithm based on a depth-first index traversal and bi-directional node expansion strategy. Furthermore, our method can be easily extended to efficiently answer the more general All-k-Nearest-Neighbor (AkNN) queries."
79DC8285,International Conference on Data Engineering,cristina re + dan suciu + nilesh dalvi,2007,Efficient Top-k Query Evaluation on Probabilistic Data,databases + application software + probabilistic database + probability + data mining + Top-k query evaluation + enterprise applications + probabilistic data + motion pictures + uncertainty + sql + SQL + SQL query evaluation + query processing + SQL queries + monte carlo simulation + computer science + probabilistic databases + concurrent computing + approximate probabilities,AuthorProvided Keywords Not Found,"Modern enterprise applications are forced to deal with unreliable, inconsistent and imprecise information. Probabilistic databases can model such data naturally, but SQL query evaluation on probabilistic databases is difficult: previous approaches have either restricted the SQL queries, or computed approximate probabilities, or did not scale, and it was shown recently that precise query evaluation is theoretically hard. In this paper we describe a novel approach, which computes and ranks efficiently the top-k answers to a SQL query on a probabilistic database. The restriction to top-k answers is natural, since imprecisions in the data often lead to a large number of answers of low quality, and users are interested only in the answers with the highest probabilities. The idea in our algorithm is to run in parallel several Monte-Carlo simulations, one for each candidate answer, and approximate each probability only to the extent needed to compute correctly the top-k answers."
7D95B879,International Conference on Data Engineering,tian xia + yang du + donghui zhang + yufei tao + feng zhu,2007,On Multidimensional k-Anonymity with Local Recoding Generalization,time complexity + computational complexity + publishing + approximation algorithms + np hard + databases + data privacy + multidimensional systems + data mining + polynomials,,"This paper presents the first theoretical study, on using local-recoding generalization (LRG) to compute a Aanonymous table with quality guarantee. First, we prove that it is NP-hard both to find the table with the maximum quality, and to discover a solution with an approximation ratio at most 5/4. Then, we develop an algorithm with good balance between the approximation ratio and time complexity. The quality of our solution is verified by experiments. •_ 2007 IEEE."
7A0F77DF,International Conference on Data Engineering,a k chandel + nick koudas + ken q pu + divesh srivastava,2007,Fast Identification of Relational Constraint Violations,Boolean decision diagrams + col + decision diagrams + testing + functional dependency + fast identification + degradation + relational databases + boolean functions + data quality + integrity constraints + Boolean functions + synthetic data + logical constraints + relational constraint violations + synthetic data sets,AuthorProvided Keywords Not Found,"Logical constraints, (e.g., `phone numbers in Toronto can have prefixes 416, 647, 905 only'), are ubiquitous in relational databases. Traditional integrity constraints, such as functional dependencies, are examples of such logical constraints as well. However, under frequent database updates, schema evolution and transformations, they can be easily violated. As a result, tables become inconsistent and data quality is degraded. In this paper we study the problem of validating collections of user defined constraints on a number of relational tables. Our primary goal is to quickly identify which tables violate such constraints. Logical constraints are potentially complex logical formuli, and we demonstrate that they cannot be efficiently evaluated by SQL queries. In order to enable fast identification of constraint violations, we propose to build and maintain specialized logical indices on the relational tables. We choose Boolean Decision Diagrams (BDD) as the index structure to aid in this task. We first propose efficient algorithms to construct and maintain such indices in a space efficient manner. We then describe a set of query re-write rules that aid in the efficient utilization of logical indices during constraint validation. We have implemented our approach on top of a relational database and tested our techniques using large collections of real and synthetic data sets. Our results indicate that utilizing our techniques in conjunction with logical indices during constraint validation offers very significant performance advantages."
7CCA933E,International Conference on Data Engineering,christine robson + yuji watanabe + masayuki numao,2007,Parts Traceability for Manufacturers,information system + manufacturing industries + search time + automotive industry + supply chains + information management + indexing + supply chain + indexation + information retrieval + parts tracking + database search + distributed dataset + management information systems + product indexing + automobile industry + automotive engineering + distributed databases + parts traceability + parts lookup + reverse-lookups + reverse-indexing + automobile manufacturer,AuthorProvided Keywords Not Found,"Recalls and defective parts are a major problem for manufacturers and a major challenge in information management. Increasingly complex supply chains have created a situation where parts' data is distributed across a volatile network of suppliers' databases. The challenge of parts tracking in this environment requires maintaining data views across constantly-restructuring networks of information systems as new suppliers are contracted and subassemblies are outsourced. In this paper we present a method for manufacturers to efficiently and automatically index all component parts in their products, allowing for complex changes to the supply chain, and enabling highly efficient reverse-lookups for recalls. Our method improves parts lookup and search time by subdividing all component part indexes into a distributed, versioned matrix of tables which models the structure of the supply chain. We introduce a method of reverse-indexing component parts using manufacturing time to pinpoint products for selective recalls, running in a fraction of the time of a traditional database search. We evaluate our method for applicability to the automotive industry using statistically re-generated data from a major automobile manufacturer to simulate a supply chain 7 layers deep, comprising 1000 individual component parts, indexing a distributed dataset totaling approximately 1TB in size."
7EEE9C1C,International Conference on Data Engineering,hui joe chang + vipul arora + muralidhar krishnaprasad + zhen hua liu,2007,XMLTable Index - An Efficient Way of Indexing and Querying XML Property Data,XML property data querying + indexing + path/value index + data mining + indexation + use case + relational databases + query processing + XMLTable Index + indexes + xml + XML + industrial relations,AuthorProvided Keywords Not Found,"Efficiently storing and querying XML has been widely studied in research and industrial settings. Major RDBMS vendors now support XML as a native datatype in their systems and provide physical means of storing schema agnostic XML data. Typically this data is stored in CLOBs, BLOBs, or tree forms, with path or value indices used to efficiently process XQuery and SQL queries. However, in many use case queries derived from industrial XML applications, we find that it is very common to query XML based on a group of related property data and to query on the master-detail relationships using the SQL XMLTable construct. We propose an indexing mechanism called the XMLTable Index which is more efficient than the path and value index approach for this class of queries, and provides a way to efficiently process these queries over any physical XML storage form. The XMLTable Index complements the path/value index approach, and can be enhanced in its capabilities by using it in conjunction with path, value, text and other domain indices."
7DB3DFD6,International Conference on Data Engineering,yuan an + robert j miller + john mylopoulos + alexander borgida,2007,A Semantic Approach to Discovering Schema Mapping Expressions,referential integrity + semantic approach + graph theory + semantic web + testing + semantic similarity + relational databases + database management systems + schema mapping expressions + conceptual mapping + local schema constraint + assembly + declarative logical expressions + conceptual model + ontologies,AuthorProvided Keywords Not Found,"In many applications it is important to find a meaningful relationship between the schemas of a source and target database. This relationship is expressed in terms of declarative logical expressions called schema mappings. The more successful previous solutions have relied on inputs such as simple element correspondences between schemas in addition to local schema constraints such as keys and referential integrity. In this paper, we investigate the use of an alternate source of information about schemas, namely the presumed presence of semantics for each table, expressed in terms of a conceptual model (CM) associated with it. Our approach first compiles each CM into a graph and represents each table's semantics as a subtree in it. We then develop algorithms for discovering subgraphs that are plausible connections between those concepts/nodes in the CM graph that have attributes participating in element correspondences. A conceptual mapping candidate is now a pair of source and target subgraphs which are semantically similar. At the end, these are converted to expressions at the database level. We offer experimental results demonstrating that, for test cases of non-trivial mapping expressions involving schemas from a number of domains, the ""semantic"" approach outperforms the traditional technique in terms of recall and especially precision."
8105BD04,International Conference on Data Engineering,carsten binnig + d kossmann + eric lo,2007,Reverse Query Processing,open world assumption + program testing + business + database application testing + database generation + reverse query processing + algebra + dbms + relational databases + statistical distributions + database schema + test databases + query processing + integrity constraints + closed world assumption + SQL query debugging + table layouts + debugging + DBMS + logic design,AuthorProvided Keywords Not Found,"Generating databases for testing database applications (e.g., OLAP or business objects) is a daunting task in practice. There are a number of commercial tools to automatically generate test databases. These tools take a database schema (table layouts plus integrity constraints) and table sizes as input in order to generate new tuples. However, the databases generated by these tools are not adequate for testing a database application. If an application query is executed against such a synthetic database, then the result of that application query is likely to be empty or contain weird results, such as a report on the performance of a sales person that contains negative sales. To solve this problem, this paper proposes a new technique called reverse query processing (RQP). RQP gets a query and a result as input and returns a possible database instance that could have produced that result for that query. RQP also has other applications; most notably, testing the performance of DBMS and debugging SQL queries."
7FD7A706,International Conference on Data Engineering,david j dewitt + samuel madden + daniel j abadi + daniel myers,2007,Materialization Strategies in a Column-Oriented DBMS,data warehouse + projection operator + column-stores + read-mostly query workload + decision support + column-oriented DBMS + database management systems + relational databases + database architecture + data structures + data warehouses + row-stores + database systems + materialization strategy,AuthorProvided Keywords Not Found,"There has been renewed interest in column-oriented database architectures in recent years. For read-mostly query workloads such as those found in data warehouse and decision support applications, ""column-stores"" have been shown to perform particularly well relative to ""row-stores"" In order for column-stores to be readily adopted as a replacement for row-stores, however, they must present the same interface to client applications as do row stores, which implies that they must output row-store-style tuples. Thus, the input columns stored on disk must be converted to rows at some point in the query plan, but the optimal point at which to do the conversion is not obvious. This problem can be considered as the opposite of the projection problem in row-store systems: while row-stores need to determine where in query plans to place projection operators to make tuples narrower, column-stores need to determine when to combine single-column projections into wider tuples. This paper describes a variety of strategies for tuple construction and intermediate result representations and provides a systematic evaluation of these strategies."
7F622553,International Conference on Data Engineering,jiong yang + shijie zhang + v cheedella,2007,Monkey: Approximate Graph Mining Based on Spanning Trees,databases + biological data + algorithm design and analysis + graph theory + data mining + mining process + spanning trees + tree graphs + frequency + approximate graph mining + preverification check + graph database + proteins + bioinformatics + spanning tree + tree data structures + lattices + algorithm Monkey,AuthorProvided Keywords Not Found,"In the recent past, many exact graph mining algorithms have been developed to find frequent patterns in a graph database. However, many networks or graphs generated from biological data and other applications may be incomplete or inaccurate. Hence, it is necessary to design approximate graph mining techniques. In this paper, we will study the problem of approximate graph mining and propose an optimized solution which uses frequent trees and a spanning tree based pre-verification check in the mining process."
80895E4C,International Conference on Data Engineering,stratis d viglas,2007,Distributed File Structures in a Peer-to-Peer Environment,peer-to-peer computing + performance testing + distributed file structures + computer networks + testing + peer-to-peer environment + distributed environment + informatics + black-box approach + distributed computing + resource allocation + fault-tolerant load-balanced scalable technique + computer applications + fault tolerant + centralized file system + load balance + data structures + distributed data + software performance evaluation + peer-to-peer network + dictionaries,AuthorProvided Keywords Not Found,"As computing becomes increasingly distributed, the need for accessing and manipulating distributed data becomes prominent. The focus so far has been on developing fault-tolerant, load-balanced and scalable techniques for distributing data across the network without placing much importance on the semantics of the applications built on top of them - much in the way a centralized file system simply serves disk pages without handling their semantics. In this paper, we continue this ""black-box"" approach and extend it to the next level. We take standard file structures and distribute them across a peer-to-peer network and test their performance. We then identify performance bottlenecks and propose ways of overcoming them. These results, we believe, exhibit the potential of seamlessly extending computing applications assuming standard (centralized) file I/O capabilities to work in distributed environments."
76C36897,International Conference on Data Engineering,ihab f ilyas + mohamed a soliman + nick koudas,2007,Finding Skyline and Top-k Bargaining Solutions,databases + game theory + top-k bargaining solution + privacy + Web interaction scenario + uncertainty + database theory + query processing + computer science + context modeling + skyline bargaining solution + Internet + protocols + internet,AuthorProvided Keywords Not Found,We address skyline and top-k processing in Web interaction scenarios. We model the problem space based on game theory principles and present new algorithms and heuristics to realize solutions efficiently.
7F1E1A41,International Conference on Data Engineering,laura m haas + steve b cousins,2007,Information for People,human-computer interaction + information management + human computer interaction + user experience,AuthorProvided Keywords Not Found,"Ordinary people have access to unprecedented volumes of information today. Researchers in the fields of information management (IM) and human-computer interaction (HCI) are reacting to this challenge from their own unique perspectives. Having access to a billion records is cool, but having access to a billion people is awesome. In this paper, we look at recent research from both communities, and speculate on how interactions between the communities could enhance the user experience of information."
7D7023C1,International Conference on Data Engineering,yossi matias + yariv matia,2007,Calibration and Profile based Synopses Error Estimation and Synopses Reconciliation,databases + calibration synopses error estimation + testing + query approximation error + statistical profile + ad hoc error estimation + error estimation function + database management systems + approximation error + heart + computer science + profile based synopses error estimation + synopses reconciliation + interference + calibration,AuthorProvided Keywords Not Found,"An important factor in the effective utilization of data synopses is the ability to have good a priori estimates on their expected query approximation errors. Such estimates are essential for the appropriate decisions regarding which synopses to build and how much space to allocate to them, which are also at the heart of the synopses reconciliation problem. We present a novel synopses error estimation method based on the construction of synopses-dependant error estimation functions. These functions are computed in a pre-processing stage using a calibration method. Subsequently, they are used to provide ad hoc error estimation w.r.t. given data sets and query workloads based only on their statistical profiles. We also present a novel approach to synopses reconciliation, using the error-estimation functions within synopses reconciliation algorithms, gaining significant efficiency improvements by lowering to a minimum and even avoiding interference to the operational databases. Our method enables the first practical solution for the dynamic synopses reconciliation problem."
7FB3A403,International Conference on Data Engineering,matthias brantner + norman may + guido moerkotte,2007,Unnesting Scalar SQL Queries in the Presence of Disjunction,database languages + Structured Query Language + relational algebra + nested SQL query + algebra + relational databases + sql + SQL + query processing + relation algebra + algebraic expression + unnesting strategy + writing + structured query language + tree data structures + bypass operator + scalar SQL query + unnesting equivalence + query translation,AuthorProvided Keywords Not Found,"Optimizing nested queries is an intricate problem. It becomes even harder if in a nested query the linking predicate or the correlation predicate occurs disjunctively. We present the first unnesting strategy that can effectively deal with such queries. The starting point of our approach is to translate SQL into the relational algebra extended by bypass operators. Then we present for the first time unnesting equivalences which are valid for algebraic expressions containing bypass operators. Applying these to the translated queries results in our effective unnesting strategy for nested SQL queries with disjunction. With an extensive experimental study (including three commercial DBMSs), we demonstrate the possible performance gains of our approach."
7B0ECB43,International Conference on Data Engineering,k chenchuan chang + govind kabra + zhen zhang,2007,Dewex: An Exploration Facility for Enabling the Deep Web Integration,meta data + databases + matrix optimization + design optimization + Dewex + graph theory + data mining + question answering + search mechanism + schematic metadata + matrix algebra + optimisation + deep Web integration + computer science + writing + metadata graph + html + service oriented architecture + Internet + query formulation + deep web + internet,AuthorProvided Keywords Not Found,"In this demo, we present Dewex, an exploration facility for answering questions like T1-TA, running on repository of over 30,000 real sources. The main technical contributions of this demo are: as our system, we present a novel facility for exploring the deep Web; as our solution, we propose a schematic metadata based source modeling, and a generalized search mechanism to compute associativity in metadata graph; and in our realization, to enable online exploration, we propose to speed up computation using matrix optimization."
7DA3BBB9,International Conference on Data Engineering,d kossmann + bjorn jarisch + m a vaz salles + cristian duda + j dittrich,2007,Bringing Precision to Desktop Search: A Predicate-based Desktop Search Architecture,Google + search engines + encoding + intranets + desktop search architecture + navigation + xml + Intranet + Internet + keyword search interface + query formulation + information search + internet,AuthorProvided Keywords Not Found,"Google and other products have revolutionized the way we search for information on the Internet, Intranet, and on our desktop. However, the current generation of search products does not exploit the structure and semantics of data, as defined by application programs (e.g., Word or Excel) that generate the data. This paper shows how search technology can be enhanced with implicit predicates, in order to take into account the structure and semantics defined by applications. Better search results are produced with tolerable performance overhead, while at the same time maintaining the simplicity of the keyword search interface."
8145DCC8,International Conference on Data Engineering,evaggelia pitoura + panos vassiliadis + konstantinos stamkopoulos,2007,Efficient Deployment of Web Service Workflows,security + web services + time measurement + file servers + greedy algorithm + greedy algorithms + xml + computer science + load distribution + cost function + web service + service provider + topology,,"The appropriate deployment of web service operations at the service provider site plays a critical role in the efficient provision of services to clients. In this paper, we assume that a service provider has several servers over which web service operations can be deployed. Then, given a workflow of web services and the topology of the servers, the most efficient mopping of operations to servers must be discovered. Efficiency is measured in terms of two cost functions that concern the execution time of the workflow and the fairness of the load distribution among the servers. We study different topologies for the workflow structure and the server connectivity and propose a suite of greedy algorithms for each combination."
81760B74,International Conference on Data Engineering,fusheng wang + patrick kling + j d m pearson + cornelius rabsch + peiya liu,2007,Web-based Collaborative Information Integration for Scientific Research,data analysis + data management + scientific research + integrable system + mathematics + information retrieval + dynamic collaboration environment + data integrity + information analysis + computer science + information integration + optical imaging + collaboration + data systems + collective contributions + scientific information systems + Internet + Web-based collaborative information integration + data integration system + internet,AuthorProvided Keywords Not Found,"Scientific research becomes increasingly reliant on collaborative effort among multiple institutions and interdisciplinary consortia, through sharing scientific experiments and data and collaboration on analysis of data and results. Besides information sharing, there is a growing need to create a framework to enable collaborative, cross-disciplinary research, which can facilitate a research community into a default mode of collaboration, including: i) enabling scientific researchers to dynamically interact with others, collaboratively author, annotate, review, comment on others' data, and discuss their research; ii) providing a dynamic collaboration environment by harnessing collective contributions from a group of researchers; and iii) facilitating broader participation of people into research projects. While traditional data management and integration systems put much focus on passively integrating existing data, the collaboration among data providers and users is quite limited. In this paper, we present a Web-based collaborative platform to dynamically integrate information for scientific research. Besides data integration, the system provides essential collaboration capabilities to boost user participation and collaboration. The system will not only enable users as active information contributors to bring significant new values to the data, but also provide an environment for scientific researchers to do collaborative research in a large research community."
7F2D4310,International Conference on Data Engineering,torsten grabs + babu krishnaswamy + shankar pal + cesar a galindolegaria + adrian baras,2007,Optimizing Similar Scalar Subqueries for XML Processing in Microsoft SQL Server,property bag scenario + relational subquery + data mining + Microsoft SQL Server + indexation + query performance + use case + XML queries + relational query optimization + scalability + sql + query processing + SQL queries + property extraction + xml + multicolumn subquery + scalar subqueries + indexing + relational databases + SQL + predicate disjunction + difference set + query optimization + XML + rowset pivoting + XML processing,AuthorProvided Keywords Not Found,"XML is often used to represent objects that expose different sets of properties. This ""property bag"" scenario is a prominent use case for the XML support added to Microsoft SQL Server 2005. However, each property extraction in our initial implementation executed as a separate relational subquery. This was problematic since query performance became unacceptable even for small data sizes when returning an increasing number of properties. We addressed this problem by developing an interesting generalization of common subexpressions. This paper makes the following contributions: (1) it introduces an equivalence rewrite for relational query optimization to fold similar scalar subqueries. Several such subqueries are merged into a single equivalent multi-column subquery using both predicate disjunction and rowset pivoting. The rewrite operates at the logical operator level which makes it equally applicable to XML queries and SQL queries. (2) We explain how this optimization can be applied to the XML property bag scenario and how it has been implemented for the XML index in Microsoft SQL Server 2005. (3) An experimental investigation with Microsoft SQL Server 2005 studies the performance characteristics of the optimization. It shows that the optimization yields significant performance improvements - without limiting essential optimizer execution plan choices."
79A60B51,International Conference on Data Engineering,pedro derose + wei shen + long vu + r ramakrishnan + anhai doan,2007,Source-aware Entity Matching: A Compositional Approach,databases + personal information management + couplings + web pages + record linkage + information management + relational optimizer + query evaluation + machine learning + compositional approach + source-aware entity matching + routing + social network analysis + human computer interaction + entity-relationship modelling,AuthorProvided Keywords Not Found,"Entity matching (a.k.a. record linkage) plays a crucial role in integrating multiple data sources, and numerous matching solutions have been developed. However, the solutions have largely exploited only information available in the mentions and employed a single matching technique. We show how to exploit information about data sources to significantly improve matching accuracy. In particular, we observe that different sources often vary substantially in their level of semantic ambiguity, thus requiring different matching techniques. In addition, it is often beneficial to group and match mentions in related sources first, before considering other sources. These observations lead to a large space of matching strategies, analogous to the space of query evaluation plans considered by a relational optimizer. We propose viewing entity matching as a composition of basic steps into a ""match execution plan"". We analyze formal properties of the plan space, and show how to find a good match plan. To do so, we employ ideas from social network analysis to infer the ambiguity and related-ness of data sources. We conducted extensive experiments on several real-world data sets on the Web and in the domain of personal information management (PIM). The results show that our solution significantly outperforms current best matching methods."
8124961B,International Conference on Data Engineering,qing zhang + nick koudas + ting yu + divesh srivastava,2007,Aggregate Query Answering on Anonymized Tables,aggregate query answering + databases + numerical sensitive attribute + data analysis + permutation-based anonymization + algorithm design and analysis + undesirable information leakage + l-diversity + privacy + information analysis + availability + query processing + categorical sensitive attribute + microdata privacy protection + anonymized table + ad hoc aggregate analysis + data privacy + k-anonymity,AuthorProvided Keywords Not Found,"Privacy is a serious concern when microdata need to be released for ad hoc analyses. The privacy goals of existing privacy protection approaches (e.g., k-anonymity and l-diversity) are suitable only for categorical sensitive attributes. Since applying them directly to numerical sensitive attributes (e.g., salary) may result in undesirable information leakage, we propose privacy goals to better capture the need of privacy protection for numerical sensitive attributes. Complementing the desire for privacy is the need to support ad hoc aggregate analyses over microdata. Existing generalization-based anonymization approaches cannot answer aggregate queries with reasonable accuracy. We present a general framework of permutation-based anonymization to support accurate answering of aggregate queries and show that, for the same grouping, permutation-based techniques can always answer aggregate queries more accurately than generalization-based approaches. We further propose several criteria to optimize permutations for accurate answering of aggregate queries, and develop efficient algorithms for each criterion."
7E05924D,International Conference on Data Engineering,charu c aggarwal,2007,"On Randomization, Public Information and the Curse of Dimensionality",data privacy preservation + databases + couplings + algorithm design and analysis + curse of dimensionality + data mining + randomization method + information analysis + data dimensionality + database theory + randomised algorithms + government + data privacy + public information,AuthorProvided Keywords Not Found,"A key method for privacy preserving data mining is that of randomization. Unlike k-anonymity, this technique does not include public information in the underlying assumptions. In this paper, we provide a first comprehensive analysis of the randomization method in the presence of public information. We define a quantification of the randomization method which we refer to as k-randomization of the data. The inclusion of public information in the theoretical analysis of the randomization method results in a number of interesting and insightful conclusions. These conclusions expose some vulnerabilities of the randomization method. We show that the randomization method is unable to effectively achieve privacy in the high dimensional case. We theoretically quantify the degree of randomization required to guarantee privacy as a function of the underlying data dimensionality. Furthermore, we show that the randomization method is susceptible to many natural properties of real data sets such as clusters or outliers. Finally, we show that the use of public information makes the choice of perturbing distribution very critical in a number of subtle ways. Our analysis shows that the inclusion of public information in the analysis makes the goal of privacy preservation more elusive than previously thought for the randomization method."
7F740C36,International Conference on Data Engineering,noureddine mouaddib + guillaume raschia + regis saintpaul,2007,Database Summarization: The SaintEtiQ System,database summarization + online linguistic summarization system + sampling methods + computational linguistics + fuzzy set theory + data mining + SaintEtiQ system + relational database + relational databases + marketing management + knowledge management + normal form + service oriented architecture + fuzzy systems,AuthorProvided Keywords Not Found,"SaintEtiQ (Saint-Paul et al., 2005) is an on-line linguistic summarization system of tables and/or views. Our approach considers a first normal form relation R(A1,...,An) in the relational database model, and constructs a new relation R*( A1,...,An), in which tuples z are summaries and attribute values are linguistic labels describing a set of tuples Rz, sub-table of R. Thus, the SaintEtiQ system identifies statements of the form ""Most tuples of R are (a11 or a12...or a1m1) and (a21...or a a2m1)""."
7CEFFCE5,International Conference on Data Engineering,irina rozenbaum + s muthukrishnan + theodore johnson,2007,Monitoring Regular Expressions on Out-of-Order Streams,sequential algorithm + parallel algorithms + pattern matching + payloads + data security + parallel algorithm + regular expression + regular expression matching + mixed algorithm + IP packet streams + out of order + automata + protocols + IP networks + out-of-order data streaming,AuthorProvided Keywords Not Found,"We present an efficient algorithm for regular expression matching on streams with out of order data, while maintaining a small state and without complete stream reconstruction. We have implemented three versions of the algorithm - sequential, parallel and mixed - and show by experimental study that the algorithms are highly effective in matching regular expressions on IP packet streams."
7F58AA6D,International Conference on Data Engineering,jiawei han + xiaoxin yin + philip s yu,2007,Object Distinction: Distinguishing Objects with Identical Names,random walk probability + couplings + merging + relational similarity + support vector machines + probability + information retrieval + random processes + DISTINCT + world wide web + svm + SVM + relational databases + neighbor tuples + set resemblance + random walk + identical names + object distinction + real database + training data,AuthorProvided Keywords Not Found,"Different people or objects may share identical names in the real world, which causes confusion in many applications. It is a nontrivial task to distinguish those objects, especially when there is only very limited information associated with each of them. In this paper, we develop a general object distinction methodology called DISTINCT, which combines two complementary measures for relational similarity: set resemblance of neighbor tuples and random walk probability, and uses SVM to weigh different types of linkages without manually labeled training data. Experiments show that DISTINCT can accurately distinguish different objects with identical names in real databases."
804A6089,International Conference on Data Engineering,p u praveen kumar + a antony raj,2007,Branch Sequencing Based XML Message Broker Architecture,application software + pattern matching + data exchange + indexing + XML filtering system + indexation + xml document + node sequencing method + information filtering + XML message broker architecture + data engineering + query processing + routing + XML twig queries + computer science + satisfiability + computer architecture + xml + XML + branch sequencing + filtering,AuthorProvided Keywords Not Found,"As XML has become the de-facto standard for data exchange in several applications, XML message brokers assume a lot of importance. Message brokers we address in this paper store a large number of user profiles, in the form of XML twig queries, that represent the data requirement of users/applications. In contrast to publisher-subscriber systems, message brokers select the part of the data that is of interest to the user and send it to her, instead of the whole matched document. In this paper, we propose a new sequencing based XML message broker architecture that handles tens of thousands of user profiles and delivers to each user the XML elements (or nodes) that satisfy the user's twig query. We introduce a novel node sequencing method called branch sequencing that converts an XML twig query into a branch sequence. Intuitively, nodes are sequenced branch by branch. We show that by storing user profiles in the form of branch sequences, and appropriately indexing them, we can efficiently generate the qualifying nodes for each profile, as the input XML document streams by. The input document nodes are streamed into the system in the document order. The branch sequencing technique we propose enables holistic matching of twig queries to the document. Another important feature of the system is that it carries out ordered profile matching. We have implemented the system and compared its performance with a state-of-art XML filtering system and have shown that the filtering time reduces significantly and the system scales up very well."
81433D69,International Conference on Data Engineering,feng cao + ying yan + chaofeng sha + aoying zhou + xiaofeng he,2007,Distributed Data Stream Clustering: A Fast EM-based Approach,application software + data streams clustering + fast EM-based approach + testing + distributed processing + data clustering + distributed system + sensor fusion + distributed environment + expectation maximization framework + distributed data stream clustering + gaussian processes + pattern clustering + unreliable distributed system + clustering algorithms + expectation-maximisation algorithm + expectation maximization + online clustering,AuthorProvided Keywords Not Found,"Clustering data streams has been attracting a lot of research efforts recently. However, this problem has not received enough consideration when the data streams are generated in a distributed fashion, whereas such a scenario is very common in real life applications. There exist constraining factors in clustering the data streams in the distributed environment: the data records generated are noisy or incomplete due to the unreliable distributed system; the system needs to on-line process a huge volume of data; the communication is potentially a bottleneck of the system. All these factors pose great challenge for clustering the distributed data streams. In this paper, we proposed an EM-based (Expectation Maximization) framework to effectively cluster the distributed data streams, with the above fundamental challenges in mind. In the presence of noisy or incomplete data records, our algorithms learn the distribution of underlying data streams by maximizing the likelihood of the data clusters. A test-and-cluster strategy is proposed to reduce the average processing cost, which is especially effective for online clustering over large data streams. Our extensive experimental studies show that the proposed algorithms can achieve a high accuracy with less communication cost, memory consumption and CPU time."
7EF6536E,International Conference on Data Engineering,mathieu roche + mark roantree + zohra bellahsene + fabien duchateau,2007,Poster Session: An Indexing Structure for Automatic Schema Matching,mediation + interoperability + xml + ontologies + semantic web + indexing + database indexing + open systems + world wide web + information management + distributed environment,,"https://hal-lirmm.ccsd.cnrs.fr/lirmm-00138117"
8146F3FC,International Conference on Data Engineering,nick koudas + c mishra,2007,A Lightweight Online Framework For Query Progress Indicators,open source data manager + data management + query execution + prototypes + lightweight online framework + sql + SQL + feedback + query processing + pipelines + predictive models + human computer interaction + query progress indicators + statistics + database systems,AuthorProvided Keywords Not Found,"Recently there has been increasing interest in the development of progress indicators for SQL queries. In this paper we present a lightweight online framework for this problem. Our framework is online, in the sense that it refines its estimate of query progress based on feedback received during query execution. It is lightweight, since our techniques are designed to impose minimal overhead on query execution without sacrificing accuracy of estimates. Our framework can estimate progressively the output size of various relational operators and pipelines. These include binary and multiway joins as well as typical grouping operations and combinations thereof. We describe the various algorithms used to efficiently implement the estimators and present the results of a thorough evaluation of a prototype implementation of our framework in an open source data manager. Our results demonstrate the feasibility and practical utility of the approach presented herein."
7DAD03D6,International Conference on Data Engineering,nicolas bruno + swarat chaudhuri,2007,An Online Approach to Physical Design Tuning,query processing + update statements + database system + physical design + tuning + account storage constraints + computer bugs + algorithm design and analysis + query workload + automated physical design tuning + database management systems + database systems,AuthorProvided Keywords Not Found,"There has been considerable work on automated physical design tuning for database systems. Existing solutions require offline invocations of the tuning tool and depend on DBAs identifying representative workloads manually. In this work, we propose an alternative approach to the physical design problem. Specifically we design algorithms that are always-on and continuously modify the current physical design reacting to changes in the query workload. Our techniques have low overhead and take into account storage constraints, update statements, and the cost to create temporary physical structures."
8072BB31,International Conference on Data Engineering,jayant r haritsa + gopal das,2007,Robust Heuristics for Scalable Optimization of Complex SQL Queries,iterative dynamic programming + robustness + dynamic programming + decision support + query execution plan + refining + sql + feature vector + SQL + query processing + database system + genetics + engines + query optimizer + cost function + search space + complex SQL queries + scalable optimization + database systems,AuthorProvided Keywords Not Found,"Modern database systems incorporate a query optimizer to identify the most efficient ""query execution plan"" for executing the declarative SQL queries submitted by users. A dynamic-programming-based approach is used to exhaustively enumerate the combinatorially large search space of plan alternatives and, using a cost model, to identify the optimal choice. While dynamic programming (DP) works very well for moderately complex queries with up to around a dozen base relations, it usually fails to scale beyond this stage due to its inherent exponential space and time complexity. Therefore, DP becomes practically infeasible for complex queries with a large number of base relations, such as those found in current decision-support and enterprise management applications. To address the above problem, a variety of approaches have been proposed in the literature. Some completely jettison the DP approach and resort to alternative techniques such as randomized algorithms, whereas others have retained DP by using heuristics to prune the search space to computationally manageable levels. In the latter class, a well-known strategy is ""iterative dynamic programming"" (IDP) wherein DP is employed bottom-up until it hits its feasibility limit, and then iteratively restarted with a significantly reduced subset of the execution plans currently under consideration. The experimental evaluation of IDP indicated that by appropriate choice of algorithmic parameters, it was possible to almost always obtain ""good"" (within a factor of twice of the optimal) plans, and in the few remaining cases, mostly ""acceptable"" (within an order of magnitude of the optimal) plans, and rarely, a ""bad"" plan. While IDP is certainly an innovative and powerful approach, we have found that there are a variety of common query frameworks wherein it can fail to consistently produce good plans, let alone the optimal choice. This is especially so when star or clique components are present, increasing the complexity of t- e join graphs. Worse, this shortcoming is exacerbated when the number of relations participating in the query is scaled upwards."
7F48D8D1,International Conference on Data Engineering,shanhung wu + kunta chuang + mingsyan chen + chungmin chen,2007,DIKNN: An Itinerary-based KNN Query Processing Algorithm for Mobile Sensor Networks,wireless sensor networks + DIKNN + indexation + data structure + spatial index + mobile sensor networks + network dynamics + indexing support + query processing + mobile computing + sensor node mobility + database indexing + k nearest neighbor + itinerary-based KNN query processing + in-network data structure + index structures + density-aware itinerary KNN query processing + centralized spatial index,AuthorProvided Keywords Not Found,"Current approaches to k nearest neighbor (KNN) search in mobile sensor networks require certain kind of indexing support. This index could be either a centralized spatial index or an in-network data structure that is distributed over the sensor nodes. Creation and maintenance of these index structures, to reflect the network dynamics due to sensor node mobility, may result in long query response time and low battery efficiency, thus limiting their practical use. In this paper, we propose a maintenance-free, itinerary-based approach called density-aware itinerary KNN query processing (DIKNN). The DIKNN divides the search area into multiple cone-shape areas centered at the query point. It then performs a query dissemination and response collection itinerary in each of the cone-shape areas in parallel. The design of the DIKNN scheme also takes into account challenging issues such as the the dynamic adjustment of the search radius (in terms of number of hops) according to spatial irregularity or mobility of sensor nodes. The simulation results show that DIKNN yields substantially better performance and scalability over previous work, both as k increases and as the sensor node mobility increases. It outperforms the second runner with up to 50% saving in energy consumption and up to 40% reduction in query response time, while rendering the same level of query result accuracy."
7FAAF791,International Conference on Data Engineering,wai gen yee + ophir frieder + dongmei jia + linh thai nguyen,2007,A Tool for Information Retrieval Research in Peer-to-Peer File Sharing Systems,databases + data analysis + peer-to-peer computing + peer-to-peer file sharing systems + information retrieval + Lime Wire + educational technology + query processing + independent component analysis + java + Gnutella standard + IR-Wire + file sharing + statistics,AuthorProvided Keywords Not Found,"We introduce IR-Wire, a tool for information retrieval research and education in peer-to-peer file-sharing systems. Built on top of Lime Wire's implementation of the popular Gnutella standard, it includes functionality to collect data on queries and shared files and stores them in a way to make analyses simple. IR-Wire is designed modularly to facilitate its customization for other uses."
814D0BE7,International Conference on Data Engineering,meng hu + shijie zhang + jiong yang,2007,TreePi: A Novel Graph Indexing Method,tree-partition-based query processing + XML documents + indexation + complex structured data + xml document + tree graphs + chemical compounds + graph queries + query processing + structured data + database indexing + xml + synthetic data + proteins + xml documents + search space + TreePi + protein networks + indexing + testing + information retrieval + graph indexing + subgraph isomorphism tests + center distance constraints + XML + frequent subtrees + graph databases,AuthorProvided Keywords Not Found,"Graphs are widely used to model complex structured data such as XML documents, protein networks, and chemical compounds. One of the fundamental problems in graph databases is efficient search and retrieval of graphs using indexing techniques. In this paper, we study the problem of indexing graph databases using frequent subtrees as indexing structures. Trees can be manipulated efficiently while preserving a lot of structural information of the original graphs. In our proposed method, frequent subtrees of a database are selected as the feature set. To save memory, the set of feature trees is shrunk based on a support threshold function and their discriminative power. A tree-partition based query processing scheme is proposed to perform graph queries. The concept of center distance constraints is introduced to prune the search space. Furthermore, a new algorithm which utilizes the location information of indexing structures is used to perform subgraph isomorphism tests. We apply our method on a wide range of real and synthetic data to demonstrate the usefulness and effectiveness of this approach."
7D733DF3,International Conference on Data Engineering,j goldstein + jingren zhou + luping ding + p a larson,2007,Dynamic Materialized Views,control tables + feedback loop + query processing + dynamic execution plan + automatic control + materialized views + space technology + dynamic materialized views + database systems + flexible materialization strategy,AuthorProvided Keywords Not Found,"A conventional materialized view blindly materializes and maintains all rows of a view, even rows that are never accessed. We propose a more flexible materialization strategy aimed at reducing storage space and view maintenance costs. A dynamic materialized view selectively materializes only a subset of rows, for example, the most frequently accessed rows. One or more control tables are associated with the view and define which rows are currently materialized. The set of materialized rows can be changed dynamically, either manually or automatically by an internal cache manager using a feedback loop. Dynamic execution plans are generated to decide whether the view is applicable at run time. Experimental results in Microsoft SQL Server show that compared with conventional materialized views, dynamic materialized views greatly reduce storage requirements and maintenance costs while achieving better query performance with improved buffer pool efficiency."
7DB5CE40,International Conference on Data Engineering,cheeyong chan + yuan ni,2007,Piggyback Optimization of XML Data Dissemination,XML data dissemination + content-based dissemination + optimisation + upstream router + indexing + information dissemination + xml + routing protocols + collaboration + XML + data dissemination + piggyback optimization,AuthorProvided Keywords Not Found,"In this paper, we have proposed a novel approach to optimize the performance of content-based dissemination of XML data by piggybacking useful annotations to the document being forwarded so that a downstream router can leverage the processing done by its upstream router to reduce its own processing overhead. The piggyback optimization approach outperforms the conventional method by a factor of 2."
7F5B9A1C,International Conference on Data Engineering,loreto bravo + andrei lopatenko,2007,Efficient Approximation Algorithms for Repairing Inconsistent Databases,databases + denial integrity constraint + merging + chromium + greedy algorithms + data integrity + efficient approximation + approximation algorithms + database management systems + computational complexity analysis + integrity constraints + demography + numerical built-in predicates + inconsistent database repair + computational complexity,AuthorProvided Keywords Not Found,"We consider the problem of repairing a database that is inconsistent wrt a set of integrity constraints by updating numerical values. In particular, we concentrate on denial integrity constraints with numerical built-in predicates. So far research in this context has concentrated in computational complexity analysis. In this paper we focus on efficient approximation algorithms to obtain a database repair and we present an algorithm that runs in O(n log n) wrt the size of the database. Our experimental evaluations show that even for large databases an approximate repair of the database can be computed efficiently despite the fact that the exact problem is computationally intractable. Finally, we show that our results can also be applied to database repairs obtained by a minimal number of tuple deletions."
7EAB11C9,International Conference on Data Engineering,russell cheng + jinchuan chen,2007,Efficient Evaluation of Imprecise Location-Dependent Queries,sampling methods + location based services + measurement errors + location based service + sampling error + uncertainty + global positioning system + query processing + location data with uncertainty + mobile computing + measurement error + imprecise location-dependent queries + probabilistic guarantee + probability distribution + location-based services,AuthorProvided Keywords Not Found,"In location-based services, it is common for a user to issue a query based on his/her current position. One such example is ""find the available cabs within two miles of my current location"". Very often, the query issuers' locations are imprecise due to measurement error, sampling error, or message delay. They may also want to protect their privacy by providing a less precise location. In this paper, we study the efficiency of queries that return probabilistic guarantees for location data with uncertainty. We classify this query into two types, based on whether the data (1) has no uncertainty (e.g., shops and restaurants), or (2) has a controlled degree of uncertainty (e.g., moving vehicles). Based on this classification, we develop three methods to improve the computational and I/O performance. The first method expands the query range based on the query issuer's uncertainty. The second idea exchanges the roles of query and data. The third technique exploits the fact that users may only be interested in answers with probabilities higher than some threshold. Experimental simulation over a realistic dataset reveals that our approaches improve the query performance significantly."
7D56A300,International Conference on Data Engineering,shuigeng zhou + jihong guan,2007,GPress: Towards Effective GML Documents Compresssion,geographic data exchange + data compression + geographic data transmission + memory + data mining + GML compressor + geographic information systems + compression ratio + markup languages + documents compression + storage management + geographic data storage + compressors + Geography Markup Language + geography + xml + specification languages + GPress + geography markup language,AuthorProvided Keywords Not Found,"This demonstration presents a GML specific compressor that exploits the unique characteristics of GML documents to achieve better compression performance than the existing XML compressors. We call this compressor GPress, an abbreviation of ""GML comPressor"". To the best of our knowledge, GPress is the first compressor developed specifically for GML documents. Experimental results show that GPress outperforms the best existing XML compressor XMill evidently in compression ratio."
7F5BFB16,International Conference on Data Engineering,brian cook + george candea + songyun duan + shivnath babu,2007,Toward Self-Healing Multitier Services,cloud computing + robustness + sun + database + history + grid computing + logic + database management systems + utopia + web server + statistics,,"Are self-healing database-centric multitier services utopia or just a hard puzzle? We argue for the latter and aim to identify the missing pieces of this puzzle. We advocate robust and scalable learning-based approaches to self-healing that we expect to work well for a large class of multitier services. We identify performance-availability problems (PAPs) as the most relevant target for self-healing, and argue that PAPs are best addressed macroscopically, outside the realm of individual tiers. Finally, we lay out a research agenda for learning-based approaches to selfhealing, to enable wider deployment of self-healing multitier services."
7B8454FF,International Conference on Data Engineering,dimitrios georgakopoulos + dannon baker,2007,The Video Event Awareness System,security + near real time + algorithm design and analysis + surveillance video stream analysis + engines + information analysis + video streaming + video signal processing + video surveillance + video evidence + Video Event Awareness System,AuthorProvided Keywords Not Found,"The Video Event Awareness System (VEAS) analyzes surveillance video streams from thousands of video cameras and automatically detects complex events in near real-time - at pace with their input video streams. For events of interest to security personnel, VEAS generates and routes alerts and related video evidence to subscribing security personnel."
7FAB489B,International Conference on Data Engineering,gaurav n pradhan + navzer engineer + balakrishnan prabhakaran + mihai nadin,2007,Integration of Motion Capture and EMG data for Classifying the Human Motions,motion capture + feature extraction + feature space + feature vector + fuzzy set theory + orthopedic surgery + fuzzy clustering + information analysis + sports medicine + dimensionality reduction + data mining + gait analysis + bioinformatics + three dimensional + singular value decomposition + image classification,,"Three dimensional motion capture facility is a powerful tool for quantitative and qualitative assessment of multijoint external movements. Electro-myograph (EMG) signals give the physiologic information of muscles while doing motions. In this paper, our objective is to integrate these two different bio-medical data together and to extract precise and accurate feature information for classifying the human motions. When both forms of data are integrated and analyzed together, the information achieved will be immensely useful to quantify the complex human motions for medical reasons or sport performances. These biological quantifications of biomechanical data, are useful for gait analysis and several orthopedic applications, such as joint mechanics, prosthetic designs, and sports medicines. The different dimensionality reduction approaches such Integral of Absolute value and Weighted Singular Value Decomposition are used to extract the preliminary features from EMG and motion capture data respectively. On combining these feature vectors, fuzzy clustering such as Fuzzy c-means (FCM) is performed on these vectors that are mapped as the points in multi-dimensional feature space. We get the degree of memberships with every cluster for each mapped point. This extracted information is used as the final feature vectors for classifying the human motions."
7E91972F,International Conference on Data Engineering,haidar samet + charles b cranston,2007,Indexing Point Triples Via Triangle Geometry,image database search + shape + indexing + point-based index + indexation + computational geometry + visual databases + database search + hyperdimensional index space + spatial relationships + k-fold rotational symmetry + triangle geometry + query processing + structured index + mobile computing + automation + database indexing + indexes + clustering algorithms + single linear dimension + indexing point,AuthorProvided Keywords Not Found,"Database search for images containing icons with specific mutual spatial relationships can be facilitated by an appropriately structured index. For the case of images containing subsets each of which consist of three icons, the one-to-one correspondence between (distinct) point triples and triangles allows the use of such triangle attributes as position, size, orientation, and ""shape"" in constructing a point-based index, in which each triangle maps to a single point in a resulting hyperdimensional index space. Size (based on the triangle perimeter) can be represented by a single linear dimension. The abstract ""shape"" of a triangle induces a space that is inherently two-dimensional, and a number of alternative definitions of a basis for this space are examined. Within a plane, orientation reduces to rotation, and (after assignment of a reference direction for the triangle) can be represented by a single, spatially closed dimension. However, assignment of a reference direction for triangles possessing a k-fold rotational symmetry presents a significant challenge. Methods are described for characterizing shape and orientation of triangles, and for mapping these attributes onto a set of linear axes to form a combined index. The shape attribute is independent of size, orientation, and position, and the characterization of shape and orientation is stable with respect to small variations in the indexed triangles."
7CFEAFC5,International Conference on Data Engineering,stanley b zdonik + ying xing + jeonghyon hwang + ugur cetintemel,2007,"A Cooperative, Self-Configuring High-Availability Solution for Stream Processing",checkpointing + delta checkpointing + collaborative self-configuring high availability approach + availability + patient monitoring + low-latency failure recovery + query processing + fine-grained checkpointing model + high availability + low latency + computer science + stream processing + collaboration + cooperative self-configuring high-availability solution,AuthorProvided Keywords Not Found,"We present a collaborative, self-configuring high availability (HA) approach for stream processing that enables low-latency failure recovery while incurring small run-time overhead. Our approach relies on a novel fine-grained checkpointing model that allows query fragments at each server to be backed up at multiple other servers and recovered collectively (in parallel) when there is a failure. In this paper, we first address the problem of determining the appropriate query fragments at each server. We then discuss, for each fragment, which server to use as its backup as well as the proper checkpoint schedule. We also introduce and analyze operator-specific delta-checkpointing techniques to reduce the overall HA cost. Finally, we quantify the benefits of our approach using results from our prototype implementation and a detailed simulator."
7DFAC5E6,International Conference on Data Engineering,naveen ashish + rabia nurayturan + sharad mehrotra + zhaoqi chen + dmitri v kalashnikov,2007,Disambiguation Algorithm for People Search on the Web,search engines + web pages + data mining + extraction techniques + information retrieval + web page + world wide web + World Wide Web + information analysis + HTML + entity-relationship graph + machine learning + entity relationship + relational data + computer science + clustering algorithms + disambiguation algorithm + People Search + Web page + html + Web sites + internet + middleware,AuthorProvided Keywords Not Found,"In this paper we develop a disambiguation algorithm and then study its impact on People Search. The proposed algorithm first uses extraction techniques to automatically extract `significant' entities such as the names of other persons, organizations, and locations on each Web page. In addition, it extracts and parses HTML and Web related data on each Web page, such as hyperlinks and email addresses. The algorithm then views all this information in a unified way: as an entity-relationship graph where entities (e.g., people, organizations, locations, Web pages) are interconnected via relationships (e.g., `Web page-mentions-person', relationships derived from hyperlinks, etc). The algorithm gains its power by being able to analyze several types of information: attributes associated with the entities (e.g., TF/IDF for Web pages) and, most importantly, direct and indirect interconnections that exist among entities in the ER graph. We next outline our approach in Section 2 and then compare it with the state of the art solutions in Section 3."
80E075A9,International Conference on Data Engineering,yasushi sakurai + masashi yamamuro + christos faloutsos,2007,Stream Monitoring under the Time Warping Distance,query processing + acceleration + dynamic time warping + pattern matching + sampling methods + data analysis + bio-medical signals + query sequence + dynamic time warping distance + biosensors + network analysis + numerical stream monitoring,AuthorProvided Keywords Not Found,"The goal of this paper is to monitor numerical streams, and to find subsequences that are similar to a given query sequence, under the DTW (dynamic time warping) distance. Applications include word spotting, sensor pattern matching, and monitoring of bio-medical signals (e.g., EKG, ECG), and monitoring of environmental (seismic and volcanic) signals. DTW is a very popular distance measure, permitting accelerations and decelerations, and it has been studied for finite, stored sequence sets. However, in many applications such as network analysis and sensor monitoring, massive amounts of data arrive continuously and it is infeasible to save all the historical data. We propose SPRING, a novel algorithm that can solve the problem. We provide a theoretical analysis and prove that SPRING does not sacrifice accuracy, while it requires constant space and time per time-tick. These are dramatic improvements over the naive method. Our experiments on real and realistic data illustrate that SPRING does indeed detect the qualifying subsequences correctly and that it can offer dramatic improvements in speed over the naive implementation."
7DB8CAAC,International Conference on Data Engineering,seppo sippu + eljas soisalonsoininen + timo lilja + riku saikkonen,2007,Online Bulk Deletion,bulk operation locks + indexing + indexation + transaction rollback + B-tree rebalancing + large database table + online bulk deletion + multigranular key-range locking protocol + database indexing + b tree index + indexes + computer science + concurrency control + B-tree index + tree data structures + protocols + concurrent computing,AuthorProvided Keywords Not Found,"We consider online bulk-delete operations on a large database table organized as a primary (sparse) B+-tree index on a multi-attribute key. Using the natural range partitions induced by prefixes of the key, we define a multi-granular key-range locking protocol in which a bulk operation locks a small number of logical fragments of the table covering the target of the operation. We also present an efficient and recoverable bulk-delete algorithm that minimizes the work needed in B-tree rebalancing and in transaction rollback. All the locks needed for a bulk-delete operation are acquired during a scan of the leaf pages covering the target key range; in this scan the records qualifying for deletion are only marked as deleted. The records are physically deleted in a rebalance phase that avoids visiting subtrees in which all records qualify for deletion, thus saving considerably on the number of rebalancing operations."
8052A6B9,International Conference on Data Engineering,weishinn ku + roger zimmermann + haixun wang,2007,Location-based Spatial Queries with Data Sharing in Wireless Broadcast Environments,wireless broadcasting + high-latency database access + visual databases + mobile technology + privacy + location-based spatial queries + scalability + wireless communication + broadcasting + wireless broadcast environments + query processing + mobile computing + spatial query processing + mobile technologies + data sharing,AuthorProvided Keywords Not Found,"Location-based spatial queries (LBSQs) refer to spatial queries whose answers rely on the location of the inquirer. Efficient processing of LBSQs is of critical importance with the ever-increasing deployment and use of mobile technologies. We show that LBSQs have certain unique characteristics that traditional spatial query processing in centralized databases does not address. For example, a significant challenge is presented by wireless broadcasting environments, which often exhibit high-latency database access. In this paper, we present a novel query processing technique that, while maintaining high scalability and accuracy, manages to reduce the latency considerably in answering location-based spatial queries. Our approach is based on peer-to-peer sharing, which enables us to process queries without delay at a mobile host by using query results cached in its neighboring mobile peers. We illustrate the appeal of our technique through extensive simulation results."
802EC27A,International Conference on Data Engineering,anastasia ailamaki + john k schindler + stratos papadomanolakis + steven w schlosser + gregory r ganger + minglong shao,2007,MultiMap: Preserving disk locality for multidimensional datasets,storage system + multidimensional datasets + prototypes + bandwidth + data mining + multidimensional systems + relational databases + MultiMap + disk locality preservation + distributed databases + spatial locality + hilbert space + full streaming bandwidth + revealing disk-specific details + multimap + database systems,AuthorProvided Keywords Not Found,"MultiMap is an algorithm for mapping multidimensional datasets so as to preserve the data's spatial locality on disks. Without revealing disk-specific details to applications, MultiMap exploits modern disk characteristics to provide full streaming bandwidth for one (primary) dimension and maximally efficient non-sequential access (i.e., minimal seek and no rotational latency) for the other dimensions. This is in contrast to existing approaches, which either severely penalize non-primary dimensions or fail to provide full streaming bandwidth for any dimension. Experimental evaluation of a prototype implementation demonstrates MultiMap's superior performance for range and beam queries. On average, MultiMap reduces total I/O time by over 50% when compared to traditional linearized layouts and by over 30% when compared to space-filling curve approaches such as Z-ordering and Hilbert curves. For scans of the primary dimension, MultiMap and traditional linearized layouts provide almost two orders of magnitude higher throughput than space-filling curve approaches."
7E71B881,International Conference on Data Engineering,rafael berlanga + j m perez + torben bach pedersen + maria jose aramburu,2007,R-Cubes: OLAP Cubes Contextualized with Documents,data warehouse + data model + text analysis + prototypes + data mining + XML documents + information retrieval + xml document + world wide web + World Wide Web + information analysis + algebra + R-Cubes + data models + structured data + OLAP cubes + xml + XML + data structures + xml documents + Internet + data warehouses + internet,AuthorProvided Keywords Not Found,"Current data warehouse and OLAP (Kimball and Ross, 2002) technologies can be efficiently applied to analyze the huge amounts of structured data that companies produce. These organizations also produce many text documents and use the Web as their largest source of external information. Although these documents include highly valuable information that should also be exploited by companies, they cannot be analyzed by current OLAP technologies because they are unstructured and mainly contain text. The current trend is to find these documents available in XML-like formats. Our proposal is to build XML document warehouses that can be used by companies to store unstructured information coming from their internal and external sources. In (Perez et al., 2005) we proposed an architecture for the integration of a corporate warehouse of structured data with a warehouse of text-rich XML documents. We call the resulting warehouse a contextualized warehouse. Since the XML document warehouse may contain documents about many different topics, we apply well-known information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999) techniques to select the context of analysis from the document warehouse. First, the user specifies an analysis context by supplying a sequence of keywords (e.g., an IR condition like ""financial crisis""). Then, the analysis is performed on a so-called R-cube (Relevance cube), which is materialized by retrieving the documents and facts related to the selected context. Each fact in the R-cube will be linked to the set of documents that describe its context, and will have assigned a numerical value representing its relevance with respect to the specified context (e.g., how important the fact is for a ""financial crisis""). In (Perez et al., 2005) we provided R-cubes with a data model and an algebra. This paper presents a prototype R-cube system, and explains how to use it."
7E9E9199,International Conference on Data Engineering,s sudarshan + tapas dutta + swarat chaudhuri,2007,Fine Grained Authorization Through Predicated Grants,col + databases + network security + data security + predicated grants + human resource management + sql + SQL + authorization + fine grained authorization + function/procedure execution + authorisation + authorization grants + access control + cell-level authorization + grant option,AuthorProvided Keywords Not Found,"Authorization in SQL is currently at the level of tables or columns. Many applications need a finer level of control. We propose a model for fine-grained authorization based on adding predicates to authorization grants. Our model supports predicated authorization to specific columns, cell-level authorization with nullification, authorization for function/procedure execution, and grants with grant option. Our model also incorporates other novel features, such as query defined user groups, and authorization groups, which are designed to simplify administration of authorizations. Our model is designed to be a strict generalization of the current SQL authorization mechanism."
7F28631B,International Conference on Data Engineering,zografoula vagena + vassilis j tsotras + mirella m moro,2007,RoXSum: Leveraging Data Aggregation and Batch Processing for XML Routing,content-based routing + message passing + RoXSum + prototypes + overlay network + batch processing + message routing + unicast + data aggregation + routing + xml + batch process + XML + batch processing (computers) + data transmission + data communication + XML routing,AuthorProvided Keywords Not Found,"Content-based routing is the primary form of communication within publish/subscribe systems. In those systems data transmission is performed by sophisticated overlay networks of content-based routers, which match data messages against registered subscriptions and forward them based on this matching. Despite their inherent complexities, such systems are expected to deliver information in a timely and scalable fashion. As a result, their successful deployment is a strenuous task. Relevant efforts have so far focused on the construction of the overlay network and the filtering of messages at each broker. However, the efficient transmission of messages has received less attention. In this work, we propose a solution that gracefully handles the transmission task, while providing performance benefits for the matching task as well. Along those lines, we design RoXSum, a message representation scheme that aggregates the routing information from multiple documents in a way that permits subscription matching directly on the aggregated content. Our performance study shows that RoXSum is a viable and effective technique, as it speeds up message routing for more than an order of magnitude."
7DA5463E,International Conference on Data Engineering,david w embley + muhammed almuhammed,2007,Ontology-Based Constraint Recognition for Free-Form Service Requests,insurance + prototypes + automatic formalization + free-form natural-language-like specification + formal specifications + formal specification + free-form service requests + ontology-based semantic-data-modeling + data models + computer science + natural languages + ontologies (artificial intelligence) + ontology-based constraint recognition + statistical analysis + constraint handling + semantic data model + natural language + ontologies + automatic recognition,AuthorProvided Keywords Not Found,"Automatic recognition and formalization of constraints from free-form service requests is a challenging problem. Its resolution would go a long way toward allowing users to make requests using free-form, natural-language-like specifications. In this paper, we address this challenge by offering an ontology-based, semantic-data-modeling approach to recognize constraints in free-form service requests. We encode domain information such as possible constraints and instances within a domain ontology in terms of object sets, relationship sets among these object sets, and operations over values in object sets and relationship sets. Our system recognizes the constraints in a service request by finding the domain ontology that best matches the request and then by using relationships and operations relevant to the request in the matched ontology to generate the service-request constraints. In experiments conducted with our prototype implementation, our system achieved an average of 96% recall and 99% precision."
7FCCA454,International Conference on Data Engineering,mi zhou + man hon wong,2007,Boundary-Based Lower-Bound Functions for Dynamic Time Warping and Their Indexing,databases + dynamic time warping + indexing + testing + indexation + time series + index structure + lower bound + euclidean distance + lower-bound functions + data engineering + computer science + time measurement + time-series data indexing + time series data,AuthorProvided Keywords Not Found,"Lower-bound functions are crucial for indexing time-series data under dynamic time warping (DTW) distance. In this paper, we propose a unified framework to explain the existing lower-bound functions. Based on the framework, we further propose a group of lower-bound functions for DTW and investigate their performances through extensive experiments. Experimental results show that the new lower-bound functions are better than the existing one in most cases. An index structure based on the new lower-bound functions is also implemented."
7DA5587F,International Conference on Data Engineering,g greco + francesco scarcello + luigi granata + lucantonio ghionna,2007,Hypertree Decompositions for Query Optimization,databases + query plans + database management system + design optimization + prototypes + polynomials + PostgreSQL + hybrid optimizer + database management systems + open-source DBMS + query-oriented hypertree decomposition + query processing + query optimization + cost function + polynomial time + input output,AuthorProvided Keywords Not Found,"The database community has investigated many structure-driven methods, which guarantee that large classes of queries may be answered in (input-output) polynomial-time. However, despite their very nice computational properties, these methods are not currently used for practical applications, since they do not care about output variables and aggregate operators, and do not exploit quantitative information on the data. In fact, none of these methods has been implemented inside any available DBMS. This paper aims at filling this gap between theory and practice. First, we define an extension of the notion of hypertree decomposition, which is currently the most powerful structural method. This new version, called query-oriented hypertree decomposition, is a suitable relaxation of hypertree decomposition designed for query optimization, and such that output variables and aggregate operators can be dealt with. Based on this notion, a hybrid optimizer is implemented, which can be used on top of available DBMSs to compute query plans. The prototype is also integrated into the well-known open-source DBMS PostgreSQL. Finally, we validate our proposal with a thorough experimental activity, conducted on PostgreSQL and on a commercial DBMS, which shows that both systems may significantly benefit from using hypertree decompositions for query optimization."
7DCB7EE6,International Conference on Data Engineering,haixun wang + xuemin lin + ada waichee fu + jian pei,2007,Computing Compressed Multidimensional Skyline Cubes Efficiently,compressed skyline cube + space exploration + space skyline object + multidimensional systems + Stellar + skyline computation analysis + scalability + multidimensional skyline cube + subspace skyline computation + database theory + query processing + collaboration + multidimensional subspace skyline + lattices + decisive subspace + skyline group lattice,AuthorProvided Keywords Not Found,"Recently, the skyline computation and analysis have been extended from one single full space to multidimensional subspaces, which can lead to valuable insights in some applications. Particularly, compressed skyline cubes in the form of skyline groups and their decisive subspaces provide a succinct summarization and compression of multidimensional subspace skylines. However, computing skyline cubes remains a challenging task since the existing methods have to search an exponential number of nonempty subspaces for subspace skylines. In this paper, we propose a novel and efficient method, Stellar, which exploits an interesting skyline group lattice on a small subset of objects which are in the skyline of the full space. We show that this skyline group lattice is easy to compute and can be extended to the skyline group lattice on all objects. After computing the skyline in the full space, Stellar only needs to enumerate skyline groups and their decisive subspaces using the full space skyline objects. Avoiding searching for skylines in an exponential number of subspaces improves the efficiency and the scalability of subspace skyline computation substantially in practice. An extensive performance study verifies the merits of our new method."
80186888,International Conference on Data Engineering,anastasia ailamaki + bruce m maggs + phillip b gibbons + todd c mowry + haifeng yu + christopher olston + anthony tomasic + charles garrod + amit manjhi,2007,Invalidation Clues for Database Scalability Services,national security + databases + prototypes + data security + cryptography + database management systems + data-intensive Web application + scalability + web server + database scalability services + invalidation clues + security of data + data privacy + service oriented architecture,AuthorProvided Keywords Not Found,"For their scalability needs, data-intensive Web applications can use a database scalability service (DBSS), which caches applications' query results and answers queries on their behalf. One way for applications to address their security/privacy concerns when using a DBSS is to encrypt all data that passes through the DBSS. Doing so, however, causes the DBSS to invalidate large regions of its cache when data updates occur. To invalidate more precisely, the DBSS needs help in order to know which results to invalidate; such help inevitably reveals some properties about the data. In this paper, we present invalidation clues, a general technique that enables applications to reveal little data to the DBSS, yet limit the number of unnecessary invalidations. Compared with previous approaches, invalidation clues provide applications significantly improved tradeoffs between security/privacy and scalability. Our experiments using three Web application benchmarks, on a prototype DBSS we have built, confirm that invalidation clues are indeed a low-overhead, effective, and general technique for applications to balance their privacy and scalability needs."
7DABF1D3,International Conference on Data Engineering,gang luo + philip s yu + kunlung wu,2007,SAO: A Stream Index for Answering Linear Optimization Queries,databases + prototypes + indexation + information retrieval + linear optimization + linear programming + history + sliding window + data stream + query processing + linear optimization queries answering + indexes + predictive models + linear approximation + stream approximate onion-like structure + petroleum,AuthorProvided Keywords Not Found,"Linear optimization queries retrieve the top-K tuples in a sliding window of a data stream that maximize/minimize the linearly weighted sums of certain attribute values. To efficiently answer such queries against a large relation, an onion index was previously proposed to properly organize all the tuples in the relation. However, such an onion index does not work in a streaming environment due to fast tuple arrival rate and limited memory. In this paper, we propose a SAO index to approximately answer arbitrary linear optimization queries against a data stream. It uses a small amount of memory to efficiently keep track of the most ""important"" tuples in a sliding window of a data stream. The index maintenance cost is small because the great majority of the incoming tuples do not cause any changes to the index and are quickly discarded. At any time, for any linear optimization query, we can retrieve from the SAO index the approximate top-K tuples in the sliding window almost instantly. The larger the amount of available memory, the better the quality of the answers is. More importantly, for a given amount of memory, the quality of the answers can be further improved by dynamically allocating a larger portion of the memory to the outer layers of the SAO index. We evaluate the effectiveness of this SAO index through a prototype implementation."
7DD3DBF5,International Conference on Data Engineering,haoliang jiang + haixun wang + philip s yu + shuigeng zhou,2007,GString: A Novel Approach for Efficient Search in Graph Databases,graph theory + information retrieval + xml document + GString + protein structure + query processing + subsequence matching + xml + proteins + graph structures + graph database search + filtering + graph search problem + search problems,AuthorProvided Keywords Not Found,"Graphs are widely used for modeling complicated data, including chemical compounds, protein interactions, XML documents, and multimedia. Information retrieval against such data can be formulated as a graph search problem, and finding an efficient solution to the problem is essential for many applications. A popular approach is to represent both graphs and queries on graphs by sequences, thus converting graph search to subsequence matching. State-of-the-art sequencing methods work at the finest granularity - each node (or edge) in the graph will appear as an element in the resulting sequence. Clearly, such methods are not semantic conscious, and the resulting sequences are not only bulky but also prone to complexities arising from graph isomorphism and other problems in searching. In this paper, we introduce a novel sequencing method to capture the semantics of the underlying graph data. We find meaningful components in graph structures and use them as the most basic units in sequencing. It not only reduces the size of resulting sequences, but also enables semantic-based searching. In this paper, we base our approach on chemical compound databases, although it can be applied to searching other complicated graphs, such as protein structures. Experiments demonstrate that our approach outperforms state-of-the-art graph search methods."
7DC0CC2C,International Conference on Data Engineering,brynley p hull + steve madden + i balakrishnan + yang zhang,2007,ICEDB: Intermittently-Connected Continuous Query Processing,wireless sensor networks + bandwidth + intermittently-connected continuous query processing + wireless sensor network + mobile sensor networks + intermittently connected embedded database + displays + data collection + ICEDB + image sensors + query processing + mobile computing + stream processing + delay-tolerant continuous query processor + distributed databases,AuthorProvided Keywords Not Found,"Current distributed database and stream processing systems assume that the network connecting nodes in the data processor is ""always on,"" and that the absence of a network connection is a fault that needs to be masked to avoid failure. Several emerging wireless sensor network applications must cope with a combination of node mobility (e.g., sensors on moving cars) and high data rates ('media-rich sensors capturing videos, images, sounds, etc.). Due to their mobility, these sensor networks display intermittent and variable network connectivity, and often have to deliver large quantities of data relative to the bandwidth available during periods of connectivity. This paper describes ICEDB (Intermittently Connected Embedded Database), a continuous query processing system for intermittently connected mobile sensor networks. ICEDB incorporates two key ideas: (1) a delay-tolerant continuous query processor, coordinated by a central server and distributed, across the mobile nodes, and, (2) algorithms for prioritizing certain query results to improve application-defined ""utility"" metrics. We describe the results of several experiments that use data collected from a small deployed network of six cars driving in and around Boston and Seattle."
75653084,International Conference on Data Engineering,christian a lang + bishwarajan bhattacharjee + timothy r malkemus + sriram padmanabhan + kawi wong,2007,Increasing Buffer-Locality for Multiple Relational Table Scans through Grouping and Throttling,DBMS architecture + database management system + buffer storage + ad-hoc queries + prototypes + bandwidth + decision support workloads + buffer-locality + decision support + database management systems + relational table scans + decision support systems + query processing + memory management + dynamic grouping + silicon + technology management + adaptive throttling + dynamic regrouping + DB2 UDB prototype + database systems,AuthorProvided Keywords Not Found,"Decision support (DSS) workloads generally contain multiple large concurrent scan operations. These are often executed as relational table scans which can take up a lot of I/O bandwidth. This is especially true for ad-hoc queries where the workload is not known in advance. Common database management systems have only limited ability to reuse memory buffer content across multiple running queries due to their treatment of queries in isolation. Previous attempts to coordinate scans for better buffer reuse were less than satisfactory due to drifting between scans and the required radical DBMS architecture changes. In this paper, we describe a new mechanism to keep similar table scans closer together during scanning. This is achieved via dynamic grouping and regrouping of scans based on their runtime behavior and via adaptive throttling of scan speeds based on scan group characteristics. The required memory footprint is very small and the effort required to extend existing database management systems is minimal, as shown in our DB2 UDB prototype. Our experiments show significant gains in end-to-end response times as well as average response times for TPC-H workloads."
7B389196,International Conference on Data Engineering,panagiotis karras + nikos mamoulis,2007,The Haar+ Tree: A Refined Synopsis Data Structure,wavelet transforms + data structure +  tree + Haar transforms + Haar + search space + tree data structures + tree searching + refined synopsis data structure +  synopsis construction + search space delimitation capacity + Haar wavelet technique,AuthorProvided Keywords Not Found,"We introduce the Haar+ tree: a refined, wavelet-inspired data structure for synopsis construction. The advantages of this structure are twofold: First, it achieves higher synopsis quality at the task of summarizing data sets with sharp discontinuities than state-of-the-art histogram and Haar wavelet techniques. Second, thanks to its search space delimitation capacity, Haar+ synopsis construction operates in time linear to the size of the data set for any monotonic distributive error metric. Through experimentation, we demonstrate the superiority of Haar+ synopses over histogram and Haar wavelet methods in both construction time and achieved quality for representative error metrics."
7E5F3D57,International Conference on Data Engineering,shuilung chuang + kevin chenchuan chang + chengxiang zhai,2007,Collaborative Wrapping: A Turbo Framework for Web Data Extraction,concrete + art + multicode decoding scheme + data mining + collaborative wrapping + online databases + textual HTML + data integrity + content-based synchronization + relational databases + query responses + turbo codes + query processing + computer science + domain-based integration + Web data extraction + html + information theory + Internet + turbo code + internet + turbo wrapper + hypermedia markup languages,AuthorProvided Keywords Not Found,"To access data sources on the Web, a crucial step is wrapping, which translates query responses, rendered in textual HTML, back into their relational form. Traditionally, this problem has been addressed with syntax-based approaches for a single source. However, as online databases multiply, we often need to wrap multiple sources, in particular for domain-based integration. Observing that sources in the same domain usually share common fields, we propose a novel wrapping concept - collaborative wrapping - where multiple sources are extracted concurrently with content-based synchronization to produce consentaneous extractions. Toward this concept, recognizing wrapping as a communication process, we develop the turbo wrapper, upon the insight of turbo codes - a multi-code decoding scheme in information theory. Our experiment shows that the turbo wrapper consistently outperforms baseline single-source methods, is robust, and does benefit from extended scales of source collaboration."
808479F9,International Conference on Data Engineering,nick koudas + arunprasad p marathe + divesh srivastava,2007,Propagating Updates in SPIDER,information processing + databases + indexing + prototypes + indexation + query performance + degradation + database tables + customer relationship management + SPIDER + query processing + pressing + string tokens + indexes + very large databases + answer accuracy + string matching + large databases,AuthorProvided Keywords Not Found,"SPIDER, developed at AT&T Labs-Research, is a system that efficiently supports flexible string matching against attribute values in large databases, and is extensively used in AT&T. The scoring methodology is based on tf.idf weighting and cosine similarity, and SPIDER maintains indexes containing string tokens and their weights, for fast matching at query time. Given the ""global"" nature of the weights maintained in the indexes, even a few updates to the underlying database tables would necessitate a (near-complete recomputation of the indexes, which can be prohibitively expensive. In this paper, we explore novel techniques to considerably reduce the cost of propagating updates in SPIDER, without a significant degradation of answer accuracy or query performance. We present experimental evidence using real data sets to demonstrate the practical benefits of our techniques."
7C65DCF5,International Conference on Data Engineering,ayse bener + evren ayorak,2007,Super Peer Web Service Discovery Architecture,network protocol + web services + network flooding + owl + data mining + web service + scalability + P2P networks + self-clustering network + content-addressable network + content addressable network + fault-tolerant distributed hash table + owl ontology + protocols + ontologies + super-peers communication + peer-to-peer computing + super-peer network protocol + robustness + super peer Web service discovery architecture + Web services + transport protocols + distributed hash table + ontologies (artificial intelligence) + service oriented architecture + OWL ontology,AuthorProvided Keywords Not Found,"Web service discovery is currently performed with centralized registries such as UDDI. In this paper, we propose a super-peer network protocol to combine the efficiency of a centralized protocols and P2P networks. For avoiding a flooding the network with search request and for minimizing the number of messages routed in the network, we represented content-addressable network (CAN) structure, which provides a scalable, fault-tolerant distributed hash table (DHT), for super-peers communication. Web service definitions implemented semantically as OWL ontology. The proposed architecture offers self-maintaining and self-clustering network where the peer groups classify the Web service definitions and each peer-group becomes the owner of a classification dynamically."
7E263A96,International Conference on Data Engineering,ying zhang + qing zhang + yidong yuan + xuemin lin,2007,Selecting Stars: The k Most Representative Skyline Operator,approximation ratio + application software + databases + polynomials + probabilistic counting technique + multicriteria decision making + index-based randomized algorithm + indexation + dynamic programming + representative skyline operator + randomized algorithm + approximation algorithms + skyline computation + np hard problem + polynomial time algorithm + randomised algorithms + exact algorithm + NP-hard problem + computer science + skyline point + computer applications + computational complexity,AuthorProvided Keywords Not Found,"Skyline computation has many applications including multi-criteria decision making. In this paper, we study the problem of selecting k skyline points so that the number of points, which are dominated by at least one of these k skyline points, is maximized. We first present an efficient dynamic programming based exact algorithm in a 2d-space. Then, we show that the problem is NP-hard when the dimensionality is 3 or more and it can be approximately solved by a polynomial time algorithm with the guaranteed approximation ratio 1-1/e. To speed-up the computation, an efficient, scalable, index-based randomized algorithm is developed by applying the FM probabilistic counting technique. A comprehensive performance evaluation demonstrates that our randomized technique is very efficient, highly accurate, and scalable."
75AECAF1,International Conference on Data Engineering,karl aberer + anwitaman datta + sarunas girdzijauskas,2007,Oscar: A Data-Oriented Overlay For Heterogeneous Environments,distributed computing + availability + heterogeneity + software measurement + application software + overlay network + usability + bandwidth + routing + internet,,"Quite a few data-oriented overlay networks have been designed in recent years. These designs often (implicitly) assume various homogeneity which seriously limit their usability in real world. In this paper we present some performance results of the Oscar overlay, which simultaneously deals with heterogeneity as observed in the internet (capacity of computers, bandwidth) as well as non-uniformity observed in data-oriented applications."
809E0C25,International Conference on Data Engineering,dengfeng gao + wensyan li + haifeng jiang,2007,Exploiting Correlation and Parallelism of Materialized-View Recommendation for Distributed Data Warehouses,data warehouse + databases + data loading complexity + scalability issues + web services + materialized-view recommendation + load distribution + business intelligence + scalability + data models + materialized query tables + distributed databases + distributed data warehouses + materialized views + cost function + scheduling + iron + data warehouses + job scheduling + bismuth + statistics,AuthorProvided Keywords Not Found,"Many large enterprises require access to distributed data warehouses for business intelligence (BI) applications. Typically distributed data warehouses are integrated into a centralized data warehouse for the benefit of easy maintenance. However, this approach needs to overcome the complexity of data loading and job scheduling as well as scalability issues. On the other hand, the approach of a fully federated system may not be feasible for data intensive BI applications. The hybrid approach via intelligent data placement is more flexible and applicable than the centralized or full-federation configuration. The current implementation of the hybrid approach to integrating distributed data warehouses is to aggregate selected data from various remote sources as materialized views and cache them at the federation server to improve the performance of complex BI query workloads. In this paper, we propose an improvement that recommends materialized query tables (MQTs) for backend servers for the benefits of load distribution and easy maintenance of aggregated data in conjunction with the current hybrid approach of data placement. Our approach considers the correlation between backend servers and recommends MQTs that are well coordinated among the backend servers and optimized for a given workload. We also exploit the parallelism property among the backend servers to make our approach run almost linearly (in contrast to exponentially) with respect to the number of backend servers, without sacrificing its recommendation quality. Experimental evaluations validate the effectiveness and efficiency of our approach."
7D4606A3,International Conference on Data Engineering,kelvin sim + limsoon wong + jinyan li + guimei liu,2007,Distance Based Subspace Clustering with Flexible Dimension Partitioning,kelvin + merging + distance based subspace clustering + pattern clustering + clustering algorithms + data mining + nCluster + flexible dimension partitioning + database theory,AuthorProvided Keywords Not Found,"Traditional similarity or distance measurements usually become meaningless when the dimensions of the datasets increase, which has detrimental effects on clustering performance. In this paper, we propose a distance-based subspace clustering model, called nCluster, to find groups of objects that have similar values on subsets of dimensions. Instead of using a grid based approach to partition the data space into non-overlapping rectangle cells as in the density based subspace clustering algorithms, the nCluster model uses a more flexible method to partition the dimensions to preserve meaningful and significant clusters. We develop an efficient algorithm to mine only maximal nClusters. A set of experiments are conducted to show the efficiency of the proposed algorithm and the effectiveness of the new model in preserving significant clusters."
7D2A0BF3,International Conference on Data Engineering,david martin + joseph y halpern + johannes gehrke + daniel kifer + ashwin machanavajjhala,2007,Worst-Case Background Knowledge for Privacy-Preserving Data Publishing,data publishing privacy + security of data + polynomials + time measurement + data mining + data privacy + publishing + polynomial time algorithm + privacy-preserving data publishing + worst-case background knowledge + frequency + sensitive information disclosure,AuthorProvided Keywords Not Found,"Recent work has shown the necessity of considering an attacker's background knowledge when reasoning about privacy in data publishing. However, in practice, the data publisher does not know what background knowledge the attacker possesses. Thus, it is important to consider the worst-case. In this paper, we initiate a formal study of worst-case background knowledge. We propose a language that can express any background knowledge about the data. We provide a polynomial time algorithm to measure the amount of disclosure of sensitive information in the worst case, given that the attacker has at most k pieces of information in this language. We also provide a method to efficiently sanitize the data so that the amount of disclosure in the worst case is less than a specified threshold."
7E012079,International Conference on Data Engineering,ralph krieger + ira assent + thomas seidl,2007,AttentionAttractor: efficient video stream similarity query processing in real time,AttentionAttractor + project management + image segmentation + video retrieval + image mosaic + video streaming + image recognition + real time + video stream similarity query processing,AuthorProvided Keywords Not Found,"In a project, customers are attracted by a video streaming application. A video camera records people passing by, and a monitor shows an alienated version of the setting accordingly. The idea is to replace the image on the video screen by a mosaic of similar images to draw their attention to the location. For successful implementation, several aspects are of key importance: the images chosen in the mosaic should be similar enough for easy recognition, and the result of the alienation should be computed fast enough for display on the screen in real time."
8177C777,International Conference on Data Engineering,stefan schonauer + alvin cheung + karin kailing,2007,"Theseos: A Query Engine for Traceability across Sovereign, Distributed RFID Databases",application software + query interface + radiofrequency identification + supply chains + information retrieval + query engine + user interfaces + history + Theseos + query processing + counterfeit detection + distributed RFID databases + engines + distributed databases + rfid tags + manufacturing + traceability data retrieval,AuthorProvided Keywords Not Found,"The ability to trace the history of individual products, especially their movement through supply and distribution chains, is key to many solutions such as targeted recalls and counterfeit detection. In most traceability applications a number of independent organizations have to work together. EPCglobal has proposed an architecture for a network of RFID databases where each database provides a standardized query interface. That architecture facilitates simple retrieval of traceability data from individual repositories, but it does not support complex traceability queries or cross-organizational query processing. Theseos (R. Agrawal, 2006) provides traceability applications with the ability to execute complex traceability queries that may span multiple RFID databases."
80B39F31,International Conference on Data Engineering,s sudarshan + ravindra guravannavar,2007,Reducing Order Enforcement Cost in Complex Query Plans,data mining + PostgreSQL + query optimizers + decision support + np hard problem + Volcano-style optimizer + sql + SQL + complex query plans + query processing + database system + optimisation + query optimization + NP-hard problem + optimization + sorting + cost function + order enforcement cost reduction + complex decision support queries + database systems,AuthorProvided Keywords Not Found,"Algorithms that exploit sort orders are widely used to implement joins, grouping, duplicate elimination and other set operations. Query optimizers traditionally deal with sort orders by using the notion of interesting orders. The number of interesting orders is unfortunately factorial in the number of participating attributes. Optimizer implementations use heuristics to prune the number of interesting orders, but the quality of the heuristics is unclear. Increasingly complex decision support queries and increasing use of covering indices, which provide multiple alternative sort orders for relations, motivate us to better address the problem of optimization with interesting orders. We show that even a simplified version of the problem is NP-hard and give principled heuristics for choosing interesting orders. We have implemented the proposed techniques in a Volcano-style optimizer, and our performance study shows significant improvements in estimated cost. We also executed our plans on a widely used commercial database system, and on PostgreSQL, and found that actual execution times for our plans were significantly better than for plans generated by those systems in several cases."
800EEB63,International Conference on Data Engineering,hans wegener,2007,Information System Construction and Maintenance Based on Business Terminology: Experiences from the Reinsurance Industry,data model + Swiss Re Data Language + data quality management + business terminology + information system reuse + terminology + specification languages + information systems + insurance data processing + reference data + physical data model + information system + reinsurance industry + information system maintenance + information analysis + management information systems + history + logical data model + software maintenance + construction industry + data models + information system construction + quality management + software reusability + organizational embedding,AuthorProvided Keywords Not Found,"During the construction phase of an information system, business concerns are expressed in the form of a logical data model. From that the physical data model is designed, taking into account IT concerns as well. Systematically managed business terminology, used as the basis for formulating these models, has been claimed to ensure alignment between the business and IT perspective. We are looking back on six years of experience with the Swiss Re Data Language, our standard for information system construction, reuse, and maintenance with respect to master data. Focusing on the needs of analytical information systems, we report about our analysis methodology (which allows for the systematic alignment of business and IT), the organizational embedding (which ensures systematic reuse of reference data), and the practical use of business terminology in data quality management."
7D818646,International Conference on Data Engineering,wei zhuang + s muthukrishnan + graham cormode,2007,Conquering the Divide: Continuous Clustering of Distributed Data Streams,algorithm design and analysis + clustering algorithms + k-center clustering + high performance computing + data acquisition + data handling + distributed computing + distributed data stream continuous clustering + distributed network,AuthorProvided Keywords Not Found,"Data is often collected over a distributed network, but in many cases, is so voluminous that it is impractical and undesirable to collect it in a central location. Instead, we must perform distributed computations over the data, guaranteeing high quality answers even as new data arrives. In this paper, we formalize and study the problem of maintaining a clustering of such distributed data that is continuously evolving. In particular, our goal is to minimize the communication and computational cost, still providing guaranteed accuracy of the clustering. We focus on the k-center clustering, and provide a suite of algorithms that vary based on which centralized algorithm they derive from, and whether they maintain a single global clustering or many local clusterings that can be merged together. We show that these algorithms can be designed to give accuracy guarantees that are close to the best possible even in the centralized case. In our experiments, we see clear trends among these algorithms, showing that the choice of algorithm is crucial, and that we can achieve a clustering that is as good as the best centralized clustering, with only a small fraction of the communication required to collect all the data in a single location."
7DF6D6DE,International Conference on Data Engineering,ahmed metwally + a el abbadi + dharma p agrawal + nagender bandi,2007,TCAM-conscious Algorithms for Data Streams,application software + data streaming + continuous summarization queries answering + network processing units + central processing unit + stream summarization + network architecture + associative memory + computer science + computer architecture + data structures + ternary content addressable memories + hardware + content-addressable storage + associative memories,AuthorProvided Keywords Not Found,"There has been significant interest in developing space and time efficient solutions for answering continuous summarization queries over data streams. While these techniques are evaluated in a standard CPU setting, many of their applications such as click-fraud detection, and network-traffic summarization typically execute on special networking architectures called network processing units (NPUs). These NPUs interface with special kind of associative memories known as the ternary content addressable memories (TCAMs). In this paper, we describe how the integrated architecture of NPU and TCAMs can be exploited towards achieving the goal of developing high-speed stream summarization solutions. We analyze popular solutions for the frequent elements problem in data stream, discuss the bottleneck issues and motivate how TCAMs can help alleviate these bottlenecks. A preliminary evaluation on an NPU platform reveals the performance gains of the TCAM-conscious techniques over software implementations."
803A93D2,International Conference on Data Engineering,christian koch + lyublena antova + dan olteanu,2007,10106Worlds and Beyond: Efficient Representation and Processing of Incomplete Information,information management + relation algebra + incomplete information + data security + databases + algebra + information security + relational algebra + data structures,,"We present a decomposition-based approach to managing probabilistic information. We introduce world-set decompositions (WSDs), a space-efficient and complete representation system for finite sets of worlds. We study the problem of efficiently evaluating relational algebra queries on world-sets represented by WSDs. We also evaluate our technique experimentally in a large census data scenario and show that it is both scalable and efficient."
803DA9FA,International Conference on Data Engineering,panagiotis g ipeirotis + kenneth wood + wisam dakka,2007,Faceted Browsing over Large Databases of Text-Annotated Objects,visualization + faceted browsing + user interfaces + relational database + multifaceted interfaces + relational databases + text-annotated objects + prefetching techniques + query processing + storage management + face + very large databases + rapid serial visual presentation + data visualisation + large databases + ranking schemes,AuthorProvided Keywords Not Found,"We demonstrate a fully working system for multifaceted browsing over large collections of text-annotated data, such as annotated images, that are stored in relational databases. Typically, such databases can be browsed across multiple facets (by topic, genre, location, and so on) and previous user studies showed that multifaceted interfaces improve substantially the ability of users to identify items of interest in the database. We demonstrate a scalable system that automatically generates multifaceted browsing hierarchies on top of a relational database that stores the underlying text-annotated objects. Our system supports a wide range of ranking alternatives for selecting and displaying the best facets and the best portions of the generated hierarchies, to facilitate browsing. We combine our ranking schemes with Rapid Serial Visual Presentation (RSVP), an advanced visualization technique, which further enhances the browsing experience and demonstrate how to use prefetching techniques to overcome the latency issues that are inherent when browsing the contents of a relational database using multifaceted interfaces."
80A40937,International Conference on Data Engineering,nacera bennacer + lobna karoui + marieaude aufaure,2007,Analyses and Fundamental ideas for a Relation Extraction Approach,text analysis + context modeling + lexical analysis + ontologies + web mining + statistical analysis + intelligent systems + relation extraction + knowledge management + data mining + html,,"Relation extraction is a difficult open research problem with important applications in several fields such as knowledge management, web mining, ontology building, intelligent systems, etc. In our research, we focus on extracting relations among the ontological concepts in order to build a domain ontology. In this paper, firstly, we answer some crucial questions related to the text analyses, the word features and the various relation types. Secondly, we use this theoretical analysis and some issues to define the fundamental ideas of our new approach. Our objective is to extract multi-type relations from the text analyses and the existent relations (in the concept hierarchy). Our approach combines a verb centered method, lexical analyses, syntactic and statistic ones. It is based on an exclusive interest to the document style during the statistic process, a rich contextual modelling that strengthens the term cooccurrence selection, a lexical analysis, a use of the existent relations in the concept hierarchy and a stepping between the various extracted relations to facilitate the evaluation made by the domain experts. Thirdly, we present an illustrative example to explain the previous ideas. © 2007 IEEE."
7D4FEFBE,International Conference on Data Engineering,paul bird + randy horman + wendy powley + baoning niu + patrick martin,2007,Poster Session: Adapting Mixed Workloads to Meet SLOs in Autonomic DBMSs,prototypes + database management system + service level + automatic control + service level objectives + cost function + database systems + filtering + resource management + database management systems + control systems + quadratic programming,,Workload adaptation allows an autonomic database management system (DBMS) to efficiently make use of its resources and meet its Service Level Objectives (SLOs) by filtering or controlling the workload presented to it. Workload adaptation has been shown to be effective for OLAP and OLTP workloads. We outline a framework of workload adaptation and explain how it can be extended to manage mixed workloads comprised of both OLAP and OLTP queries. Experiments with IBM‘ DB2‘ Universal Database_ are presented that illustrate the effectiveness of our techniques. © 2007 IEEE.
7A019174,International Conference on Data Engineering,stilian stoev + murad s taqqu + mariosmarios hadjieleftheriou + george kollios,2007,"Norm, Point, and Distance Estimation Over Multiple Signals Using Max-Stable Distributions",approximation theory + point estimation + signal processing + computer networks + NonControlled Keywords Not Found + stable distribution + Controlled Keywords Not Found + distributed computing + database theory,AuthorProvided Keywords Not Found,"Consider a set of signals fs : {1, ..., N} _ [0, ..., M] appearing as a stream of tuples (i, fs (i)) in arbitrary order of i and s. We would like to devise one pass approximate algorithms for estimating various functionals on the dominant signal fmax, defined as fmax = {(i, maxs fs (i)), _i}. For example, the ""worst case influence"" which is the F1-norm of the dominant signal (Cormode and Muthukrishnan, 2003), general Fp-norms, and special types of distances between dominant signals. The only known previous work in this setting are the algorithms of Cormode and Muthukrishnan and Pavan and Tirtha-pura (2005) which can only estimate the F1-norm over fmax-No previous work addressed more general norms or distance estimation. In this work, we use a novel sketch, based on the properties of max-stable distributions, for these more general problems. The max-stable sketch is a significant improvement over previous alternatives in terms of simplicity of implementation, space requirements, and insertion cost, while providing similar approximation guarantees. To assert our statements, we also conduct an experimental evaluation using real datasets."
7F1132A9,International Conference on Data Engineering,steven furnell + udo bleimann + klauspeter fischer + woldemar fuhrmann,2007,Security Policy Enforcement in BPEL-Defined Collaborative Business Processes,prototypes + security policy + business communication + information analysis + collaboration + control systems + informatics + information flow + web services + process control + xml + business process + service oriented computing + business process execution language + web service + information security,,"This paper presents an approach to security policy enforcement with collaborative business processes defined using BPEL and deployed across enterprise domain boundaries for execution. The assessment of compliance with security policies at the location where a BPEL script is to be executed is facilitated by re-formulating the security policies with respect to the potential of violation inherent in BPEL The results of an analysis of the security-relevant semantics of BPEL-defined business processes conducted for this purpose indicate the paramount role of information flow analysis in business processes. Based on these results, the paper proposes an XML-based schema for specifying security policies for cross-organisational business processes that allows for automatic checking of BPEL scripts for compliance to these security policies. The paper also introduces a prototype implementation of an automatic compliance check that approves the feasibility of the method for practical application in security policy enforcement. © 2007 IEEE."
7A21E1DD,International Conference on Data Engineering,beng chin ooi + claudia plant + ying yan + christian bohm,2007,Efficiently Processing Continuous k-NN Queries on Data Streams,indexing + data streams + k-nearest neighbor queries + data structure + indexation + packaging + intrusion detection + relational databases + high throughput + continuous k-NN queries + query processing + nearest neighbor + k nearest neighbor + exact k-NN monitoring + data structures + throughput,AuthorProvided Keywords Not Found,"Efficiently processing continuous k-nearest neighbor queries on data streams is important in many application domains, e. g. for network intrusion detection. Usually not all valid data objects from the stream can be kept in main memory. Therefore, most existing solutions are approximative. In this paper, we propose an efficient method for exact k-NN monitoring. Our method is based on three ideas, (1) selecting exactly those objects from the stream which are able to become the nearest neighbor of one or more continuous queries and storing them in a skyline data structure, (2) delaying to process those objects which are not immediately nearest neighbors of any query, and (3) indexing the queries rather than the streaming objects. In an extensive experimental evaluation we demonstrate that our method is applicable on high throughput data streams requiring only very limited storage."
79750AFD,International Conference on Data Engineering,zongxian yin + junghsien chiang,2007,Patterns Discovery on Complex Diagnosis and Biological Data Using Fuzzy Latent Variables,fuzzy set theory + data mining + complex data + fuzzy latent variables + computer science + biology computing + clustering algorithms + distribution model + fuzzy clustering + latent variable + em algorithm + biology + data analysis + biological data + algorithm design and analysis + patterns discovery + medical diagnosis + PLV clustering algorithm + data engineering + fuzzy degrees concept + data type + pattern clustering + expectation maximization + complex diagnosis + possibilitic latent variables,AuthorProvided Keywords Not Found,"This paper proposes a new clustering algorithm referred to as the possibilitic latent variables (PLV) clustering algorithm. This algorithm provides a powerful tool for the analysis of complex data, such as clinical diagnosis and biological expressions data, due to its robustness to various data distributions and its accuracy in establishing appropriate groups from data. The algorithm combines a distribution model and the fuzzy degrees concept. Compared to the expectation-maximization (EM) algorithm, which is a well-known distribution estimating algorithm, the PLV algorithm has the considerable advantage that it can be applied to various data types, i.e. it is not restricted solely to Gaussian data distributions. Additionally, the proposed algorithm has a better performance than the well-known fuzzy clustering algorithm, i.e. the FCM algorithm, where it can address compact regions, other than simply dividing objects into several equal populations. The performance of the proposed algorithm is verified by conducting clustering tasks on the contents of several medical diagnosis and biological expressions datasets."
7F58E243,International Conference on Data Engineering,shahin shayandeh + shahram ghandeharizadeh,2007,Greedy Cache Management Techniques for Mobile Devices,mobile computing + internet telephony + mobile device + base stations + mobile devices + base station + greedy algorithms + bandwidth,,"Mobile devices are configured with one or more wireless cards that provide limited radio-range and unreliable transmission. These energy-constrained devices are configured with a fixed amount of storage. A device may set aside a fraction of its local storage as a cache to minimize use of the network when servicing requests, enhancing metrics such as startup latency and data availability. In this paper, we focus on a repository of continuous media (audio and video) clips and study several greedy cuche management techniques and their cache hit rates. This metric reflects what percentage of requests for clips is serviced when a mobile device is disconnected from the network. The device becomes network detached due to factors such as residing in a densely populated geographical location with over committed network bandwidth or travels to geographical areas with no base station coverage. We investigate repositories of both equisized and variable-sized clips, identifying limitations of the current techniques. Our primary contribution is development of three novel techniques to address these limitations. They are adaptable and provide competitive cache hit rates. One technique, called Dynamic Simple, provides a higher cache hit rate and adapts faster to changing patterns of access to clips when compared with other techniques. •_ 2007 IEEE."
7CEDE8C1,International Conference on Data Engineering,barbara carminati + elena ferrari + andrea perego,2007,Private Relationships in Social Networks,cryptography + personal information + authorisation + web services + social network + data privacy + internet,,"Current social networks implement very simple protection mechanisms, according to which a user can state whether his/her personal data, relationships, and resources should be either public or accessible only by him/herself (or, at most, by users with whom he/she has a direct relationship). This is not enough, in that there is the need of more flexible mechanisms, making a user able to decide which network participants are authorized to access his/her resources and personal information. With this aim, in 121 we have proposed an access control model where authorized users are denoted based on the relationships they participate in. Nonetheless, we believe that this is just a first step towards a more comprehensive privacy framework for social networks. Indeed, besides users resources and personal data, also users relationships may convey sensitive information. For this reason, in this paper we focus on relationship protection, by proposing a strategy exploiting cryptographic techniques to enforce a selective dissemination of information concerning relationships across a social network. © 2007 IEEE."
7D7B1421,International Conference on Data Engineering,eser aygun + zehra cataltepe,2007,An Improvement of Centroid-Based Classification Algorithm for Text Classification,k nearest neighbor + classification algorithms + text analysis + clustering algorithms + testing + euclidean distance + internet + confusion matrix + frequency + nearest neighbor + training data,,"k-nearest neighbor and centroid-based classification algorithms are frequently used in text classification due to their simplicity and performance. While k-nearest neighbor algorithm usually performs well in terms of accuracy, it is slow in recognition phase. Because the distances/similarities between the new data point to be recognized and all the training data need to be computed. On the other hand, centroid-based classification algorithms are very fast, because only as many distance/similarity computations as the number of centroids (i.e. classes) needs to be done. In this paper, we evaluate the performance of centroid-based classification algorithm and compare it to nearest mean and nearest neighbor algorithms on 9 data sets. We propose and evaluate an improvement on centroid-based classification algorithm. Proposed algorithm starts from the centroids of each class and increases the weight of misclassified training data points on the centroid computation until the validation error starts increasing. The weight increase is done based on the training confusion matrix entries for misclassified points. The proposed algorithm results in smaller test error than centroid-based classification algorithm in 7 out of 9 data sets. It is also better than 10-nearest neighbor algorithm in 8 out of 9 data sets. We also evaluate different similarity metrics together with centroid and nearest neighbor algorithms. We find out that, when Euclidean distance is turned into a similarity measure using division as opposed to exponentiation, Euclidean-based similarity can perform almost as good as cosine similarity. •_ 2007 IEEE."
7E663833,International Conference on Data Engineering,jinghai rao + alberto sardinha + norman sadeh,2007,Enforcing Context-Sensitive Policies in Collaborative Business Environments,semantic web + authorisation + web services + software architecture + groupware + computer architecture + access control + computer experiment + service oriented architecture + collaboration + scalability + web service + service oriented architectures,,"As enterprises seek to engage in increasingly rich and agile forms of collaboration, they are turning towards service-oriented architectures that enable them to selectively expose different levels of functionality to both existing and prospective business partners. This Includes enforcing access control policies whose elements are tied to changing contractual relationships or to information obtained from external sources (e.g. ratings, credit worthiness, export restrictions, etc.). To ensure maximum openness, we argue that such sources of contextual information should themselves be represented as web services that can be identified and accessed on the fly, us required to enforce relevant policies. We propose an architecture for enforcing context-sensitive access control policies in which sources of information can be annotated with rich semantic profiles. This includes a meta-control architecture for dynamically orchestrating policy reasoning together with the identification and access of external sources of information required to enforce policies. We show that this architecture can be implemented as an extension to XACMLs PIP and context handler functionality: We proceed to show that our architecture extends to a broader class of corporate and regulatory policies. The paper also presents computational experiments aimed at evaluating the scalability of our architecture. •_ 2007 IEEE."
80B4ECE9,International Conference on Data Engineering,debmalya biswas + iigon kim,2007,Atomicity for P2P based XML Repositories,robustness + xml + xml documents + p2p + database system + fault tolerance + database systems + web services + fault tolerant + service provider + web service + user interfaces + xml document,,"Over the years, the notion of transactions has become synonymous with providing fault-tolerance, reliability and robustness to database systems. However, challenges arise when we try to apply them to novel computing paradigms such as ActiveXML (AXML) systems. AXML provides an elegant platform to integrate the power of XML, Web services and Peer to Peer (P2P) paradigms by allowing (active) Web services calls to be embedded within XML documents. We propose a transactional framework which provides relaxed ACID properties to AXML systems. Relaxed atomicity is usually provided with the help of compensation. However, current compensation based models assume the existence of a pre-defined compensating operation. Also, compensation is assumed to be more or less peer (or service provider) dependent, i.e., the original and compensating services are provided by the same peer. We show how compensation for AXML transactions can be constructed dynamically at run-time and achieved in a peer independent manner. Finally, we consider the issue of peer disconnection, an inherent trait of P2P systems, and propose an innovative solution based on peer _chainingî."
8036740B,International Conference on Data Engineering,nick cook + santosh k shrivastava + p w robinson,2007,The rigorous implementation of a fair exchange protocol for non-repudiable Web service interactions - a case study,security + security protocols + formal specifications + non repudiation + xml + protocols + web service + middleware + authentication + formal specification + web services + collaboration,,"The correct implementation of security protocols is a challenging task. To achieve a high degree of confidence in an implementation, as with any software, ideally one requires both: (i) a formal specification that has been subjected to verification, and (ii) tool support to generate an implementation from the verified specification. The formal specification and verification of security protocols has attracted considerable attention, with corresponding advances. However, the state of the art in the generation of implementations has not progressed beyond relatively simple protocols. This paper presents a case study on the implementation of a deterministically fair non-repudiation protocol. Such protocols are among the most complex of security protocols. Sub-protocols are typically required to guarantee timely termination. A trusted third party must be involved to guarantee fairness. Finally, to satisfy requirements such as non-repudiable audit, significant infrastructure support is needed. The case study demonstrates an improved approach to protocol implementation. Starting with a formal specification, a rigorous process with considerable tool support leads to the deployment of a protocol implementation in a flexible Web services-based execution framework. The paper concludes with an evaluation of the approach. © 2007 IEEE."
7D8A2192,International Conference on Data Engineering,wei yu + jiaheng cao + qing li + junpeng chen,2007,OS_RANK: Structure Analysis for Ontology Ranking,cost effectiveness + information analysis + structure analysis + ontologies + computer science + spine + domain knowledge + search engines + formal specifications + semantic web,,"Ontologies have been shown to be beneficial for representing domain knowledge, and are quickly becoming the backbone of the Semantic Web. The need for ontology ranking techniques is crucial as the ontology reuse becomes increasingly important. In this paper, a new approach has been proposed for ranking ontologies on the Semantic Web. In particular, query terms provided by users are regarded as containing special information about domain knowledge of interest. Each ontology candidates are analyzed separately and ranked with respect to the structure and semantics. Experiments are performed and the results show that our method is cost-effective. •_ 2007 IEEE."
7EB8BEED,International Conference on Data Engineering,peter mayer + alexander stage + christoph schroth + till janner,2007,A Holistic Architecture for Collaborative and Highly Automatized e-Business Platforms,open systems + context modeling + transport protocols + standardization + groupware + collaboration + business process + graphical models + service oriented architecture + electronic commerce,,"In this work, we propose an e-Business architecture that takes into account the specific needs of small and medium-sized enterprises (SMEs) and present a modeldriven, highly flexible and collaborative approach that fosters cross-organizational interoperability. The specifications defined by the standardization body UN/CEFACT (e.g. UN/CEFACT Modeling Methodology, Core Component Technical Specification) and the ehXML standard are central composites of our novel holistic framework that aims at reducing effort required for performing business transactions. The introduction of methods for automatized business process and data negotiation is a further cornerstone of this work. •_ 2007 IEEE."
7F1D22EA,International Conference on Data Engineering,jiawei han + xuehua shen + h gonzalez,2007,Cost-Conscious Cleaning of Massive RFID Data Sets,radio frequency + radiofrequency identification + massive RFID data sets + appropriate technology + tag-reader configuration + RFID system + cost-conscious cleaning + computer science + data cleaning + very large databases + radio frequency interference + cost function + data handling + error correction,AuthorProvided Keywords Not Found,"Efficient and accurate data cleaning is an essential task for the successful deployment of RFID systems. Although important advances have been made in tag detection rates, it is still common to see a large number of lost readings due to radio frequency (RF) interference and tag-reader configurations. Existing cleaning techniques have focused on the development of accurate methods that work well under a wide set of conditions, but have disregarded the very high cost of cleaning in a real application that may have thousands of readers and millions of tags. In this paper, we propose a cleaning framework that takes an RFID data set and a collection of cleaning methods, with associated costs, and induces a cleaning plan that optimizes the overall accuracy-adjusted cleaning costs by determining the conditions under which inexpensive methods are appropriate, and those under which more expensive methods are absolutely necessary."
80FBAAA8,International Conference on Data Engineering,justin donaldson,2007,Music Recommendation Mapping and Interface Based on Structural Network Entropy,recommender system + algorithm design and analysis + entropy + human computer interaction + navigation + informatics + data visualization + recommendation system + music + data mining + recommender systems,,"""Recommendation systems generally produce the results of their output to their users in the form of an ordinal list. In the interest of simplicity, these lists often obscure, abstract, or omit many relevant metrics pertaining to the measured """"strength"""" of the recommendations or the relationships the recommended items share with each other. This information is often useful for coming to a better understanding of the nature of how the items are structured according to the recommendation data. This paper describes the ZMDS algorithm, a novel way of analyzing the fundamental network structure of recommendation results. Furthermore, it also describes a nonlinear repulsion plot method as a utility for mapping and interacting with the results generated by the ZMDS algorithm on music recommendation data. A novel """"recommendation Map"""" web application implements both the ZMDS algorithm and the repulsion plot interface and are offered as an example of both components working together. © 2007 IEEE."""
80A789D7,International Conference on Data Engineering,kerry taylor + paul brebner + m j kearney + dana zhang + vladimir tosic + kelly lam,2007,Towards Declarative Monitoring of Declarative Service Compositions,middleware + service oriented architecture + web services + quality control + information analysis + software engineering + computer science + web service + quality of service,,"One of the great promises of Web Setvice technology is re-use through composition of basic services into new virtual services. To facilitate easier development of such compositions, recent approaches to declarative service composition ask a composition designer to provide a declarative specification of a composition goal, instead of an imperative workflow of composed services. Middleware services use this goal and expressive descriptions of available services to automatically generate an imperative workflow. However, if an error occurs during this process the source of the problem becomes difficult to trace. In this paper, we discuss how an integration of two existing middleware infrastructures, the CSIROs semantic composition architecture and the Web Service Offerings Infrastructure (WSOI), can be used for monitoring of functional correctness and quality of service (QoS) in declarative Web service compositions. The combination supplements the generated workflows with checking of functional correctness, measurement and calculation of QoS metrics, and reporting of discovered problems. In future, It will be extended with correlation and analysis of monitored information to determine probable causes of problems and appropriate corrective actions. This will provide improved transparency and quality control that will be needed in practice. •_ 2007 IEEE."
7DEA0C1C,International Conference on Data Engineering,linhao xu + zhiyong huang + beng chin ooi + hua lu + dan lin,2007,Adapting Relational Database Engine to Accommodate Moving Objects in SpADE,application interfaces + relational database engine + business + information retrieval + visual databases + relational database system + relational database + relational databases + global positioning system + Global Positioning System + query processing + mobile computing + indexes + web + Web + engines + data access + MySQL + JDBC + wireless network + moving object management + spatio-temporal autonomic database engine,AuthorProvided Keywords Not Found,"In this work, we present our implementation for managing moving objects on top of a popular relational database system MySQL, namely SpADE (spatio-temporal autonomic database engine for managing moving objects). In our SpADE system, non-static entities like vehicles and pedestrians are abstracted as moving objects. They obtain positioning information with GPS (Global Positioning System) receivers installed, and are able to communicate via wireless network with the server, sending queries to and receiving results from it. The server is responsible for managing moving object information and processing queries from mobile users. By employing the industry standard JDBC for the data access, our server can also support providing services for other application interfaces such as the Web."
7D165BDA,International Conference on Data Engineering,yi chen + jianchun fan + himal khatri + s kambhampati,2007,QPIAD: Query Processing over Incomplete Autonomous Databases,computer science + information retrieval + query optimization + internet + data engineering + null value + database management systems + acoustical engineering + databases + web server + creep + naive bayes classifiers,,"""Incompleteness due to missing attribute values (aka """"null values"""") is very common in autonomous web databases, on which user accesses are usually supported through mediators. Traditional query processing techniques that focus on the strict soundness of answer tuples often ignore tuples with critical missing attributes, even if they wind up being relevant to a user query. Ideally we would like the mediator to retrieve such relevant uncertain answers and gauge their relevance by accessing their likelihood of being relevant answers to the query. However, the autonomous nature of the databases poses several challenges, such as the restricted access privileges, limited query patterns, and sensitivity of database and network resource consumption in the web environment. We introduce a novel query rewriting and optimization framework QPIAD that tackles these challenges to retrieve relevant uncertain answers. Our technique involves reformulating the user query based on approximate functional dependencies (AFDs) among the database attributes and ranking these queries using value distributions learned from Na‰ve Bayes Classifiers. Empirical studies demonstrate the effectiveness of our approach in retrieving relevant uncertain answers with high precision, high recall and manageable cost. © 2007 IEEE."""
8077371A,International Conference on Data Engineering,gultekin ozsoyoglu + ali cakmak + sulieman baniahmad + nattakarn ratprasartporn + jonathan po,2007,Evaluating Different Ranking Functions for Context-Based Literature Search,citation analysis + digital libraries + score function + digital library + testing + ontologies + semantic properties + intrusion detection + text analysis + search engines,,"Context-based literature digital library search is a new search paradigm that creates an effective ranking of query outputs by controlling query output topic diversity. We define contexts as pre-specified ontology-based terms and locate the paper set of a context based on semantic properties of the context (ontology) term. In order to provide a comparative assessment of papers in a context and effectively rank papers returned as search outputs, prestige scores are attached to all papers with respect to their assigned contexts. In this paper, we present three different prestige score (ranking) functions for the context-based environment, namely, citation-based, text-based, and pattern-based score functions. Using biomedical publications as the lest case and Gene Ontology as the context hierarchy, we have evaluated the proposed ranking functions in terms of their accuracy and separability. We have found that text-based and pattern-based score functions yield better accuracy and separability than citation-based score functions. •_ 2007 IEEE."
7F5CCB1C,International Conference on Data Engineering,christian a lang + i slanoi + bishwaranjan bhattacharjee + timothy ray malkemus,2007,Poster Session: Improved Buffer Size Adaptation through Cache/Controller Coupling,resource management + tuning + adaptive control + feedback loop + database systems + acceleration + statistics + database management systems + database system + availability,,"Database workloads seldom remain static. A system tuned by an expert for the current environment, might not always remain optimal. To deal with this situation, database systems have been incorporating self tuning features. An important component is self tuning memory or bufferpools. These often work on a feedback loop and take some time to converge to an optimal state. The transition to the optimal state could be accelerated if future access patterns could be taken into account when making decisions on the bufferpool size. In this paper, we describe a caching algorithm for scans on bufferpools, which keeps track of ongoing scans and the state of each scan. The proposed algorithm results in a better hit ratio and, more importantly, provides a way to predict future access patterns. This property is beneficial for providing feedback to a self-tuning memory controller to make better allocation decisions. •_ 2007 IEEE."
7EA5D206,International Conference on Data Engineering,oded maimon + sigal elnekave,2007,Incremental Clustering of Mobile Objects,indexation + mobile computing + wireless communication + data mining + global positioning system + mobile computer + gps + meteorology + resource management + clustering algorithms + marketing management + mobile computers + weather forecasting + resource allocation,,"Moving objects are becoming increasingly attractive to the data mining community due to continuous advances in technologies like GPS, mobile computers, and wireless communication devices. Mining spatio-temporal data can benefit many different functions: marketing team managers for identifying the right customers at the right time, cellular companies for optimizing the resources allocation, web site administrators for data allocation matters, animal migration researchers for understanding migration patterns, and meteorology experts for weather forecasting. In this research we use a compact representation of a mobile trajectory and define a new similarity measure between trajectories. We also propose an incremental clustering algorithm for finding evolving groups of similar mobile objects in spatio-temporal data. The algorithm is evaluated empirically by the quality of object clusters (using Dunn and Rand indexes), memory space efficiency, execution times, and scalability (run time vs. number of objects)."
7FDE02B0,International Conference on Data Engineering,antonietta lanza + donato malerba + annalisa appice,2007,An Integrated Platform for Spatial Data Mining within a GIS Environment,association rule + morphology + geographic information systems + spatial analysis + data mining + association rules + thematic maps + topographic map + cartography + topographic maps + data visualization + computer architecture + geographic visualization,,"The strength of GIS is in providing a rich data infrastructure for combining disparate data in meaningful ways by using a spatial arrangement (e.g., proximity). As a toolbox, a GIS allows planners to perform spatial analysis using geo-processing functions such as map overlay, connectivity measurements or thematic map coloring. Although, this makes effective the geographic visualization of individual variables, complex multi-variate dependencies are easily overlooked. The required step to take GIS beyond a toot for automating cartography is to incorporate the ability of analyzing and condensing a large number of geo-referenced variables into a single forecast or score. This is where data mining promises great potential benefits and the reason why there is such a hand-in-glove fit between GIS and data mining. Following the mainstream of this research, we propose to integrate GIS and data mining functionality in a closely coupled open and extensible GIS architecture. This is done by resorting to emerging spatial data mining technology that deals with the substantial complexity added from the spatial dimension. We illustrate an example of topographic map interpretation where resorting to data mining facilities to discover both operational definitions of morphologies characterizing the landscape (i.e., spatial classification rules) and frequent spatial interactions of two or more spatially-referred objects (i.e., spatial association rules), In both cases, discovered patterns correspond to what geographers, geologists and town planners are interested in while interpreting a map, although they are never explicitly represented in topographic maps or in a GIS-model. © 2007 IEEE."
815E9A68,International Conference on Data Engineering,tommi karkkainen + miika nurminen + anne marita honkaranta,2007,ProcMiner: Advancing Process Analysis and Management,technology management + process mining + data mining + xml database + engineering management + publishing + prototypes + xml + text mining + quality system + total quality management + process model + information technology + information analysis,,"This paper contributes both to research and practice on process mining. Previous research on process mining has focused on mining patterns from event log files to generate process models. The process mining approach adopted in this paper is focused on producing patterns about process models, not the models themselves. The approach is demonstrated by ProcMiner - an explorative research prototype for management, consolidating, publishing, retrieving, and analyzing process models. Content-based document clustering is applied to process models represented as XML database in order to find topical groups from models. In practice, organizations face numerous challenges in managing their process models. The models may be heterogeneous or ambiguous. The modeling software may change over time or due to differences in departmental purchases. ProcMiner was used in quality system development initiative at the University of Jyv_skyl_. The findings support previous model engineering research, showing that multiple actions are needed to ensure consistency of process models, and to make them efficiently manageable. © 2007 IEEE."
7F6626FA,International Conference on Data Engineering,aris gkoulalasdivanis + polixeni zacharouli + vassilios s verykios,2007,A k-Anonymity Model for Spatio-Temporal Data,web services + generic algorithm + hazards + databases + location based service + temporal databases + bluetooth + mobile computing + global positioning system + data engineering + location based services + data privacy,,"The unprecedented growth In Location-Based Services (LBS) take-up along with the continuously increasing storage capabilities of modern systems, and have facilitated the collection of information related to users activities in space and time. In itself this fact constitutes a serious hazard to the privacy of individuals. In this paper, we extend existing work in the preservation of historical k-anonymity, by (i) enabling each user to have numerous spatio-temporal movement patterns (a.k.a. LBQIDs) associated with his profile, (ii) adapting the generalization algorithm of k-anonymity to account for those multiple LBQIDs, (iii) defining a set of novel spatial regions which behave as dynamically constructed mix-zones, and (iv) introducing an unlinking algorithm, applied when the generalization algorithm fails, which protects users by unlinking their future requests from previous ones. Moreover, as part of our contribution, we construct a data generator that allows for the composition of spatio-temporal datasets. Finally, we use these datasets to provide an extensive and thorough experimental evaluation of our approach. •_ 2007 IEEE."
815AED17,International Conference on Data Engineering,moustafa a hammad + adesola omotayo + ken barker,2007,A Cost Model for Storing and Retrieving Data in Wireless Sensor Networks,distributed algorithm + memory + intelligent sensors + wireless sensor networks + application software + distributed algorithms + environmental monitoring + data retrieval + data storage + wireless sensor network + data handling + information retrieval,,"Many applications require storing data in Wireless Sensor Networks (WSNs). For example, in environmental monitoring applications, WSN may archive sensor data for retrieval at periodic Intervals. In contrast to conventional network data storage, storing data in WSNs is challenging because of the limited power, memory, and communication bandwidth of WSNs. This paper identifies the critical parameters of WSNs and proposes a cost model for data storage and retrieval. This paper also proposes a distributed algorithm that utilizes the cost model to intelligently distribute excess data from sensor nodes over WSN. The proposed algorithm chooses for remote storage nodes with the most extra memory and the least communication cost. The algorithm also adapts dynamically to changes in the storage requirements of sensor nodes. The benefits of the cost model are experimentally evaluated by using the algorithm in a simulated WSN. Results from the experiments show that as much as 30% more data may be stored in WSN at a slightly higher communication cost when a nodes data is distributed across WSN instead of only being stored locally in the node. •_ 2007 IEEE."
7FDB1306,International Conference on Data Engineering,anja klein + g hackenbroich + honghai do + m kamstedt + wolfgang lehner,2007,Representing Data Quality for Streaming and Static Data,relational databases + automation + production + data models + sensors + intelligent sensors + computer science + data quality + relational database + application software + quality management,,"In smart item environments, multitude of sensors are applied to capture data about product conditions and usage to guide business decisions as well as production automation processes. A big issue in this application area is posed by the restricted quality of sensor data due to limited sensor precision as well as sensor failures and malfunctions. Decisions derived on incorrect or misleading sensor data are likely to be faulty. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In this paper, we present a flexible model for the efficient transfer and management of data quality for streaming as well as static data. We propose a data stream metamodel to allow for the propagation of data quality from the sensors up to the respective business application without a significant overhead of data. Furthermore, we present the extension of the traditional RDBMS metamodel to permit the persistent storage of data quality information in a relational database. Finally, we demonstrate a data quality metadata mapping to close the gap between the streaming environment and the target database. Our solution maintains a flexible number of DQ dimensions and supports applications directly consuming streaming data or processing data filed in a persistent database. •_ 2007 IEEE."
7DECBD3E,International Conference on Data Engineering,a el saddik + m a hossain + pradeep k atrey,2007,Modeling Quality of Information in Multi-sensor Surveillance Systems,quality of information + invasion of privacy + ambient intelligence + q factor + context modeling + sensor fusion,,"Current surveillance systems use multiple sensors and media processing techniques in order to record/detect information of interest in terms of events. Assessing the quality of information (QoI) of a surveillance system is an important task as any misleading information may lead to suspicion, undesired consequences, and unwanted invasion of privacy. In this paper, we propose a model to characterize QoI in multi-sensor surveillance systems in terms of four quality parameters, which are: accuracy, certainty, time-liness and integrity. The proposed model is extendable to include other quality parameters if deemed necessary for different task-specific scenarios, such as the ambient intelligence environment, which aims to provide context-aware personalized services to the poeple living in that environment. To demonstrate the utility of the proposed method, we provide experimental results in a surveillance system designed for identifying authorized entry in the observation area. © 2007 IEEE."
7E2F03A9,International Conference on Data Engineering,salvatore orlando + alessandro roncato + a raffaela + claudio silvestri + fernando j braz + renzo orsini,2007,Approximate Aggregations in Trajectory Data Warehouses,data warehouse + data cube + testing + mobile communication + data warehousing + warehousing + data mining + data warehouses,,"In this papser we discuss how data warehousing technology can be used to store aggregate information about trajectories and perform OLAP operations over them. To this end, we define a data cube with spatial and temporal dimensions, discretized according to a regular grid. We investigate in depth some issues related to the computation of a holistic aggregate function, i.e, the presence, which returns the number of distinct trajectories occurring in a given spatio-temporal area. In particular, we introduce a novel way to compute an approximate, but nevertheless very accurate, presence aggregate function, which uses only a bounded amount of measures stored in the base cells of our cuboid. We also concentrate on the loading phase of our data warehouse, which has to deal with an unbounded stream of trajectory observations. We suggest how the complexity of this phase can be reduced, and we analyse the errors that this procedure induces at the level of the subaggregates stored in the base cells. These errors and the accuracy of our approximate aggregate functions are carefully evaluated by means of tests performed on synthetic trajectory datasets."
7DE5C266,International Conference on Data Engineering,a h van bunningen + peter m g apers + ling feng + maarten m fokkinga,2007,Ranking Query Results using Context-Aware Preferences,information need + probabilistic model + history + information retrieval + probability + relational databases + uncertainty + context modeling + database system + computer science + information technology + information needs + telematics,,"To better serve users Information needs without requiring comprehensive queries from users, a simple yet effective technique is to explore the preferences of users. Since these preferences can differ for each context of the user, we introduce context-aware preferences. To anchor the semantics of context-aware preferences in a traditional probabilistic model of information retrieval, we present a semantics for context-aware preferences based on the history of the user. An advantage of this approach is that the inherent uncertainty of context information, due to the fact that context information is often acquired through sensors, can be easily integrated in the model. To demonstrate the feasibility of our approach and current bottlenecks we provide a naive implementation of our technique based on database views. © 2007 IEEE."
8137B999,International Conference on Data Engineering,h v jagadish + nuwee wiwatwattana + divesh srivastava + laks v s lakshmanan,2007,X^ 3: A Cube Operator for XML OLAP,distributed computing + relational databases + algorithm design and analysis + lattices + tree data structures + data warehouse + xml + data cube + data mining + data warehouses,,"With increasing amounts of data being exchanged and even generated or stored in XML, a natural question is how to perform OLAP on XML data, which can be structurally heterogeneous (e.g., parse trees) and/or marked-up text documents. A core operator for OLAP is the data cube. While the relational cube can be extended in a straightforward way to XML, we argue such an extension would not address the specific issues posed by XML While in a relational warehouse, facts are flat records and dimensions may have hierarchies, in an XML warehouse, both facts and dimensions may be hierarchical. Second, XML is flexible: (a) an element may have, missing or repeated subelements; (b) different instances of the same element type may have different structure. We identify the challenges introduced by these features of XML for cube definition and computation. We propose a definition for cube adapted for XML data warehouse, including a suitably generalized specification mechanism. We define a cube lattice over the aggregates so defined. We then identify properties of this cube lattice that can be leveraged to allow optimized computation of the cube. Finally, we present the results of an extensive performance evaluation experiment gauging the behavior of alternative algorithms for cube computation. © 2007 IEEE."
815E74FC,International Conference on Data Engineering,vahideh sadat sadeghi + khashayar yaghmaie,2007,A Novel Visual Feature Extraction and Its Application in Vowel Recognition,image recognition + feature extraction + clustering algorithms + pixel + environmental noise + tracking + neural nets + speech recognition + neural network + neural networks + voting,,"Speech recognition techniques have been developed dramatically in recent years. Nevertheless, errors caused by environmental noise are still a serious problem in recognition. Employing algorithms to detect and follow the motion of lips have been widely used to improve the performance of speech recognition algorithms. This paper presents a novel technique to recognize vowels. Lip features extracted by using a combined method are used as input parameters to a neural network system for recognition. Accuracy of the proposed method is verified by using it to recognize 6 main Farsi vowels. •_ 2007 IEEE."
80D2EBE5,International Conference on Data Engineering,ivan t bowman + kenneth salem,2007,Semantic Prefetching of Correlated Query Sequences,query processing + request stream + pattern analysis + client-server systems + storage management + client requests + computer science + detectors + information analysis + throughput + correlated query sequences + semantic prefetching,AuthorProvided Keywords Not Found,"We present a system that optimizes sequences of related client requests by combining small requests into larger ones, thus reducing per-request overhead. The system predicts upcoming requests and their parameter values based on past observations, and prefetches results that are expected to be needed. We describe how the system makes its predictions and how it uses them to optimize the request stream. We also characterize the benefits with several experiments."
7E73E2FC,International Conference on Data Engineering,quanzhong li + minglong sha + latha s colby + guy m lohman + volker markl + kevin beyer,2007,Adaptively Reordering Joins during Query Execution,filtering + low latency + database management system + query optimization + adaptive systems + statistical distributions + demography + pipelines + statistics + database management systems + nested loops + cost function + indexation,,"""Traditional query processing techniques based on static query optimization are ineffective in applications where statistics about the data are unavailable at the start of query execution or where the data characteristics are skewed and change dynamically. Several adaptive query processing techniques have been proposed in recent years to overcome the limitations of static query optimizers through either explicit re-optimization of plans during execution or by using a row-routing based approach. In this paper, we present a novel method for processing pipelined join plans that dynamically arranges the join order of both inner and outer-most tables at run-time. We extend the Eddies concept of """"moments of symmetry"""" to reorder indexed nested-loop joins, the join method used by all commercial DBMSs for building pipelined query plans for applications for which low latencies are crucial. Unlike row-routing techniques, our approach achieves adaptability by changing the pipeline itself, which avoids the bookkeeping and routing decision associated with each row. Operator selectivities monitored during query execution are used to change the execution plan at strategic points, and the change of execution plans utilizes a novel and efficient technique for avoiding duplicates in the query results. Our prototype implementation in a commercial DBMS shows a query execution speedup of up to 8 times. •_ 2007 IEEE."""
78DFF5BA,International Conference on Data Engineering,wei wang + jun huan + d w williams,2007,Graph Database Indexing Using Structured Graph Decomposition,pattern matching + hash tables + indexation + data structure + fast isomorphic lookup + directed acyclic graph + database graphs + tree graphs + secondary structure + query processing + table lookup + database indexing + computer science + proteins + similarity queries + subgraph isomorphism queries + structured graph decomposition + data structures + chemicals + adjacency matrices + canonical representation + databases + hash table + indexing + testing + data engineering + directed graphs + code-based canonical representation + graph database indexing,AuthorProvided Keywords Not Found,"We introduce a novel method of indexing graph databases in order to facilitate subgraph isomorphism and similarity queries. The index is comprised of two major data structures. The primary structure is a directed acyclic graph which contains a node for each of the unique, induced subgraphs of the database graphs. The secondary structure is a hash table which cross-indexes each subgraph for fast isomorphic lookup. In order to create a hash key independent of isomorphism, we utilize a code-based canonical representation of adjacency matrices, which we have further refined to improve computation speed. We validate the concept by demonstrating its effectiveness in answering queries for two practical datasets. Our experiments show that for subgraph isomorphism queries, our method outperforms existing methods by more than an order of magnitude."
7F2ABDC1,International Conference on Data Engineering,rajeev agrawal + william i grosky + farshad fotouhi,2007,Ranking Privacy Policy,business + reflection + privacy policy + data privacy,,"Almost all company websites collect some information about the user in some form. The information may be a simple IP address of the host to extensive personal information about the user. It is now a well established procedure for a company to state its privacy policy on their website, due to the prevalent laws of the land or just to establish their credibility. But the stated privacy policy may not be fully understood by website visitors. In this paper, we suggest a mathematical model which will assign a privacy score/rank to a privacy policy, after analyzing the different components of that companys privacy statement. This score can be one criterion to decide whether to continue using a certain website. © 2007 IEEE."
7E354056,International Conference on Data Engineering,adrian m teisanu + mariano p consens + daniel c zilio + calisto zuzarte + sam s lightstone + mokhtar kandil,2007,Poster Session Problem definition for effective workload management,database management systems,,The paper Introduces the problem of designing dynamic workload management ( WM) tools that are aware of the diversity of classes of users and their diverse access patterns. Our approach should be contrasted with the current WM tools and their ability of detecting performance degradations in accordance with the users preference goals. We define the problem and suggest a formal definition that allows the further development of algorithms and architectures that allow the Implementation of effective on-line database tuning strategies. •_ 2007 IEEE.
80AA2F4C,International Conference on Data Engineering,abdulmotaleb elsaddik + gregor v bochmann + khalil elkhatib,2007,A QoS-based Service Composition for Content Adaptation,information technology + self organization + html + transcoding + quality of service + content management + information retrieval + java + hardware + internet,,"Todays Internet suffers from the problem of heterogeneity in client devices, network connectivity, content format, and users preferences. The framework presented in this paper tackles this problem using the approach of service composition to support distributed multimedia applications. The discussed framework for trans-coding multimedia streams uses self-organizing, resilient data distribution algorithms. The framework takes into consideration the profile of communicating devices, network connectivity, exchanged content formats, context description, and available adaptation services to find a chain of adaptation services that could be applied to the content. Part of the framework is a selection algorithm that finds the best sequence of adaptation services that can maximize the users satisfaction with the delivered content. © 2007 IEEE."
7E42FF29,International Conference on Data Engineering,bernhard seeger + m cammed + jeff kramer,2007,Dynamic Metadata Management for Scalable Stream Processing Systems,resource management + computer architecture + publish subscribe + mathematics + database systems + frequency + message passing + stream processing + meta data + middleware + scalability,,"Adaptive query processing is of utmost importance for the scalability of data stream processing systems due to the long-running queries and fluctuating stream characteristics. An essential prerequisite for adaptive runtime components is the presence of suitable metadata capturing the runtime state. As most of the metadata in such a system gets outdated over time, appropriate update mechanisms are required. Dynamic metadata management deals with the dynamic provision and continuous maintenance of metadata. This paper does not only address the issues in dynamic metadata management such as metadata dependencies and metadata update concepts, but also presents a scalable framework to efficiently manage the diversity of dynamic metadata in todays data stream processing systems. The core of our field-tested metadata framework is a publish-subscribe architecture that enables the system to identify and compute only the currently required metadata. This tailored metadata provision is crucial to scalability as maintaining all available metadata at runtime causes significant computational overhead when the number of continuous queries increases."
7DC9CCDD,International Conference on Data Engineering,kien hua + danzhou liu,2007,Support Concurrent Queries in Multiuser CBIR Systems,process design + indexing + sampling methods + image retrieval + concurrent computing + computer science,,"Various techniques have been developed for different query types in content-based image retrieval (CBIR) systems such as sampling queries, constrained sampling queries, multiple constrained sampling queries, k-NN queries, constrained k-NN queries, and multiple localized k-NN queries. In this paper, we propose a generalized query model suitable for expressing queries of different types, and investigate efficient processing techniques for this new framework. We develop new storage and query processing techniques to exploit sequential access and leverage interquery concurrency to share computation. Our experimental results, based on the Corel dataset, indicates that the proposed optimization can significantly reduce average response time in a multiuser environment. © 2007 IEEE."
7F8137EA,International Conference on Data Engineering,gokay burak akkus,2007,Semantic Web Services Composition: A Network Analysis Approach,user requirements + intelligent systems + network analysis + web services + service discovery + software as a service + graph theory + ontologies + intelligent agent + semantic web + software agents + web service + information systems + adaptive systems,,"""""""Software as a service"""" approach has become a reality since efforts of both industry and research focused on service Integration on the web. It would be possible to see the true potential of such a distributed infrastructure when these services can be combined together as parts of a workflow in order to have a common functionality. The collective composition requires adaptive, dynamic systems such that all parties may be subject to unexpected changes. It also requires Intelligent systems that are able to compose services as building blocks to generate a single service that meets user requirements. In this paper we present an agent based system that guides service discovery and service composition by integrating graph theory into the web services domain. © 2007 IEEE."""
7D05AB44,International Conference on Data Engineering,alberto salguero + francisco araque + m a vila + mari angeles aguilar + ramon a carrasco,2007,dmFSQL: a Server for Data Mining,iterative methods + xml + database languages + marketing management + satisfiability + data mining + fuzzy systems + high level languages + sql + delta modulation + structured query language + concrete + database management systems + database systems + direct marketing,,"At present, we have proceeded to extend SQL into a new language called dmFSQL (data mining Fuzzy Structured Query Language) which can be used to solve real problems of Data Mining. In this paper, we present an architecture that permits us to use this language dmFSQL for Oracle© Database. This enables us to evaluate the process of Data Mining at both a theoretical and a practical level. Thus, we use the proposed system to solve a real problem of Direct Marketing Management. We consider that this model satisfies the requirements of Data Mining systems. © 2007 IEEE."
7BAC1E2A,International Conference on Data Engineering,jingren zhou + p a larson,2007,Efficient Maintenance of Materialized Outer-Join Views,databases + warehousing + data warehousing + xml + data warehouses + materialized views + database system,,"Queries containing outer joins are common in data warehousing applications. Materialized outer-join views could greatly speed up many such queries but most database systems do not allow outer joins in materialized views. In part, this is because outer-join views could not previously be maintained efficiently when base tables are updated. In this paper we show how to efficiently maintain general outer-join views, that is, views composed of selection, projection, inner and outer joins. Foreign-key constraints are exploited to reduce maintenance overhead. Experimental results show that maintaining an outer-join view need not be more expensive than maintaining an inner-join view. © 2007 IEEE."
7D6086CC,International Conference on Data Engineering,christian huemer + philipp liegl,2007,A UML Profile for Core Components and their Transformation to XSD,design rules + web service + electronic commerce + web services + data handling + e commerce + electronic data interchange + unified modeling language + xml schema + xml schemas + ebxml + standardization + ontologies + xml,,"In business-to-business e-commerce, traditional electronic data interchange (EDI) approaches such as UN/EDIFACT have been superseded by approaches like web services and ebXML. Nevertheless, a precise and common semantic definition of business documents exchanged is needed. In order to become independent from a transfer syntax, we prefer defining the documents as platform independent models. An approach that follows this idea is the UN/CEFACTs core component standard. Core components are reusable semantic building blocks which can be combined in various ways to create shared libraries of interoperable business documents. In order to use standard UML modeling tools we have developed a UML profile for the core components standard. Furthermore, we adapt the UN/CEFACT naming and design rules for the UML profile in order to derive XML schemas for business document exchanges. The overall approach is demonstrated by using a specific example from the field of e-commerce."
7D318BC1,International Conference on Data Engineering,stratis d viglas + gao cong + wenfei fan + byron choi,2007,Updating Recursive XML Views of Relations,heuristic algorithm + xml + relational data + computational linguistics + logic + relational databases + side effect,,"This paper investigates the view update problem for XML views published from relational data. We consider (possibly) recursively defined XML views, compressed into DAGs and stored in relations. We provide new techniques to efficiently support XML view updates specified in terms of XPath expressions with recursion and complex filters. The interaction between XPath recursion and DAG compression of XML views makes the analysis of XML view updates intriguing. Furthermore, many issues are still open even for relational view updates, and need to be explored. In response to these, we revise the update semantics to accommodate XML side effects based on the semantics of XML views, and present efficient algorithms to translate XML updates to relational view updates. Moreover, we propose a mild condition on SPJ views, and show that under this condition the analysis of deletions on relational views becomes PTIME while the insertion analysis is NP-complete. Finally, we present an experimental study to verify the effectiveness of our techniques. •_ 2007 IEEE."
811B9543,International Conference on Data Engineering,ester m mungure + kenneth sorensen + torben bach pedersen + claus aage jensen,2007,A Data and Query Model for Dynamic Playlist Generation,databases + internet + metric space + music + world wide web + meta data + navigation + digital music + multidimensional systems + data models + data model + computer science + prototypes,,"Motivated by the increasing amount of digital music on the WWW, this paper presents a data and query model for dynamic playlist generation. Queries are continuous, i.e., songs are retrieved one at a time, allowing the user to dynamically influence the retrieval. The model can support arbitrary similarity measures, requiring only the identity property of the metric space to be obeyed. Using the similarity measure, we are able to retrieve songs similar to a given seed song and avoid retrieval of songs similar to disliked songs. Additionally, the model allows combined querying on both music metadata and musical content. The model has been implemented in a prototype system that is able to support a very large number of both songs and simultaneous users. © 2007 IEEE."
7FF9E76F,International Conference on Data Engineering,kouroupetroglou christos + salampasis michail,2007,Adaptive Browsing Shortcuts: Personalising the User Interface of a Specialised Voice Web Browser for Blind People,navigation + ontologies + html + user interface + world wide web + user interfaces + web page + switches + web pages + time measurement + informatics,,"Browsing shortcuts is a mechanism which facilitates blind people to move efficiently to various elements of a web page (e.g. functional elements such as forms, navigational aids etc.), hence operating effectively as an interaction method and a vital counterbalance to low accessibility of web pages. Results of a quantitative analysis which measured navigation performance and cognitive overhead criteria (task completion time, number of keystrokes, web page reading times) with and without the use of browsing shortcuts, showed that browsing shortcuts has a statistically significant positive effect on navigation performance of blind people. In this paper we examine further the idea of browsing shortcuts by presenting a personalised user interface of a specialised voice web browser. Three ways of personalising the user interface are presented based on the reordering and adaptation of browsing shortcuts and well as by incorporating recommendations about browsing shortcut selection. © 2007 IEEE."
8.14E+28,International Conference on Data Engineering,mamoru hoshi + michihiro kobayakawa,2007,A Partial Retrieval of Music Data with Different Bit Rate using MPEG-4 TwinVQ Audio Compression,data mining + data compression + encoding + autocorrelation + music + information systems + audio compression + robustness + multiple signal classification + transform coding + approximation theory + information retrieval,,"The present paper describes a method for partial retrieval of a piece of music using the autocorrelation coefficients computed in the encoding step of TwinVQ (Transform-domain Weighted Interleave Vector Quantization) audio compression (MPEG-4 audio standard). Our key contribution is to realize partial retrieval of a piece of music that is robust with respect to bit rate using un approximution relation. The upproximulion relution is based on the fact that the i-th autocorrelation coefficient with bit rate B1 of a piece of music computed in the encoding step of TwinVQ audio compression can approximate the j-th autocorrelation coefficient with bit rate B2 of the piece of music, where i = _B1/B2j_. First, we present our frame work for music information retrieval and music retrieval of a piece of music based on the autocorrelation coefficients computed in the encoding step of Twin VQ audio compression. Then, we show experimental results of partial retrieval of a piece of music on 1, 023 pieces of music. The experimental results indicate that partial retrieval using the autocorrelation coefficients has excellent retrieval performance for queries of various hit rates. © 2007 IEEE."
7C67CC42,International Conference on Data Engineering,hetal thakkar + yijian bai + haixun wang + carlo zaniolo,2007,Optimizing Timestamp Management in Data Stream Management Systems,robustness + testing + frequency,,"It has long been recognized that multi-stream operators, such as union and join, often have to wait idly in a temporarily blocked state, as a result of skews between the timestamps of their input streams. Recently, it has been shown that the injection of heartbeat information through punctuation tuples can alleviate this problem. In this paper, we propose and investigate more effective solutions that use timestamps generated on-demand to reactivate idle-waiting operators. We thus introduce a simple execution model that efficiently supports on-demand punctuation. Experiments show that response time and memory usage are reduced substantially by this approach. •_ 2007 IEEE."
7FA4BA07,International Conference on Data Engineering,nur zincirheywood + xiao luo,2007,Incorporating Temporal Information for Document Classification,computer science + genetic programming + kernel + pattern analysis + feature selection + text analysis + support vector machines + encoding + genetic algorithms,,"In this paper, we propose a novel document classification system where the Recurrent Linear Genetic Programming is employed to classify documents that are represented in encoded word sequences by Self Organizing feature Maps. The results using different feature selection techniques on Reuters 21578 data set show that the proposed system can analyze the temporal sequence patterns of a document and achieve competitive performance on classification. •_ 2007 IEEE."
8066952F,International Conference on Data Engineering,chinghsiang tsai + pujeng cheng + chenming hung,2007,LiveConcept: Web Search using Structured Query,classification + clustering algorithms + internet + search engine + web pages + taxonomy + information science + computer science + search engines,,"In this paper, we propose an approach that structure the given query as a taxonomy, called a query taxonomy, from the users perspective. The proposed approach is based on an unsupervised classification method, which uses the dynamic Web as the training corpus. With query taxonomy, users can browse relevant Web documents more conveniently and comprehensibly. Based on the proposed approach, we empirically build a search engine system, LiveConcept, and verify the feasibility and the effectiveness of the system after a series of user study. •_ 2007 IEEE."
8076490E,International Conference on Data Engineering,ehsan sheybani + elke a rundensteiner + giti javidi + jianyu liang + aparna s varde,2007,Learning the Relative Importance of Features in Image Data,computer science + scanning electron microscopy + transmission electron microscopy + nanotechnology + bioinformatics + distance function + etching + iterative methods + image processing + engineering drawings + image features + mechanical engineering + image analysis,,"In computational analysis in scientific domains, images are often compared based on their features, e.g., size, depth and other domain-specific aspects. Certain features may be more significant than others while comparing the images and drawing corresponding inferences for specific applications. Though domain experts may have subjective notions of similarity for comparison, they seldom have a distance function that ranks the image features based on their relative importance. We propose a method called FeaturesRank for learning such a distance function in order to capture the semantics of the images. We are given training samples with pairs of images and the extent of similarity identified for each pair. Using a guessed initial distance function, FeaturesRank clusters the given images in levels. It then adjusts the distance function based on the eiror between the clusters and training samples using heuristics proposed in this paper. The distance function that gives the lowest error is the output. This contains the features ranked in the order most appropriate the domain. FeaturesRank is evaluated with real image data from nanotechnology and bioinformatics. The results of our evaluation are presented in the paper. •_ 2007 IEEE."
7D5068B3,International Conference on Data Engineering,chengcui zhang + weibang chen + xin chen,2007,A Multiple Instance Learning Framework for Incident Retrieval in Transportation Surveillance Video Databases,feedback + information retrieval + data mining + support vector machines + high dimensional data + semantic gap + trajectory + transportation + svm + radio frequency,,"""Traffic incidents are frequent query targets in a transportation surveillance video database. Therefore, understanding and retrieving transportation videos based on their semantic contents becomes an urgent task. For this purpose, this paper proposes an interactive Multiple Instance Learning (MIL) framework for semantic video retrieval. It incorporates techniques in multimedia processing, data mining, and information retrieval. By tracking vehicles trajectories in a video and modeling semantic events, the framework initiates a progressive learning process guided by the users Relevance Feedback (RF). The choice of RF is for reducing the """"semantic gap"""" between the machine-readable features and the high level human concepts, which is a popular technique in the area of Content-based Image Retrieval (CBIR). With the information provided by RF, a mapping between semantic video retrieval and MIL is established. Due to its robustness to high-dimensional data, One-class SVM is selected to be the core learning algorithm for MIL in this framework. Although the proposed work is intended for transportation surveillance videos, it is designed as a general framework and can be tailored to other applications as well. The effectiveness of the algorithm is demonstrated by our experiments on real-life transportation surveillance videos. © 2007 IEEE."""
7FF62E26,International Conference on Data Engineering,onur kutlubay + burak turhan,2007,Mining Software Data,quality management + software engineering + programming + application software + software metrics + learning artificial intelligence + software development processes + software quality + data mining,,"Data mining techniques and machine learning methods are commonly used in several disciplines. It is possible that they could also provide a basis for quality assessment of software development processes and the final software product. Number of researches who employ such techniques and methods on software cost and effort estimation are increasing. This article provides a software quality perspective on data mining and machine learning applications, and explains the current research challenges that are related to incorporating these tools based on several metrics derived from the software development processes and the software itself. •_ 2007 IEEE."
7DE51BB2,International Conference on Data Engineering,christian schueller + wolfgang woerndl + rolf wojtech,2007,A Hybrid Recommender System for Context-aware Recommendations of Mobile Applications,rule based + testing + mobile computing + user interfaces + collaborative filtering + displays + application software + recommender systems + recommender system + bandwidth + point of interest + collaboration + internet,,"The goal of the work in this paper is towards the incorporation of context in recommender systems in the domain of mobile applications. The approach recommends mobile applications to users based on what other users have installed in a similar context. The idea is to apply a hybrid recommender system to deal with the added complexity of context. We have designed and realized the application to test our ideas. Users can select among several content-based or collaborative filtering components, including a rule-based module using information on point-of-interests in the vicinity of the user, and a component for the integration of traditional collaborative filtering. The implementation is Integrated in a framework supporting the development and deployment of mobile services. © 2007 IEEE."
7E8C9E93,International Conference on Data Engineering,murat goksedef + a s etaneruyar + g demir,2007,Effects of Session Representation Models on the Performance of Web Recommender Systems,collaboration + data mining + accuracy + feature vector + navigation + recommender systems + real time systems + web accessibility + web services + association rules + recommender system + filtering,,"Usage pattern discovery is a crucial part of web recommendation systems and is strongly related with the performance of the recommender. User sessions which are constructed from web access logs give information about a users navigational behavior. These session records are one of the main resources in the pattern discovery stage. The representation of the sessions can be in different forms such as feature vectors based on page durations or existence/nonexistence of pages. This work analyzes the effects of the different recommendation models which consider different characteristics of user sessions. For this purpose, we used three different recommender models. The first one considers only the existence of the visited pages in a session and the view time of each page. The second recommender model considers only the order of the visited pages in each session. The third model is based on the co-occurrence of the visited pages among user sessions. Our experimental results show that using the ordering information improves the prediction accuracy of the next request. © 2007 IEEE."
7E9F4324,International Conference on Data Engineering,chengcui zhang + weibang chen + wenlin liu,2007,An Automatic and Robust Method for Microarray Image Analysis and the Related Information Retrieval for Microarray Databases,image retrieval + microarray databases + genomics + image segmentation + information retrieval + cancer + dna microarray + robustness + image processing + dna + image analysis + genetic engineering + data analysis + data mining,,"DNA microarray is an increasingly important technique that allows biologists to monitor expression levels of thousands of genes in parallel. This technique is widely used in biological research for studying genomic aberrations in cancer and other genetic diseases, and therefore, it has great potential for clinical diagnostics in the future. However, there are still several critical problems with this technology. In this study, we focus on two important issues. The first issue is related to automatic gridding and spot segmentation for microarray images. It has been reported that the quality of spot segmentation significantly influences data precision in the subsequent data analysis. However, nowadays microarray image analysis software still requires users fine tuning to obtain acceptable results. Another important issue is how to automatically collect related information regarding all genes on the microarray slide for subsequent data analysis and data mining. To relieve researchers from manually correcting image processing results and manually collecting the related information for genes, in this paper, we proposed an automatic and robust method for microarray image analysis and the related information retrieval module which is integrated with the proposed database schema for microarray image data. •_ 2007 IEEE."
7F61524B,International Conference on Data Engineering,duydinh le + shin ichi satoh + michael e houle + d phuoc + tat nguyen,2007,Finding Important People in Large News Video Databases Using Multimodal and Clustering Analysis,knowledge discovery + scalability + broadcasting + face detection + clustering analysis + face recognition + statistical analysis + data mining + cluster analysis + databases,,"The wide availability of large scale databases requires more efficient and scalable tools for data understanding and knowledge discovery. In this paper, we present a method to find important people who have appeared repeatedly in a certain time period from large news video databases. Specifically, we investigate two issues: how to group similar faces to find dominant groups and how to label these groups by the corresponding names for identification. These are challenging problems because firstly people can appear with large appearance variations such as hair styles, illumination conditions and poses that make comparing between similar faces more difficult; secondly, the number of people and their occurrence frequencies that are unknown make finding dominant and useful groups more complicated: and finally, the fact that in news video faces and names usually do not appear together can make troubles in aligning faces and names. To handle above problems, we propose using the relevant set correlation based clustering model which can efficiently handle dataset of millions of objects represented in thousands or even millions of dimensions to find groups of similar faces from the large and noisy face dataset. Then in order to identify faces in clusters, names extracted from the transcripts are filtered and used to find the best correspondences by using methods developed in the statistical machine translation literature. Experiments on large video datasets containing hundreds of hours showed that our system can efficiently find out important people by not only their appearance hut also their identification. •_ 2007 IEEE."
7F528369,International Conference on Data Engineering,ninghui li + tiancheng li + suresh venkatasubramanian,2007,t-Closeness: Privacy Beyond k-Anonymity and l-Diversity,k-anonymity privacy requirement + databases + attribute disclosure + identifying attributes + data security + t-closeness + equivalence class + l-diversity + privacy + microdata publishing + database theory + computer science + earth mover distance measure + earth + data privacy + publishing,AuthorProvided Keywords Not Found,"The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain ""identifying"" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments."
7F8A80FA,International Conference on Data Engineering,bart kuijpers + alejandro a vaisman,2007,A Data Model for Moving Objects Supporting Aggregation,environmental economics + data structures + data warehouses + data model + relational databases + first order + geographic information systems + online analytical processing + data models + geographic information system,,""
7D26E44B,International Conference on Data Engineering,vebjorn ljosa + a k singh,2007,APLA: Indexing Arbitrary Probability Distributions,multivariate distribution + historical business transactions + query uncertain information + indexation + multidimensional systems + statistical distributions + query processing + piecewise linear + database indexing + probability distribution + approximation theory + indexing + arbitrary probability distribution indexing + sensor readings + biomedical images + biosensors + range query + image sensors + index multivariate distributions + probabilistic range queries + uncertain data + adaptive piecewise-linear approximations + k-NN queries + biomedical imaging + APLA + moving objects,AuthorProvided Keywords Not Found,"The ability to store and query uncertain information is of great benefit to databases that infer values from a set of observations, including databases of moving objects, sensor readings, historical business transactions, and biomedical images. These observations are often inexact to begin with, and even if they are exact, a set of observations of an attribute of an object is better represented by a probability distribution than by a single number, such as a mean. In this paper, we present adaptive, piecewise-linear approximations (APLAs), which represent arbitrary probability distributions compactly with guaranteed quality. We also present the APLA-tree, an index structure for APLAs. Because APLA is more precise than existing approximation techniques, the APLA-tree can answer probabilistic range queries twice as fast. APLA generalizes to multiple dimensions, and the APLA-tree can index multivariate distributions using either one-dimensional or multidimensional APLAs. Finally, we propose a new definition of k-NN queries on uncertain data. The new definition allows APLA and the APLA-tree to answer k-NN queries quickly, even on arbitrary probability distributions. No efficient k-NN search was previously possible on such distributions."
7E11E9BC,International Conference on Data Engineering,shubha u nabar + d j b thomas + rajeev motwani,2007,Auditing a Batch of SQL Queries,database systems + computational complexity + polynomials + semantic networks + np hard problem + sql + computer science + diabetes + private information + data privacy + blood pressure,,"In this paper, we study the problem of auditing a batch of SQL queries: given a set of SQL queries that have been posed over a database, determine whether some subset of these queries have revealed private information about an individual or group of individuals. In [2], the authors studied the problem of determining whether any single SQL query in isolation revealed information forbidden by the database systems data disclosure policies. In this paper, we extend this work to the problem of auditing a batch of SQL queries. We define two different notions of auditing - semantic auditing and syntactic auditing - and show that while syntactic auditing seems more desirable, it is in fact NP-hard to achieve. The problem of semantic auditing of a batch of SQL queries is, however, tractable and we give a polynomial time algorithm for this purpose. •_ 2007 IEEE."
7F746E48,International Conference on Data Engineering,paolo ciaccia + marco patella + ilaria bartolini,2007,PIBE: Manage Your Images the Way You Want!,visualization + shape + graphical user interfaces + displays + image browsing + digital photography + pressing + feature extraction + clustering algorithms + image retrieval + PIBE + gui + GUI + internet,AuthorProvided Keywords Not Found,"A customizable system for image browsing, named PIBE, is proposed. In details, PIBE provides the user with a set of browsing and personalization facilities that enable an effective and efficient exploration of the image collection. The approach is novel and appealing because: 1) the personalization actions over the hierarchical organization of images are local, 2) the storage of the browsing structure is persistent, and 3) the provided GUI makes browsing and personalization facilities extremely intuitive and ""easy-to-use""."
7D196BFA,International Conference on Data Engineering,meiling shyu + shuching chen + min chen,2007,Hierarchical Temporal Association Mining for Video Event Detection in Video Databases,feature extraction + data management + data mining + pattern recognition + temporal databases + association rule mining + data access + association rules + distributed databases + domain knowledge + database indexing,,"With the proliferation of multimedia data and ever-growing requests for multimedia applications, new challenges are emerged for efficient and effective managing and accessing large audio-visual collections. In this paper, we present a novel framework for video event detection, which plays an essential role in high-level video indexing and retrieval. Especially, since temporal information in a video sequence is critical in conveying video content, a hierarchical temporal association mining approach is developed to systematically capture the characteristic temporal patterns with respect to the events of interest. In this process, the unique challenges caused by the loose video structure and skewed data distribution issues are effectively tackled. In addition, an adaptive mechanism is proposed to determine the essential thresholds which are generally defined manually in the traditional association rule mining (ARM) approach. This framework thus largely relaxes the dependence on the domain knowledge and contributes to the ultimate goal of automatic video content analysis. •_ 2007 IEEE."
80E07E0E,International Conference on Data Engineering,weishinn ku + roger zimmermann + sushama shroff + wenchih peng,2007,Privacy Protected Query Processing on Spatial Networks, + location based services + private information + location based service + neural networks + euclidean space + privacy + range query + mobile device + mobile computing + computer science + mobile devices,,"With the proliferation of mobile devices (e.g., PDAs, cell phones, etc.), location-based services have become more and more popular in recent years. However, users have to reveal their location information to access location-based services with existing service infrastructures. It is possible that adversaries could collect the location information, which in turn invades user's privacy. There are existing solutions for query processing on spatial networks and mobile user privacy protection in Euclidean space. However there is no solution for solving queries on spatial networks with privacy protection. Therefore, we aim to provide network distance spatial query solutions which can preserve user privacy by utilizing K-anonymity mechanisms. In this paper, we present two novel query algorithms, PSNN and PSRQ, for answering nearest neighbor queries and range queries on spatial networks without revealing private information of the query initiator. The effectiveness of our privacy protected algorithms has been validated using real world road networks. In addition, we demonstrate the appeal of our technique using extensive simulation results."
7DC49FC5,International Conference on Data Engineering,s k singh + susanne e hambrusch + sanjay prabhakar + chris mayfield + rohan shah,2007,Indexing Uncertain Categorical Data,application software + database management system + web pages + database integration + biology + indexing + R-tree + indexation + uncertainty handling + relational databases + uncertainty + database management systems + r tree + database indexing + inverted index structure + computer science + inverted index + data cleaning + uncertain categorical data indexing + categorical data + tree data structures + error correction + database systems,AuthorProvided Keywords Not Found,"Uncertainty in categorical data is commonplace in many applications, including data cleaning, database integration, and biological annotation. In such domains, the correct value of an attribute is often unknown, but may be selected from a reasonable number of alternatives. Current database management systems do not provide a convenient means for representing or manipulating this type of uncertainty. In this paper we extend traditional systems to explicitly handle uncertainty in data values. We propose two index structures for efficiently searching uncertain categorical data, one based on the R-tree and another based on an inverted index structure. Using these structures, we provide a detailed description of the probabilistic equality queries they support. Experimental results using real and synthetic datasets demonstrate how these index structures can effectively improve the performance of queries through the use of internal probabilistic information."
7F9298A2,International Conference on Data Engineering,felix naumann + veronique tietz + jana bauckmann + ulf leser,2007,Efficiently Detecting Inclusion Dependencies,data integration + meta data + computer science + proteins + data structures + data integrity + relational databases + database integration + metadata,,"Data sources for data integration often come with spurious schema definitions such as undefined foreign key constraints. Such metadata are important for querying the database and for database integration. We present our algorithm SPIDER (Single Pass Inclusion DEpendency Recognition) for detecting inclusion dependencies, as these are the automatically testable part of a foreign key constraint. For IND detection all pairs of attributes must be tested. SPIDER solves this task very efficiently by testing all attribute pairs in parallel. It analyzes a 2 GB database in _ 20 min and a 21 GB database in _ 4 h."
7EDF9529,International Conference on Data Engineering,yicheng tu + bin yao + william j schroeder + sanjay prabhakar + song liu,2007,Using Control Theory for Load Shedding in Data Stream Management,load shedding + databases + mechanical engineering + control theory + data processing + feedback control + database performance + robustness + engineering management + database management systems + feedback + resource allocation + formal feedback control + control systems + data stream management system + environmental management,AuthorProvided Keywords Not Found,"Database performance can be greatly affected by environmental and internal dynamics such as workloads and system configurations. Existing strategies to maintain performance under such dynamics are often found to have poor robustness. To remedy this problem, we propose a systematic solution that takes advantages of formal feedback control techniques. In this demo, we show how the control-based solution derived from a dynamic DSMS model can be utilized to guide load shedding with the target of maintaining data processing delays."
81566AC6,International Conference on Data Engineering,ryan bishop + barbara carminati + patrick c k hung + elena ferrari,2007,Security Conscious Web Service Composition with Semantic Web Support,semantic web + simple object access protocol + software system + software design + data security + open systems + internet + service oriented architecture + web services + software architecture + privacy + computer security + software systems + web service,Semantic Web + Web service composition + security constraints + REI,"A Web service Is a software system designed to support Interoperable application-to-application interactions over the Internet. Recently, there has been a growing interest in Web service composition, and some languages (e.g., WSBPEL and BPML) for modeling the composition have been proposed. In this paper, we focus on security constraints of Web service composition with semantic Web support, which have not been deeply investigated so far. Based on our prior work, we present a method for modeling security constraints and a brokered architecture, which exploits the REI reasoner, to build composite Web services according to the specified security constraints. •_ 2007 IEEE."
7ECA4A8C,International Conference on Data Engineering,lyublena antova + dan olteanu + c t koch,2007,MayBMS: Managing Incomplete Information with Probabilistic World-Set Decompositions,database languages + incomplete information + information management + relational algebra + finite world-sets + data mining + PostgreSQL + testing + algebra + possible worlds + SQL-like language + scalability + sql + SQL + query processing + relation algebra + MayBMS + relational algebra queries + query language + probabilistic world-set decompositions,AuthorProvided Keywords Not Found,"Managing incomplete information is important in many real world applications. In this demonstration we present MayBMS - a system for representing and managing finite sets of possible worlds - that successfully combines expressiveness and efficiency. Some features of MayBMS are: completeness of the representation system for finite world-sets; space-efficient representation of large world-sets; scalable evaluation and support for full relational algebra queries; and probabilistic extension of the representation system and the query language. MayBMS is implemented on top of PostgreSQL. It models incomplete data using the so-called world-set decompositions (WSDs) (Ruggles et al., 2004). For this demonstration, we introduce a probabilistic extension of world-sets and WSDs, where worlds or correlations between worlds have probabilities. The main idea underlying probabilistic WSDs is to use relational factorization combined with probabilistic independence in order to efficiently decompose large world-sets into a set of independent smaller relations. Queries in MayBMS can be expressed in an SQL-like language with special constructs that deal with incompleteness and probabilities. MayBMS rewrites and optimizes user queries into a sequence of relational queries on world-set decompositions."
81459739,International Conference on Data Engineering,evaggelia pitoura + panos vassiliadis + kostas stefanidis,2007,Adding Context to Preferences,information processing + multidimensional attributes + computer networks + data structure + indexation + multidimensional systems + profile tree + query processing + information specification + context resolution problem + usability + information handling + computer science + context modeling + tree data structures + personalization systems + contextual preferences,AuthorProvided Keywords Not Found,"To handle the overwhelming amount of information currently available, personalization systems allow users to specify the information that interests them through preferences. Most often, users have different preferences depending on context. In this paper, we introduce a model for expressing such contextual preferences. Context is modeled as a set of multidimensional attributes. We formulate the context resolution problem as the problem of (a) identifying those preferences that qualify to encompass the context state of a query and (b) selecting the most appropriate among them. We also propose an algorithm for context resolution that uses a data structure, called the profile tree, that indexes preferences based on their associated context. Finally, we evaluate our approach from two perspectives: usability and performance."
7F29999E,International Conference on Data Engineering,sweeseong wong + wingkin sung + limsoon wong,2007,CPS-tree: A Compact Partitioned Suffix Tree for Disk-based Indexing on Large Genome Sequences,disk-based representation + databases + indexing + data structure + indexation + sequences + disk-based indexing + bit packing + tree traversal operations + storage scheme + storage management + genetics + biology computing + compact partitioned suffix tree + bioinformatics + genome sequence + genomics + search retrieval + pattern search + tree data structures + query formulation + large genome sequences,AuthorProvided Keywords Not Found,"Suffix tree is an important data structure for indexing a long sequence (like a genome sequence) or a concatenation of sequences. It finds many applications in practice, especially in the domain of bioinformatics. Suffix tree allows for efficient pattern search with time independent of the sequence length. However, the performance of disk-based suffix tree is a concern as it is slowed down significantly by poor localized access resulting in high 10 disk access. The focus of this paper is to design an IO-efficient and compact partitioned suffix tree representation (CPS-tree) on disk. We show that representing suffix tree using CPS-tree has several advantages. First, our representation allows us to visit any node in the suffix tree by accessing at most log n pages of the tree where n is the length of the sequence. Second, our storage scheme improves the access pattern and reduces the number of page fault resulting in efficient search retrieval and efficient tree traversal operations. Third, by bit packing, our index is compact. Experimental results show that CPS-tree outperforms other indexes on disk. When fully loaded into the main memory, CPS-tree is still efficient. Hence, we expect CPS-tree to be a good disk-based representation of suffix tree, with potential use in practical applications."
7F08FEC9,International Conference on Data Engineering,xiang lian + lei chen,2007,A General Cost Model for Dimensionality Reduction in High Dimensional Spaces,local dimensionality reduction + high dimensional space + similarity search + data analysis + indexing + curse of dimensionality + fast similarity search + indexation + information retrieval + space technology + degradation + data dimensionality + general cost model + range query + query processing + global dimensionality reduction + tree index + computer science + retrieval efficiency + synthetic data + image retrieval,AuthorProvided Keywords Not Found,"Similarity search usually encounters a serious problem in the high dimensional space, known as the ""curse of dimensionality"". In order to speed up the retrieval efficiency, previous approaches usually reduce the dimensionality of the entire data set to a fixed lower value before building indexes (referred to as global dimensionality reduction (GDR)). More recent works focus on locally reducing the dimensionality of data to different values (called the local dimensionality reduction (LDR)). However, so far little work has formally evaluated the effectiveness and efficiency of both GDR and LDR for range queries. Motivated by this, in this paper, we propose a general cost model for both GDR and LDR, in light of which we introduce a novel LDR method, PRANS. It can achieve high retrieval efficiency with the guarantee of optimality given by the formal model. Finally, a B+-tree index is constructed over the reduced partitions for fast similarity search. Extensive experiments validate the correctness of our cost model on both real and synthetic data sets, and demonstrate the efficiency and effectiveness of the proposed PRANS method."
7FE150E7,International Conference on Data Engineering,masaru kitsuregawa + ying zhang + xuemin lin + yidong yuan + xiaofang zhou + j xu yu,2007,Summarizing Order Statistics over Data Streams with Duplicates,approximation theory + data analysis + histograms + tail + data mining + data streams + order statistics summarization + rank queries processing + query processing + relative error + duplicated data elements + data structures + order statistic + rank queries approximation + statistics,AuthorProvided Keywords Not Found,"In this paper, we investigated the problem of approximately processing rank queries against distinct data elements in a data stream with the presence of duplicated data elements. Novel space and time efficient techniques are developed for continuously maintaining order statistics so that rank queries can be answered with a relative error guarantee. This is the first work providing the space and time efficient data stream techniques to process approximate rank queries with relative error guarantees against distinct data elements."
7F7208D9,International Conference on Data Engineering,dongwoo won + dennis mcleod,2007,Ontology-driven Rule Generalization and Categorization for Market Data,rfid tags + association rule + ontology + association rules + frequency + ontologies + search space + computer science + data mining + radio frequency identification + supply chain + privacy + rfid + space technology + supply chains + hierarchical clustering + information analysis,,"Radio Frequency Identification (RFID) is an emerging technique that can significantly enhance supply chain processes and deliver customer service improvements. RFID provides user with efficient tracking on the flow of products throughout the wholesale process. However, the large number of information that has been generated from such a process creates difficulty in extracting and analyzing useful information. In this paper, we propose a method to mine the large data sets that allows smaller and more relevant search space compared to the original data sets. Our work is constructed from the following approaches: ontology-driven rule generalization which concentrates on controlling the level of items, and rule categorization using hierarchical association rule clustering that group the generated rules from the given problem space into hierarchical search space. The detailed steps for rule generalization based on ontologies are presented, as well as the algorithms for rule categorization using hierarchical association rule clustering is developed. Our experiment proves the feasibility of our work which shows the significant reduction of the search space by decreasing the number of rules to be looked at and increasing the relevance among the rules. •_ 2007 IEEE."
80E90C56,International Conference on Data Engineering,mario a nascimento + beng chin ooi + anthony k h tung + yueguo chen,2007,SpADe: On Shape-based Pattern Detection in Streaming Time Series,time series analysis + SpADe + pattern analysis + dynamic time warping + streaming time series + pattern matching + streaming data + query patterns + warping distances + Spatial Assembling Distance + time series + euclidean distance + database management systems + predefined pattern monitoring + longest common subsequence + subsequence matching + time measurement + Euclidean distance + assembly + on shape-based pattern detection,AuthorProvided Keywords Not Found,"Monitoring predefined patterns in streaming time series is useful to applications such as trend-related analysis, sensor networks and video surveillance. Most current studies on such monitoring employ Euclidean distance to calculate the similarities between given query patterns and subsequences of streaming time series. Euclidean distance has been shown to be ineffective in measuring distances of time series in which shifting and scaling usually exist. Consequently, warping distances such as dynamic time warping (DTW), longest common subsequence (LCSS), have been proposed to handle warps in temporal dimension. However, they are inadequate in handling shifting and scaling in amplitude dimension. Moreover, they have been designed mainly for full sequence matching, whereas in online monitoring applications, we typically have no knowledge on the positions and lengths of possible matching subsequences. In this paper, we first discuss the weaknesses of existing warping distances on detecting patterns from streaming time series. We then propose a novel warping distance, which we name Spatial Assembling Distance (SpADe), that is able to handle shifting and scaling in both temporal and amplitude dimensions. We further propose an efficient approach for continuous pattern detection using SpADe, that is fundamental for subsequence matching on streaming data. Finally, our experimental results show that SpADe is effective and efficient for continuous pattern detection in streaming time series."
7E3171F4,International Conference on Data Engineering,hong cheng + jiawei han + xifeng yan + chihwei hsu,2007,Discriminative Frequent Pattern Analysis for Effective Classification,information gain + pattern analysis + pattern classification + indexing + solids + frequent pattern-based classification + data mining + kernel + association rules + empirical study + scalability + relational data + discriminative frequent pattern analysis + feature selection + frequent pattern mining,AuthorProvided Keywords Not Found,"The application of frequent patterns in classification appeared in sporadic studies and achieved initial success in the classification of relational data, text documents and graphs. In this paper, we conduct a systematic exploration of frequent pattern-based classification, and provide solid reasons supporting this methodology. It was well known that feature combinations (patterns) could capture more underlying semantics than single features. However, inclusion of infrequent patterns may not significantly improve the accuracy due to their limited predictive power. By building a connection between pattern frequency and discriminative measures such as information gain and Fisher score, we develop a strategy to set minimum support in frequent pattern mining for generating useful patterns. Based on this strategy, coupled with a proposed feature selection algorithm, discriminative frequent patterns can be generated for building high quality classifiers. We demonstrate that the frequent pattern-based classification framework can achieve good scalability and high accuracy in classifying large datasets. Empirical studies indicate that significant improvement in classification accuracy is achieved (up to 12% in UCI datasets) using the so-selected discriminative frequent patterns."
7F325E2F,International Conference on Data Engineering,alonso silva + luciano barbosa + juliana freire,2007,Organizing Hidden-Web Databases by Clustering Visible Web Documents,document handling + Web forms + data mining + information retrieval + database management systems + entropy + Web document clustering + pattern clustering + context modeling + hidden web + Internet + hidden-Web database + internet + hyperlinked objects,AuthorProvided Keywords Not Found,"In this paper we address the problem of organizing hidden-Web databases. Given a heterogeneous set of Web forms that serve as entry points to hidden-Web databases, our goal is to cluster the forms according to the database domains to which they belong. We propose a new clustering approach that models Web forms as a set of hyperlinked objects and considers visible information in the form context - both within and in the neighborhood of forms - as the basis for similarity comparison. Since the clustering is performed over features that can be automatically extracted, the process is scalable. In addition, because it uses a rich set of metadata, our approach is able to handle a wide range of forms, including content-rich forms that contain multiple attributes, as well as simple keyword-based search interfaces. An experimental evaluation over real Web data shows that our strategy generates high-quality clusters - measured both in terms of entropy and F-measure. This indicates that our approach provides an effective and general solution to the problem of organizing hidden-Web databases."
754ABA4B,International Conference on Data Engineering,jeffrey f naughton + david j dewitt + tochukwu iwuchukwu + anhai doan,2007,K-Anonymization as Spatial Indexing: Toward Scalable and Incremental Anonymization,indexing + databases + spatial index + indexation + database indexing + multidimensional systems,,"In this paper we observe that k-anonymizing a data set is strikingly similar to building a spatial index over the data set, so similar in fact that classical spatial indexing techniques can be used to anonymize data sets. We use this observation to leverage over 20 years of work on database indexing to provide efficient and dynamic anonymization techniques. Experiments with our implementation show that the R-tree index-based approach yields a batch anonymization algorithm that is orders of magnitude more efficient than previously proposed algorithms and has the advantage of supporting incremental updates. Finally, we show that the anonymizations generated by the R-tree approach do not sacrifice quality in their search for efficiency; in fact, by several previously proposed quality metrics, the compact partitioning properties of R-trees generate anonymizations superior to those generated by previously proposed anonymization algorithms."
7D49F195,International Conference on Data Engineering,davood rafiei + reza sherkat,2007,On MBR Approximation of Histories for Historical Queries: Expectations and Limitations,blood pressure + approximation theory + temperature measurement + indexing + R-tree + trees (mathematics) + indexation + spatial index + time series + heuristic + history + multidimensional systems + r tree + historical queries processing + multidimensional time-series + query processing + filter-and-refine scheme + minimum bounding hyper-rectangle approximation + spatial index structure + filtering,AuthorProvided Keywords Not Found,"Traditional approaches for efficiently processing historical queries, where a history is a multidimensional time-series, employ a two step filter-and-refine scheme. In the filter step, an approximation of each history often as a set of minimum bounding hyper-rectangles (MBRs) is organized using a spatial index structure such as R-tree. The index is used to prune redundant disk accesses and to reduce the number of pairwise comparisons required in the refine step. To improve the efficiency of the filtering step, a heuristic is used to decrease the expected number of MBRs that overlap with a query, by reducing the volume of empty space indexed by the index. The heuristic selects, among all possible splitting schemes of a history, the one which results to a set of MBRs with minimum total volume. Although this heuristic is expected to improve the performance of spatial and history based queries with small temporal and spatial extents, in many real settings, the performance of historical queries depends on the extent of the query. Moreover, the optimal approximation of a history is not always the one with minimum total volume. In this paper, we present the limitations of using volume as a criteria for approximating histories, specially in high dimensional cases, where it is not feasible to index MBRs by traditional spatial index structures."
80F10156,International Conference on Data Engineering,j woycheese + varsha raghavan + elke a rundensteiner + abhishek mukherji,2007,FireStream: Sensor Stream Processing for Monitoring Fire Spread,pattern analysis + intelligent buildings + FireStream + fire spread monitoring + MJoin solution + sensor stream processing + fires + shared window execution + stream processing + engines + computerised monitoring + intelligent sensors + smoke detectors + building management systems,AuthorProvided Keywords Not Found,"This demonstration presents FireStream, a sensor stream processing system which provides services for run-time detection, monitoring and visualization of fire spread in intelligent buildings that can be of great benefit to first responders. Our system can effectively handle large heterogeneous sensor streams using shared window execution and dynamic participant handling to yield a high-ary MJoin solution."
8061BB32,International Conference on Data Engineering,omer egecioglu + ping wu + a el abbadi + dharma p agrawal,2007,DeltaSky: Optimal Maintenance of Skyline Deletions without Exclusive Dominance Region Generation,shape + algorithm design and analysis + skyline deletion + trees (mathematics) + b tree + B-Tree + hyperrectangles + branch and bound + software maintenance + DeltaSky algorithm + materialized skyline view + query processing + acceleration + skyline query computation + deletion maintenance + computer science + exclusive dominance region generation + cost function + skyline removal + database systems,AuthorProvided Keywords Not Found,"This paper addresses the problem of efficient maintenance of a materialized skyline view in response to skyline removals. While there has been significant progress on skyline query computation, an equally important but largely unanswered issue is on the incremental maintenance for skyline deletions. Previous work suggested the use of the so called exclusive dominance region (EDR) to achieve optimal I/O performance for deletion maintenance. However, the shape of an EDR becomes extremely complex in higher dimensions, and algorithms for its computation have not been developed. We derive a systematic way to decompose a d-dimensional EDR into a collection of hyper-rectangles. We show that the number of such hyper-rectangles is O(md), where m is the current skyline result size. We then propose a novel algorithm DeltaSky which determines whether an intermediate R-tree MBR intersects with the EDR without explicitly calculating the EDR itself. This reduces the worse case complexity of the EDR intersection check from O(md) to O(md). Thus DeltaSky helps the branch and bound skyline algorithm achieve I/O optimality for deletion maintenance by finding only the newly appeared skyline points after the deletion. We discuss implementation issues and show that DeltaSky can be efficiently implemented using one extra B-Tree. Moreover, we propose two optimization techniques which further reduce the average cost in practice. Extensive experiments demonstrate that DeltaSky achieves orders of magnitude performance gain over alternative solutions."
7DCAD6D3,International Conference on Data Engineering,mingliang wei + changhao jiang + marc snir,2007,Programming Patterns for Architecture-Level Software Optimizations on Frequent Pattern Mining,application software + software architecture + computer science + computer architecture + data mining + data structures + architecture-level software optimization + programming patterns + programming + frequent pattern mining,AuthorProvided Keywords Not Found,"One very important application in the data mining domain is frequent pattern mining. Various authors have worked on improving the efficiency of this computation, mostly focusing on algorithm-level improvement. More recent work has explored architecture specific optimizations of this computation. Our goal in this paper is to provide a systematic approach to architecture-level software optimizations by identifying applicable tuning patterns. We show the generality and effectiveness of these patterns by tuning several frequent pattern mining algorithms and showing significant performance improvements."
7D8C1922,International Conference on Data Engineering,charu c aggarwal,2007,On Density Based Transforms for Uncertain Data Mining,intermediate representation + concrete + data mining + testing + missing data + density-based transforms + missing data handling + uncertain data mining + feature extraction + demography + data sets + error-prone data handling + mathematical foundations + data privacy + data handling + statistical analysis + privacy preserving data mining,AuthorProvided Keywords Not Found,"In spite of the great progress in the data mining field in recent years, the problem of missing and uncertain data has remained a great challenge for data mining algorithms. Many real data sets have missing attribute values or values which are only approximately measured or imputed. In some methodologies such as privacy preserving data mining, it is desirable to explicitly add perturbations to the data in order to mask sensitive values. If the underlying data is not of high quality, one cannot expect the corresponding algorithms to perform effectively. In many cases, it may be possible to obtain quantitative measures of the errors in different entries of the data. In this paper, we will show that this is very useful information for the data mining process, since it can be leveraged to improve the quality of the results. We discuss a new method for handling error-prone and missing data with the use of density based approaches to data mining. We discuss methods for constructing error-adjusted densities of data sets, and using these densities as intermediate representations in order to perform more accurate mining. We discuss the mathematical foundations behind the method and establish ways of extending it to very large scale data mining problems. As a concrete example of our technique, we show how to apply the intermediate density representation in order to accurately solve the classification problem. We show that the error-based method can be effectively and efficiently applied to very large data sets, and turns out to be very useful as a general approach to such problems."
7FCC103C,International Conference on Data Engineering,jianwei niu + william h winsborough + mark reith,2007,Apply Model Checking to Security Analysis in Trust Management,software systems + system testing + algorithms + systems analysis + language + management + maturity model + formal verification + government + security analysis + access control + polynomials + automation + hazards + authorisation + control + model checking + scalability + policies + security,,"Standard Form 298 (Rev. 8/98) Prescribed by ANSI Std. Z39.18"
7D4D1158,International Conference on Data Engineering,hanuma kodavalla + robin dhananjay dhamankar + vishal kathuria,2007,Transaction Isolation and Lazy Commit,transaction processing + personal information management + transaction commit + information management + transaction isolation + software systems + lazily committed transactions + warehousing + database management systems + mobile device + database system + throughput + database systems,AuthorProvided Keywords Not Found,"In order to guarantee durability of transactions (D in the ACID properties) database systems issue a synchronous log write on transaction commit. However, in many scenarios such as queue processing and personal information management systems, durability on commit is not required as the transactions rolled back due to a crash can be reprocessed using the application state. Avoiding such synchronous I/O improves system throughput and response time. Also, when the database is running on a mobile device or a laptop, reduced I/O consumes less power and increases battery life - a significant consideration for mobile users. Transactions that do not require durability can be ""lazily committed"", i.e. the log is not written synchronously upon commit. If another transaction reads such lazily committed data and performs an independently committed external action based on it, then an inconsistency may result if the effects of the lazily committed transaction are lost in a crash. We present a solution that provides the efficiency of lazily committed transactions and the flexibility for other transactions to read only durably committed data. We demonstrate that even in the presence of a large number of readers requesting durably committed data our approach reduces I/Os significantly."
7F9263B7,International Conference on Data Engineering,beng chin ooi + anthony k h tung + lizhen xu + shiyuan wang,2007,Efficient Skyline Query Processing on Peer-to-Peer Networks,databases + merging + peer-to-peer computing + tree structure + computer networks + information retrieval + query accessing patterns + distributed environment + query load balancing + history + P2P environment + hot spot + scalability + p2p + query processing + skyline query processing + resource allocation + balanced tree structured P2P network + satisfiability + load balance + tree data structures + search space + peer-to-peer network,AuthorProvided Keywords Not Found,"Skyline query has been gaining much interest in database research communities in recent years. Most existing studies focus mainly on centralized systems, and resolving the problem in a distributed environment such as a peer-to-peer (P2P) network is still an emerging topic. The desiderata of efficient skyline querying in P2P environment include: 1) progressive returning of answers, 2) low processing cost in terms of number of peers accessed and search messages, 3) balanced query loads among the peers. In this paper, we propose a solution that satisfies the three desiderata. Our solution is based on a balanced tree structured P2P network. By partitioning the skyline search space adaptively based on query accessing patterns, we are able to alleviate the problem of ""hot"" spots present in the skyline query processing. By being able to estimate the peer nodes within the query subspaces, we are able to control the amount of query forwarding, limiting the number of peers involved and the amount of messages transmitted in the network. Load balancing is achieved in query load conscious data space splitting/merging during the joining/departure of nodes and through dynamic load migration. Experiments on real and synthetic datasets confirm the effectiveness and scalability of our algorithm on P2P networks."
7FA58579,International Conference on Data Engineering,yali zhu + elke a rundensteiner,2007,Adapting Partitioned Continuous Query Processing in Distributed Systems,statistical distributions + distributed processing + query optimization + protocols + time measurement + distributed system + load balance + resource allocation + fluctuations + shape + distributed systems,SplitB + SplitC,"Partitioned query processing is an effective method to process continuous queries with large stateful operators in a distributed systems. This method typically partitions input data into non-overlapping portions, with each query plan instance installed on a separate machine processing only one portion of the data. Dynamic redistribution of load among machines is then employed to handle fluctuating stream characteristics. However, existing load redistribution solutions have made the implicit assumption that no heat query optimization is conducted at runtime on any of the participating machines, i.e., all local query plan instances are static and thus remain identical. This is restrictive for dynamic stream systems, where data partitions may experience significant fluctuations in selectivities or arrival rates over time ?thus warranting local plan reoptimization. This raises the new problem that the heterogeneity of plan shapes among different machines must be tackled when doing load redistribution. To address this, we propose two new load balancing strategies along with corresponding protocols, that can balance the workload across a set of machines while seamlessly handling the complexity caused by local plan changes. The PTLB strategy is plan-agnostic, requiring no detailed knowledge of the underlying query plan. The MSLB strategy is plan-aware, that is, it rebalances the load by comparing the plan shape differences on the participating machines. All proposed techniques have been implemented in the DCAPE continuous query system. Our experiments demonstrate that the application of both query optimization and load balancing results in superior performance compared to applying either of the adaptation techniques alone - as has been the state-of-the-art in the current literature. Our evaluation compares the relative applicability and efficiency of the two proposed techniques PTLB and MSLB. •_ 2007 IEEE."
8093DEF2,International Conference on Data Engineering,kenneth a ross,2007,Efficient Hash Probes on Modern Processors,SIMD instructions + hash table + bucketized Cuckoo hash table + indexing + latency-hiding + bandwidth + payloads + Cell SPE processor + parallel processing + Pentium 4 + processor scheduling + simd instructions + modern processor scheduling + database + stream processing + branch instructions + file organisation + data structures + out of order + database systems,AuthorProvided Keywords Not Found,"Bucketized versions of Cuckoo hashing can achieve 95-99% occupancy, without any space overhead for pointers or other structures. However, such methods typically need to consult multiple hash buckets per probe, and have therefore been seen as having worse probe performance than conventional techniques for large tables. We consider workloads typical of database and stream processing, in which keys and payloads are small, and in which a large number of probes are processed in bulk. We show how to improve probe performance by (a) eliminating branch instructions from the probe code, enabling better scheduling and latency-hiding by modern processors, and (b) using SIMD instructions to process multiple keys/payloads in parallel. We show that on modern architectures, probes to a bucketized Cuckoo hash table can be processed much faster than conventional hash table probes, for both small and large memory-resident tables. On a Pentium 4, a probe is two to four times faster, while on the Cell SPE processor a probe is ten times faster."
75BFEF41,International Conference on Data Engineering,beng chin ooi + jianzliong li + shensfei shi + m lupur,2007,Clustering wavelets to speed-up data dissemination in structured P2P MANETs,bandwidth + computer science + non volatile memory + nonvolatile memory + data dissemination + p2p + mobile communication + indexing + overlay network + mobile device + similarity search + multiresolution analysis + information analysis + k means clustering + mobile computing + public transport + ad hoc networks + k means,,"This paper introduces a fast data dissemination method for structured peer-to-peer networks. The work is motivated on one side by the increase in non-volatile memory available on mobile devices and, on the other side, by observed behavioral patterns of the users. We envision a scenario where users come together for short periods of time (e.g. public transport, conference sessions) and wish to be able to share large collections of data. With hundreds and even thousands of data items stored on small devices, content publication is simply too energy and time consuming. By indexing summary information obtained by a combination of multi-resolution analysis and k-means, our method (Hyper-M) is able to cut down the overall construction time of an overlay network such as CAN by an order of magnitude, as well as provide fast approximate similarity search on such a network. The results of our extensive experimental studies confirm that Hyper-M is both energy and time efficient, and provides good precision and recall. •_ 2007 IEEE."
7F3A7524,International Conference on Data Engineering,minos garofalakis + petros maniatis + joseph m hellerstein,2007,Proof Sketches: Verifiable In-Network Aggregation,sampling methods + data analysis + cryptographic signatures + web services + Flajolet-Martin sketches + worst-case bounds + distributed system + distributed processing + random sampling + cryptography + intrusion detection + central processing unit + query processing + compact verification + formal verification + proof sketches + distributed systems + digital signatures + verifiable in-network aggregation + data analysis queries + distributedaggregation,AuthorProvided Keywords Not Found,"A work on distributed, in-network aggregation assumes a benign population of participants. Unfortunately, modern distributed systems are plagued by malicious participants. In this paper we present a first step towards verifiable yet efficient distributed, in-network aggregation in adversarial settings. We describe a general framework and threat model for the problem and then present proof sketches, a compact verification mechanism that combines cryptographic signatures and Flajolet-Martin sketches to guarantee acceptable aggregation error bounds with high probability. We derive proof sketches for count aggregates and extend them for random sampling, which can be used to provide verifiable approximations for a broad class of data-analysis queries, e.g., quantiles and heavy hitters. Finally, we evaluate the practical use of proof sketches, and observe that adversaries can often be reduced to much smaller violations in practice than our worst-case bounds suggest."
8042FDAD,International Conference on Data Engineering,ihab f ilyas + amr elhelw + volker markl + wing lau + c zuzarte,2007,Collecting and Maintaining Just-in-Time Statistics,lightweight sensitivity analysis + prototypes + sensitivity analysis + dbms + statistics collection + database management systems + statistical distributions + cardinality estimation error + query processing + query optimization + cost function + query compilation + DBMS + just-in-time statistics + statistical analysis + query-specific statistics + statistics,AuthorProvided Keywords Not Found,"Traditional DBMSs decouple statistics collection and query optimization both in space and time. Decoupling in time may lead to outdated statistics. Decoupling in space may cause statistics not to be available at the desired granularity needed to optimize a particular query, or some important statistics may not be available at all. Overall, this decoupling often leads to large cardinality estimation errors and, in consequence, to the selection of suboptimal plans for query execution. In this paper, we present JITS, a system for proactively collecting query-specific statistics during query compilation. The system employs a lightweight sensitivity analysis to choose which statistics to collect by making use of previously collected statistics and database activity patterns. The collected statistics are materialized and incrementally updated for later reuse. We present the basic concepts, architecture, and key features of JITS. We demonstrate its benefits through an extensive experimental study on a prototype inside the IBM DB2 engine."
7E8ADBAC,International Conference on Data Engineering,bernhard seeger + michael cammert + christoph heinz + jurgen kramer + sonny vaupel + udo wolske,2007,Flexible Multi-Threaded Scheduling for Continuous Queries over Data Streams,resource management + graph theory + scheduling + data processing + cost function + scalability + application software + database management systems + multi threading,,"A variety of real-world applications share the property that data arrives in form of transient streams. Data stream management systems (DSMS) provide convenient solutions to the problem of processing continuous queries on those streams. Within a DSMS, the scheduling of the queries and their operators has proved to be of utmost importance. Previous approaches addressing this issue can be divided into two categories: either each operator runs in its own thread or all operators, combined in one query graph, run in a single thread. Both approaches suffer from severe drawbacks concerning the thread overhead on the one hand and the stalls due to expensive operators on the other hand. To overcome these drawbacks, we propose in this work a hybrid approach that flexibly assigns threads to subgraphs of the query graph. We complement this approach with a suitable strategy to determine these subgraphs. The results of an experimental study substantiate the feasibility of our approach and its superiority to previous approaches."
80B89E29,International Conference on Data Engineering,r l mishra + aditya telang + sharma chakravarthy,2007,Ranking Issues for Information Integration,usability + information integration + search engines + information retrieval systems + information retrieval system + information retrieval + computer science + database systems + internet + robustness,,"Ranking of query/search answers, although introduced by early information retrieval systems, has become mandatory for internet searches. When the answers of a query or search are varying in quality and are large in numbers, it is necessary to rank/order these answers based on some criteria. From a users' viewpoint, ranking is extremely useful especially when associated with the retrieval of a few (topk) answers. Currently, the notion of ranking is being applied to database query answers (ordering based on user criteria) in order to retrieve top-k answers. One of the challenges is to push ranking computation into the query processing stage to make it efficient and eliminate post processing operations. In this paper, we argue that top-k answers and ranking will become mandatory as well for searches that involve integration of information from heterogeneous domains. However, it is not clear how ranking can be supported as sources are autonomous and support different characteristics and capabilities. We analyze possible alternatives for supporting ranking while answering queries in this context and propose potential approaches. The discussion is in the context of InfoMosaic, a framework proposed by the authors for information integration."
7E61DD99,International Conference on Data Engineering,yannis velegrakis + divesh srivastava,2007,Using Queries to Associate Metadata with Data,database languages + meta data + data security + information security + algebra + information analysis + relational database + relational databases + data models + query processing + associate metadata + data structures + relational model + ontologies,AuthorProvided Keywords Not Found,"As relational databases proliferate and become increasingly complex, both in their internal structure and in their interactions with other databases and applications, there is a growing need to associate a variety of metadata with the underlying data. Even though the need has been apparent, a simple, elegant approach to uniformly model and query both data and metadata has been elusive. In this paper, we argue that the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their association."
7E693B69,International Conference on Data Engineering,prithviraj sen + a deshpande,2007,Representing and Querying Correlated Tuples in Probabilistic Databases,application software + database languages + probabilistic database + query semantics + technical report + query languages + query evaluation + machine learning + database management systems + uncertainty + possible worlds + query processing + correlated tuples querying + computer science + graphical models + correlated tuples representation + casting + probabilistic databases + query language + probabilistic graphical model,AuthorProvided Keywords Not Found,"Probabilistic databases have received considerable attention recently due to the need for storing uncertain data produced by many real world applications. The widespread use of probabilistic databases is hampered by two limitations: (1) current probabilistic databases make simplistic assumptions about the data (e.g., complete independence among tuples) that make it difficult to use them in applications that naturally produce correlated data, and (2) most probabilistic databases can only answer a restricted subset of the queries that can be expressed using traditional query languages. We address both these limitations by proposing a framework that can represent not only probabilistic tuples, but also correlations that may be present among them. Our proposed framework naturally lends itself to the possible world semantics thus preserving the precise query semantics extant in current probabilistic databases. We develop an efficient strategy for query evaluation over such probabilistic databases by casting the query processing problem as an inference problem in an appropriately constructed probabilistic graphical model. We present several optimizations specific to probabilistic databases that enable efficient query evaluation. We validate our approach by presenting an experimental evaluation that illustrates the effectiveness of our techniques at answering various queries using real and synthetic datasets."
7E9772B0,International Conference on Data Engineering,seekiong ng + jin chen + mong li lee + wynne hsu,2007,Labeling network motifs in protein interactomes for protein function prediction,protein protein interaction + shape + LaMoFinder + computer networks + protein-protein interaction network + complex networks + network topology + protein interactomes + frequency + biological networks + biological network + gene ontology + genetics + biology computing + protein function prediction + labeling + proteins + network motifs labeling + ontologies + network motif,AuthorProvided Keywords Not Found,"Biological networks such as the protein-protein interaction (PPI) network have been found to contain small recurring subnetworks in significantly higher frequencies than in random networks. Such network motifs are useful for uncovering structural design principles of complex biological networks. However, current network motif finding algorithms models the PPI network as a uni-labeled graph, discovering only unlabeled and thus relatively uninforma-tive network motifs as a result. Our objective is to exploit the currently available biological information that are associated with the vertices (the proteins) to capture not only the topological shapes of the motifs, but also the biological context in which they occurred in the PPI networks for network motif applications. We present a method called LaMoFinder to label network motifs with gene ontology terms in a PPI network. We also show how the resulting labeled network motifs can be used to predict unknown protein functions. Experimental results showed that the labeled network motifs extracted are biologically meaningful and can achieve better performance than existing PPI topology based methods for predicting unknown protein functions."
7E805271,International Conference on Data Engineering,xindong wu + xingquan zhu,2007,Discovering Relational Patterns across Multiple Databases,relational pattern discovery + pattern analysis + data analysis + DRAMA + data mining + trees (mathematics) + association rules + hybrid frequent pattern tree + relational databases + data collection + drama + computer science + satisfiability + multiple databases,AuthorProvided Keywords Not Found,"Relational patterns across multiple databases can reveal special pattern relationships hidden inside data collections. Existing research in data mining has made significant efforts in discovering different types of patterns from single or multiple databases, but how to find patterns that have a higher support in database A than in database B with a given support threshold a is still an open problem. We propose in this paper DRAMA, a systematic framework for discovering relational patterns across multiple databases. More specifically, given a series of data collections, we try to discover patterns from different databases with patterns' relationships satisfying the user specified constraints. Our method seeks to build a hybrid frequent pattern tree (HFP-tree) from multiple databases, and mine patterns from the HFP-tree by integrating users' constraints into the pattern mining process."
7D41723C,International Conference on Data Engineering,philippe rigaux + witold litwin + c du mouza,2007,SD-Rtree: A Scalable Distributed Rtree,workstation clusters + indexing + large spatial datasets + calculus + window queries + query processing + distributed balanced binary spatial tree + SD-Rtree + data structures + tree data structures + redundancy + scalable distributed Rtree + error correction + scalable distributed data structure,AuthorProvided Keywords Not Found,"We propose a scalable distributed data structure (SDDS) called SD-Rtree. We intend our structure for point and window queries over possibly large spatial datasets distributed on clusters of interconnected servers. SD-Rtree generalizes the well-known Rtree structure. It uses a distributed balanced binary spatial tree that scales with insertions to potentially any number of storage servers through splits of the overloaded ones. A user/application manipulates the structure from a client node. The client addresses the tree through its image that the splits can make outdated. This may generate addressing errors, solved by the forwarding among the servers. Specific messages towards the clients incrementally correct the outdated images."
7FC20423,International Conference on Data Engineering,h v jagadish + kianlee tan + yingguang li,2007,SPRITE: A Learning-Based Text Retrieval System in DHT Networks,text analysis + progressive index tuning by examples + peer-to-peer computing + indexing + bandwidth + information retrieval + indexation + learning-based text retrieval system + SPRITE + queries + routing + retrieval effectiveness + sprite + DHT network + structured P2P network,AuthorProvided Keywords Not Found,"In this paper, we propose SPRITE (selective progressive index tuning by examples), a scalable system for text retrieval in a structured P2P network. Under SPRITE, each peer is responsible for a certain number of terms. However, for each document, SPRITE learns from (past) queries to select only a small set of representative terms for indexing; and these terms are progressively refined with subsequent queries. We implemented the proposed strategy, and compare its retrieval effectiveness in terms of both precision and recall against a static scheme (without learning) and a centralized system (ideal). Our experimental results show that SPRITE is nearly as effective as the centralized system, and considerably outperforms the static scheme."
815917A9,International Conference on Data Engineering,karl aberer + ivana podnar + toan luu + martin rajman + fabius klemm,2007,Scalable Peer-to-Peer Web Retrieval with Highly Discriminative Keys,document handling + cost-efficient retrieval + highly discriminative keys + peer-to-peer Web retrieval + peer-to-peer computing + prototypes + indexing + bandwidth + information retrieval + indexation + distributed global index + scalability + p2p + Web-size document collections + full-text databases + full-text Web retrieval + engines + scalability analysis + Internet + cost efficiency + internet + structured P2P network,AuthorProvided Keywords Not Found,"The suitability of peer-to-peer (P2P) approaches for full-text Web retrieval has recently been questioned because of the claimed unacceptable bandwidth consumption induced by retrieval from very large document collections. In this contribution we formalize a novel indexing/retrieval model that achieves high performance, cost-efficient retrieval by indexing with highly discriminative keys (HDKs) stored in a distributed global index maintained in a structured P2P network. HDKs correspond to carefully selected terms and term sets appearing in a small number of collection documents. We provide a theoretical analysis of the scalability of our retrieval model and report experimental results obtained with our HDK-based P2P retrieval engine. These results show that, despite increased indexing costs, the total traffic generated with the HDK approach is significantly smaller than the one obtained with distributed single-term indexing strategies. Furthermore, our experiments show that the retrieval performance obtained with a random set of real queries is comparable to the one of centralized, single-term solution using the best state-of-the-art BM25 relevance computation scheme. Finally, our scalability analysis demonstrates that the HDK approach can scale to large networks of peers indexing Web-size document collections, thus opening the way towards viable, truly-decentralized Web retrieval."
7FFF04AD,International Conference on Data Engineering,serge abiteboul + neoklis polyzotis + tova milo + karl schnaitter,2007,On-Line Index Selection for Shifting Workloads,resource management +  + physical design + data analysis + investments + sql + database system + database indexing + level of detail + control systems + cost function + throughput + statistics + database systems,Neoklis Polyzotis_,"This paper introduces COLT (Continuous On-Line Tuning), a novel framework that continuously monitors the workload of a database system and enriches the existing physical design with a set of effective indices. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to self-regulate its own performance, lowering its overhead when the system is well tuned and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We describe an implementation of the proposed framework in the PostgreSQL database system and evaluate its performance experimentally. Our results validate the effectiveness of COLT and demonstrate its ability to modify the system configuration in response to changes in the query load."
7E7A0E0E,International Conference on Data Engineering,f silvestri + ricardo baezayates + vassilis plachouras + fabricio junqueira + claris castillo,2007,Challenges on Distributed Web Retrieval,query throughput + search engines + web pages + centralized system + search engine + distributed Web retrieval system + distributed search engine + information retrieval + indexation + scalability + Web search engine + replicated cluster + web search engine + throughput + Internet + Web sites + internet + Web data + hardware,AuthorProvided Keywords Not Found,"In the ocean of Web data, Web search engines are the primary way to access content. As the data is on the order of petabytes, current search engines are very large centralized systems based on replicated clusters. Web data, however, is always evolving. The number of Web sites continues to grow rapidly and there are currently more than 20 billion indexed pages. In the near future, centralized systems are likely to become ineffective against such a load, thus suggesting the need of fully distributed search engines. Such engines need to achieve the following goals: high quality answers, fast response time, high query throughput, and scalability. In this paper we survey and organize recent research results, outlining the main challenges of designing a distributed Web retrieval system."
7F997873,International Conference on Data Engineering,ashraf aboulnaga + kenneth salem + ahmed a soror,2007,Database Virtualization: A New Frontier for Database Tuning and Physical Design,resource management + software systems + application software + hardware + virtual environment + physical design + virtual machine + resource allocation + database tuning + database systems + query optimizer + database management systems + database system + query optimization + virtual machines + operating systems,,"Resource virtualization is currently being employed at all levels of the IT infrastructure to improve provisioning and manageability, with the goal of reducing total cost of ownership. This means that database systems will increasingly be run in virtualized environments, inside virtual machines. This has many benefits, but it also introduces new tuning and physical design problems that are of interest to the database research community. In this paper, we discuss how virtualization can benefit database systems, and we present the tuning problems it introduces, which relate to setting the new _tuning knobsî that control resource allocation to virtual machines in the virtualized environment. We present a formulation of the virtualization design problem, which focuses on setting resource allocation levels for different database workloads statically at deployment and configuration time. An important component of the solution to this problem is modeling the cost of a workload for a given resource allocation. We present an approach to this cost modeling that relies on using the query optimizer in a special virtualization-aware _what-if î mode. We also discuss the next steps in solving this problem, and present some long-term research directions."
7F4FA849,International Conference on Data Engineering,bulent tastan + pinar yolum + feyza merve isik,2007,Automatic Adaptation of BPEL Processes Using Semantic Rules: Design and Development of a Loan Approval System,semantic web + application software + business process execution language + web services + engines + protocols + information security + reasoning engine,,"Dynamic service composition is an important challenge, for many applications. This paper considers dynamic composition where the parties involved and the process they execute changes based on context. In such a setting, creating compositions from scratch is costly. Instead, we advocate automatic adaptation of existing compositions to fit the requested service demands. Our approach starts from existing BPEL processes. The details and constraints on the environment are expressed by semantic rules. We automatically decide which services are necessary to complete a desired service in a given BPEL process and which services can be removed from, the process by using a reasoning engine. Based on this information, our flow generator modifies existing BPEL processes to derive effective, variations. The newly generated BPEL process is then executed. We study this approach in the context of a loan approval system and show how the modifications can be done on example scenarios. © 2007 IEEE."
76325229,International Conference on Data Engineering,xiang lian + lei chen + ge yu + guoren wang + jeffrey xu yu,2007,Similarity Match Over High Speed Time-Series Streams,demand forecasting + pattern matching + data analysis + data management + high speed time-series streams + stream data management + information retrieval + time series + information filtering + query time series data + multistep filtering + similarity-based time series retrieval + query processing + pattern time series data + technology forecasting + weather forecasting + stream processing + media streaming + time series data + search space + multiscaled segment mean + similarity match + filtering,AuthorProvided Keywords Not Found,"Similarity-based time series retrieval has been a subject of long term study due to its wide usage in many applications, such as financial data analysis, weather data forecasting, and multimedia data retrieval. Its original task was to find those time series similar to a pattern (query) time series data, where both the pattern and data time series are static. Recently, with an increasing demand on stream data management, similarity-based stream time series retrieval has raised new research issues due to its unique requirements during the stream processing, such as one-pass search and fast response. In this paper, we address the problem of matching patterns over high-speed stream time series data. We will develop a novel representation, called multi-scaled segment mean (MSM), for stream time series data, which can be incrementally computed and thus perfectly adapted to the stream characteristics. Most importantly, we propose a novel multi-step filtering mechanism over the multi-scaled representation. Analysis indicates that the mechanism can greatly prune the search space and thus offer fast response. Extensive experiments show the multi-scaled representation together with the multi-step filtering scheme can efficiently filter out false candidates and detect patterns, compared to the multiscaled wavelet."
7EA25D07,International Conference on Data Engineering,jun yang + adam silberstein,2007,Many-to-Many Aggregation for Sensor Networks,in-network aggregation + sampling methods + wireless sensor networks + multicast aggregation + sensor network + many-to-many aggregation + wireless sensor network + cost function + in-network sensor control + intelligent sensors + data collection + wildlife,AuthorProvided Keywords Not Found,"Wireless sensor networks have enormous potential to aid data collection in a number of areas, such as environmental and wildlife research. In this paper, we address the challenges of supporting many-to-many aggregation in a sensor network. An application of many-to-many aggregation is in-network control of sensors. For expensive sensing tasks such as sap flux measurements and camera repositioning, we use low-cost information obtained at multiple other nodes in the network to control such tasks, e.g., decreasing sampling rates when readings are predictable or unimportant, while increasing sampling rates when there are interesting activities. In general, there is a many-to-many relationship between sources (nodes providing control inputs) and destinations (nodes requiring control outputs). We present a method for implementing many-to-many aggregation in a sensor network that minimizes the communication cost by optimally balancing a combination of multicast and in-network aggregation. Our optimization technique is efficient in finding the initial solution and handling dynamic updates."
7CF07382,International Conference on Data Engineering,alexey pryakhin + matthias schubert + christian bohm + michael j gruber + peter kunath,2007,ProVeR: Probabilistic Video Retrieval using the Gauss-Tree,Gauss-tree + databases + search engines + prototypes + power method + search engine + probability + trees (mathematics) + information retrieval + visual databases + gaussian distribution + probability density functions + motion pictures + content-based retrieval + decoding + layout + probabilistic video retrieval + gaussian processes + video retrieval + object representation + Gaussian processes + content-based video retrieval + probabilistic queries + probability density function,AuthorProvided Keywords Not Found,"Modeling objects by probability density functions (pdf) is a new powerful method to represent complex objects in databases. By representing an object as a pdf e.g. a Gaussian, it is possible to represent very large and complex objects in a compact and still descriptive way. In this contribution, we propose ProVeR a prototype search engine for content-based video retrieval which represents a video as a set of Gaussians. The Gaussians are managed by the Gauss-tree, an index structure allowing the efficient processing of probabilistic queries. ProVeR provides even non-expert users with an intuitive method for efficient, content-based retrieval of videos containing similar shots and scenes."
760AC9CF,International Conference on Data Engineering,zhengdao xu + hansarno jacobsen,2007,Evaluating Proximity Relations Under Uncertainty,location based services + location based service + upper bound + uncertainty handling + uncertainty + statistical distributions + frequency + mobile computing + proximity relations + computer science + mobile objects + proximity relation + cost effectiveness + probability distribution + location-based services + cost-effective estimation,AuthorProvided Keywords Not Found,"For location-based services it is often essential to efficiently process proximity relations among mobile objects, such as to establish whether a group of friends or family members are within a given distance of each other A severe limitation in accurately establishing such relations is the inaccuracy of dynamically obtained position data, the point in time, and the frequency with which the position data is collected. In this paper, we use the common model of interpreting the unknown position of an object by a probability distribution centered around the last know position of the object. While this approach is straight forward, it poses severe difficulties for establishing the truth or falsehood of the proximity relation. To address this problem, we analytically quantify the lower and upper bounds of the size of the smallest circle that covers the mobile objects involved in the proximity relation. Based on this result we propose two novel algorithms that closely monitor the relation at low location update cost. Furthermore, we develop a cost-effective estimation technique to determine the probability of match for a given proximity relation."
7F5AE333,International Conference on Data Engineering,james a shine + james p rogers + shashi shekhar + james m kang + mete celik,2007,Mining At Most Top-K% Mixed-drove Spatio-temporal Co-occurrence Patterns: A Summary of Results,algorithm design and analysis + application software + strategic planning + computer science + algae + data mining,,"Mixed-drove spatio-temporal co-occurrence patterns (MDCOPs) represent subsets of object-types that are located together in space and time. Discovering MDCOPs is an important problem with many applications such as planning battlefield tactics, and tracking predator-prey interactions. However, determining suitable interest measure thresholds is a difficult task. In this paper, we define the problem of mining at most top-K% MDCOPs without using userdefined thresholds and propose a novel at most topK% MDCOP mining algorithm. Analytical and experimental results show that the proposed algorithm is correct and complete. Results show the proposed method is computationally more efficient than na‰ve alternatives."
7F3F4E4C,International Conference on Data Engineering,francois bry + paulalavinia patranjan + hendrik grallert + melanie eckert,2007,Evolution of Distributed Web Data: An Application of the Reactive Language XChange,concrete + database languages + distributed Web data + data mining + query languages + informatics + distributed Web sites + tree graphs + resource description framework + query processing + XChange language + xml + Internet + internet,AuthorProvided Keywords Not Found,"Many data sources on the Web evolve: they change their content over time, typically as reactions to events. Such changes often need to be mirrored in data on other Web nodes: updates need to be propagated. To respond to the need for evolution and reactivity both locally and globally, the language XChange has been developed. We demonstrate its applicability in a concrete scenario of distributed Web sites of a scientific community with mutual data dependencies."
7F736FCA,International Conference on Data Engineering,ihab f ilyas + mohamed a soliman + k chenchuan chang,2007,Top-k Query Processing in Uncertain Databases,radar tracking + databases + top-k query processing + computational linguistics + probabilistic formulations + possible worlds semantics + uncertainty + possible worlds + top-k semantics + voltage + uncertain databases + data models + query processing + uncertain data settings + computer science + state space model,AuthorProvided Keywords Not Found,"Top-k processing in uncertain databases is semantically and computationally different from traditional top-k processing. The interplay between score and uncertainty makes traditional techniques inapplicable. We introduce new probabilistic formulations for top-k queries. Our formulations are based on ""marriage"" of traditional top-k semantics and possible worlds semantics. In the light of these formulations, we construct a framework that encapsulates a state space model and efficient query processing techniques to tackle the challenges of uncertain data settings. We prove that our techniques are optimal in terms of the number of accessed tuples and materialized search states. Our experiments show the efficiency of our techniques under different data distributions with orders of magnitude improvement over naive materialization of possible worlds."
7F0190C6,International Conference on Data Engineering,jinfeng ni + chinya v ravishankar,2007,Pointwise-Dense Region Queries in Spatio-temporal Databases,resource management + pointwise-dense region queries + application software + databases + location based services + location based service + space technology + visual databases + engineering management + data engineering + air traffic control + mobile computing + computer science + temporal databases + location-based services + spatio-temporal databases,AuthorProvided Keywords Not Found,"Applications such as traffic management and resource scheduling for location-based services commonly need to identify regions with high concentrations of moving objects. Such queries are called dense region queries in spatio-temporal databases, and desire regions in which the density of moving objects exceeds a given threshold. Current methods for addressing this important class of queries suffer from several drawbacks. For example, they may fail to find all dense regions, provide ambiguous answers, impose restrictions on size, or lack a notion of local density. We address these issues in this paper, starting with a new definition of dense regions. We show that we are able to answer dense region queries completely and uniquely using this definition. Dense regions in our approach may have arbitrary shape and size, as well as local density guarantees. We present two methods, the first, an exact method, and the second, an approximate method. We demonstrate through extensive experiments that our exact method is efficient and is superior to current approaches. Our approximate method runs orders of magnitude faster than our exact method, at the cost of a tolerable loss of accuracy."
7FE254CB,International Conference on Data Engineering,feida zhu + jiawei han + hong cheng + philip s yu + xifeng yan,2007,Mining Colossal Frequent Patterns by Core Pattern Fusion,information system + data analysis + indexing + scientific computing + data mining + association rules + empirical study + approximation algorithms + explosives + colossal pattern + colossal frequent pattern mining + core pattern fusion + clustering algorithms + bioinformatics + numerical analysis + pattern recognition,AuthorProvided Keywords Not Found,"Extensive research for frequent-pattern mining in the past decade has brought forth a number of pattern mining algorithms that are both effective and efficient. However, the existing frequent-pattern mining algorithms encounter challenges at mining rather large patterns, called colossal frequent patterns, in the presence of an explosive number of frequent patterns. Colossal patterns are critical to many applications, especially in domains like bioinformatics. In this study, we investigate a novel mining approach called pattern-fusion to efficiently find a good approximation to the colossal patterns. With Pattern-Fusion, a colossal pattern is discovered by fusing its small core patterns in one step, whereas the incremental pattern-growth mining strategies, such as those adopted in Apriori and FP-growth, have to examine a large number of mid-sized ones. This property distinguishes pattern-fusion from all the existing frequent pattern mining approaches and draws a new mining methodology. Our empirical studies show that, in cases where current mining algorithms cannot proceed, pattern-fusion is able to mine a result set which is a close enough approximation to the complete set of the colossal patterns, under a quality evaluation model proposed in this paper."
7FA730D4,International Conference on Data Engineering,josep domingo ferrer + agusti solanas + a m balleste + j sanz,2007,A Genetic Approach to Multivariate Microaggregation for Database Privacy,genetic engineering + data engineering + data privacy + information analysis + genetic algorithms + location based service + mobile computing + genetics + public policy + database management systems + genetic algorithm + databases,,"Microdata, i.e. sets of records containing information on individual respondents, are important for planning or analysis purposes in a number of human activities: healthcare, market analysis, public policies, etc. As a consequence, statistical agencies and large enterprises routinely gather such microdata. However, this information cannot be freely released because the privacy of the respondents would be jeopardized. Against common belief, suppressing the direct identifiers (names, passport numbers, etc.) is not enough to ensure respondent privacy: e.g. if a data set contains civil status, age and sensitive information (salary, diseases, etc.) for a number of people, a 17-year"
7F59826C,International Conference on Data Engineering,sarah pramanik + qiang zhu + diana l kolbe,2007,On k-Nearest Neighbor Searching in Non-Ordered Discrete Data Spaces,k-nearest neighbor searching + indexing + k-nearest neighbor query retrieval + information retrieval + indexation + hamming distance + granularity-enhanced Hamming distance + sequences + multidimensional systems + query processing + nonordered discrete data space + coarse granularity + database indexing + k nearest neighbor + dna + bioinformatics + genomics + multidimensional database + electronic commerce + multidimensional database indexing,AuthorProvided Keywords Not Found,"A k-nearest neighbor (k-NN) query retrieves k objects from a database that are considered to be the closest to a given query point. Numerous techniques have been proposed in the past for supporting efficient k-NN searches in continuous data spaces. No such work has been reported in the literature for k-NN searches in a non-ordered discrete data space (NDDS). Performing k-NN searches in an NDDS raises new challenges. The Hamming distance is usually used to measure the distance between two vectors (objects) in an NDDS. Due to the coarse granularity of the Hamming distance, a k-NN query in an NDDS may lead to a large set of candidate solutions, creating a high degree of non-determinism for the query result. We propose a new distance measure, called granularity-enhanced Hamming (GEH) distance, that effectively reduces the number of candidate solutions for a query. We have also considered using multidimensional database indexing for implementing k-NN searches in NDDSs. Our experiments on synthetic and genomic data sets demonstrate that our index-based k-NN algorithm is effective and efficient in finding k-NNs in NDDSs."
80231696,International Conference on Data Engineering,walid g aref + chiyin chow + mohamed f mokbel,2007,The New Casper: A Privacy-Aware Location-Based Database Server,privacy-aware location-based database server + database management system + privacy-aware query processor + prototypes + data security + location based service + location anonymizer + information security + privacy + Casper + database management systems + data engineering + PLACE + process engineering + query processing + mobile computing + computer science + file servers + registers + data privacy + place,AuthorProvided Keywords Not Found,"This demo presents Casper; a framework in which users entertain anonymous location-based services. Casper consists of two main components; the location anonymizer that blurs the users' exact location into cloaked spatial regions and the privacy-aware query processor that is responsible on providing location-based services based on the cloaked spatial regions. While the location anonymizer is implemented as a stand alone application, the privacy-aware query processor is embedded into PLACE; a research prototype for location-based database servers."
7643F93A,International Conference on Data Engineering,ganesh ramesh + raymond t ng + laks v s lakshmanan + shaofeng bu,2007,Preservation Of Patterns and Input-Output Privacy,pattern preservation + decision tree + input-output privacy + data mining + biomarkers + random perturbation + decision trees + data privacy + input output + decoding + privacy preserving data mining,AuthorProvided Keywords Not Found,"Privacy preserving data mining so far has mainly focused on the data collector scenario where individuals supply their personal data to an untrusted collector in exchange for value. In this scenario, random perturbation has proved to be very successful. An equally compelling, but overlooked scenario, is that of a data custodian, which either owns the data or is explicitly entrusted with ensuring privacy of individual data. In this scenario, we show that it is possible to minimize disclosure while guaranteeing no outcome change. We conduct our investigation in the context of building a decision tree and propose transformations that preserve the exact decision tree. We show with a detailed set of experiments that they provide substantial protection to both input data privacy and mining output privacy."
8161238B,International Conference on Data Engineering,c yu + weiyi meng + hai he + yiyao lu + hongkun zhao,2007,Annotating Structured Data of the Deep Web,multiannotator approach + search engines + web accessibility + data mining + annotation wrapper + information retrieval + helium + relational databases + data collection + HTML form-based search interfaces + query processing + structured data + deep Web data collection + html + Internet + deep web + internet,AuthorProvided Keywords Not Found,"An increasing number of databases have become Web accessible through HTML form-based search interfaces. The data units returned from the underlying database are usually encoded into the result pages dynamically for human browsing. For the encoded data units to be machine processable, which is essential for many applications such as deep Web data collection and comparison shopping, they need to be extracted out and assigned meaningful labels. In this paper, we present a multi-annotator approach that first aligns the data units into different groups such that the data in the same group have the same semantics. Then for each group, we annotate it from different aspects and aggregate the different annotations to predict a final annotation label. An annotation wrapper for the search site is automatically constructed and can be used to annotate new result pages from the same site. Our experiments indicate that the proposed approach is highly effective."
7C1614E4,International Conference on Data Engineering,yijian bai + shaorong liu + carlo zaniolo + fusheng wang + peiya liu,2007,RFID Data Processing with a Data Stream Query Language,database languages + application software + temporal operators + radio frequency + temporal event detection + radiofrequency identification + power system + data processing + query languages + RFID technology + sliding-window constructs + sliding window + data stream query language + query processing + object-tracking technologies + computer science + rfid tags + RFID data processing + object tracking + data stream management system + query language,AuthorProvided Keywords Not Found,"RFID technology provides significant advantages over traditional object-tracking technologies and is increasingly adopted and deployed in real applications. RFID applications generate large volume of streaming data, which have to be automatically filtered, processed, and transformed into semantic data, and integrated into business applications. Indeed, RFID data are highly temporal, and RFID observations form complex temporal event patterns which can be very different for various RFID applications. Thus, it is desirable to have a general RFID data processing framework with a powerful language, for the end users to express a variety of queries on RFID data streams, as well as detecting complex events patterns. While data stream management systems (DSMSs) are emerging for optimized stream data processing, they usually lack the language construct support for temporal event detection. In this paper, we discuss a stream query language to provide comprehensive temporal event detection, through temporal operators and extension of sliding-window constructs. With the integration of temporal event detection, a DSMS has the capability to serve as a powerful system for RFID data processing."
7D483BE6,International Conference on Data Engineering,georgia koutrika + yannis ioannidis + alkis simitsis,2007,Generalized Pr_cis Queries for Logical Database Subset Creation,relational databases + boolean algebra + database languages + data mining + database systems + relational database,,"As a large fraction of available information resides in databases, the need for facilitating access for the large majority of users becomes increasingly more important. Pr_cis queries are free-form queries that generate entire multirelation databases, which are logical subsets of existing ones. A logical subset contains not only items directly related to the given query selections but also items implicitly related to them in various ways with the purpose of providing to the user much greater insight into the original data. This paper is concerned with the definition and generation of logical database subsets based on pr_cis queries under a generalized perspective that removes several restrictions of previous work and handles queries containing multiple terms combined using the operators AND, OR, and NOT. © 2007 IEEE."
7F9E7568,International Conference on Data Engineering,gokul soundararajan + jin chen + cristiana amza + madalin mihailescu,2007,Outlier Detection for Fine-grained Load Balancing in Database Clusters,resource allocation + engines + database systems + outlier detection + file servers + load balance + data centers + database management systems + internet + resource management + hardware + service level + database system,,"Recent industry trends towards reducing the costs of ownership in large data centers emphasize the need for database system techniques for both automatic performance tuning and efficient resource usage. The goal is to host several database applications on a shared server farm, including scheduling multiple applications on the same physical server or even within a single database engine, while meeting each applications service level agreement. Automatic provisioning of database servers to applications and virtualization techniques, such us, live virtual machine migration have been proposed us useful tools to address this problem. In this paper we argue that by allocating entire server boxes and migrating entire application stacks in cases of server overload, these solutions are too coarse-grained for many overload situations. Hence, they may result in resource usage inefficiency, performance penalties, or both. We introduce an outlier detection algorithm which zooms in to the fine-grained query contexts which are most affected by an environment change and/or where a perceived overload problem is likely to originate from. We show that isolating these query contexts through either memory quota enforcements or fine-grained load balancing across different database replicas of their respective applications allows us to alleviate resource interference in many cases of overload. © 2007 IEEE."
7DEFF34B,International Conference on Data Engineering,kostas gratsias + yannis theodoridis + elias frentzos,2007,Index-based Most Similar Trajectory Search,R-tree-like structures + database indexing + similarity search + spatial database + spatiotemporal database + index-based most similar trajectory search + indexation + visual databases + moving object databases + spatiotemporal trajectory similarity search + general-purpose spatiotemporal index + tree data structures,AuthorProvided Keywords Not Found,"The problem of trajectory similarity in moving object databases is a relatively new topic in the spatial and spatiotemporal database literature. Existing work focuses on the spatial notion of similarity ignoring the temporal dimension of trajectories and disregarding the presence of a general-purpose spatiotemporal index. In this work, we address the issue of spatiotemporal trajectory similarity search by defining a similarity metric, proposing an efficient approximation method to reduce its calculation cost, and developing novel metrics and heuristics to support k-most-similar-trajectory search in spatiotemporal databases exploiting on existing R-tree-like structures that are already found there to support more traditional queries. Our experimental study, based on real and synthetic datasets, verifies that the proposed similarity metric efficiently retrieves spatiotemporally similar trajectories in cases where related work fails, while at the same time the proposed algorithm is shown to be efficient and highly scalable."
7FCBF535,International Conference on Data Engineering,ling liu + bugra gedik + kimlung wu + philip s yu,2007,"Lira: Lightweight, Region-aware Load Shedding in Mobile CQ Systems",query processing + lightweight region-aware load shedding + mobile computing + resource allocation + mobile CQ systems + position-update load + wireless networks + mobile nodes + wireless communication + middleware + LIRA + location-based continual queries,AuthorProvided Keywords Not Found,"To provide high-quality results for location-based, continual queries (CQs) in a mobile system, the query processor usually demands receiving frequent position updates from the mobile nodes. However, processing frequent updates often causes the query processor to become overloaded, under which updates must be dropped randomly, bringing down the query-result accuracy and negating the benefits of frequent updates. In this paper, we develop LIRA - a lightweight, region-aware load-shedding technique for preventively reducing the position-update load of a query processor, while maintaining high-quality query results. Instead of receiving too many updates and then randomly dropping some of them, LIRA uses a region-aware partitioning mechanism to identify the most beneficial shedding regions to cut down the position updates sent by the mobile nodes within those regions. Based on the densities of mobile nodes and queries in a region, LIRA judiciously applies different amounts of update reduction for different regions, aiming to minimize the negative impacts of load shedding on query-result accuracy. Experimental results show that LIRA is vastly superior to random update dropping and clearly outperforms other alternatives that do not possess region-aware load-shedding capabilities. Moreover, due to its lightweight nature, LIRA introduces very little overhead."
817A74F3,International Conference on Data Engineering,xiaohui gu + philip s yu + haixun wang,2007,Adaptive Load Diffusion for Multiway Windowed Stream Joins,adaptive load diffusion + prototypes + diffusion operator + fine-grained load balancing + distributed processing + semantics-preserving tuple routing + intrusion detection + image analysis + routing + multiway windowed stream joins + resource allocation + clustering algorithms + stream processing + distributed host + load balance + dynamic stream environment + cluster system + parallel join processing,AuthorProvided Keywords Not Found,"In this paper, we present an adaptive load diffusion operator to enable scalable processing of multiway windowed stream joins (MWSJs) using a cluster system. The load diffusion is achieved by a set of novel semantics-pre serving tuple routing algorithms. Different from previous work, the load diffusion operator can (1) preserve the MWSJ semantics while spreading tuples to different hosts for parallel join processing; (2) achieve fine-grained load balancing among distributed hosts; and (3) perform semantics-preserving online adaptations to maintain optimal performance in dynamic stream environments. We have implemented a prototype of the distributed MWSJ framework on top of the System S distributed stream processing system. Our experiment results based on both real data streams and synthetic workloads show that the load diffusion algorithms can efficiently scale-up the performance of MWSJ processing with low overhead."
80A66ECC,International Conference on Data Engineering,angela nicoara + gustavo alonso,2007,Making Applications Persistent at Run-time,application software + compile time + relational databases + java + computer science + engines + nonvolatile memory + testing + data flow analysis,,"Persistence is a common requirement in many applications. In existing systems, persistence is added to an application at either compile or deployment time by using a variety of mechanisms. In this paper we extend the notion of orthogonal persistence to make it dynamic: persistence becomes not only an orthogonal concern but one that can be added to an application at run-time without interrupting its operations. •_ 2007 IEEE."
7FE48E81,International Conference on Data Engineering,feifei li + jimeng sun + g mihaila + spiros papadimitriou + ioana stanoi,2007,Hiding in the Crowd: Privacy Preservation on Evolving Streams through Correlation Tracking,multivariate streams + autocorrelation structure + correlation tracking + data analysis + algorithm design and analysis + privacy preservation + data mining + mathematical analysis + evolving streams + autocorrelation + data models + random perturbation + data privacy + publishing,AuthorProvided Keywords Not Found,"We address the problem of preserving privacy in streams, which has received surprisingly limited attention. For static data, a well-studied and widely used approach is based on random perturbation of the data values. However, streams pose additional challenges. First, analysis of the data has to be performed incrementally, using limited processing time and buffer space, making batch approaches unsuitable. Second, the characteristics of streams evolve over time. Consequently, approaches based on global analysis of the data are not adequate. We show that it is possible to efficiently and effectively track the correlation and autocorrelation structure of multivariate streams and leverage it to add noise which maximally preserves privacy, in the sense that it is very hard to remove. Our techniques achieve much better results than previous static, global approaches, while requiring limited processing time and memory. We provide both a mathematical analysis and experimental evaluation on real data to validate the correctness, efficiency, and effectiveness of our algorithms."
858F83CE,International Conference on Data Engineering,stratos papadomanolakis + anastassia ailamaki,2007,An Integer Linear Programming Approach to Database Design,branch and bound + lp relaxation + linear optimization + linear program + query optimization + database system + indexation + database design + combinatorial optimization + cost estimation,,"Existing Index selection tools rely on heuristics to efficiently search within the large space of alternative solutions and to minimize the overhead of using the query optimizer for cost estimation. Index selection heuristics, despite being practical, are hard to analyze and formally compute how close they get to the optimal solution. In this paper we propose a model for index selection based on Integer Linear Programming (ILP). The ILP formulation enables a wealth of combinatorial optimization techniques for providing quality guarantees, approximate solutions and even for computing optimal solutions. We present a system architecture for ILP-bassd index selection, in the context of commercial database systems. Our ILP-based approach offers higher solution quality, efficiency and scalability without sacrificing any of the precision offered by existing index selection tools. © 2007 IEEE."
7CF397D2,International Conference on Data Engineering,jens teubner + torsten grust + jan rittinger,2007,eXrQuy: Order Indifference in XQuery,order indifference + query languages + algebra + XQuery order semantics + relational XQuery compiler + programming language semantics + fn:unordered() + concatenated codes + Pathfinder + eXrQuy + engines + xml + XQuery expressions + tree data structures + database systems,AuthorProvided Keywords Not Found,"There are more spots than immediately obvious in XQuery expressions where order is immaterial for evaluation - this affects most notably, but not exclusively, expressions in the scope of unordered {} and the argument of fn:unordered(). Clearly, performance gains are lurking behind such expression contexts but the prevalent impact of order on the XQuery semantics reaches deep into any compliant XQuery processor, making it non-trivial to set this potential free. Here, we describe how the relational XQuery compiler Pathfinder uniformly exploits such order indifference in a purely algebraic fashion: Pathfinder-emitted plans faithfully implement the required XQuery order semantics but (locally) ignore order wherever this is admitted."
81624B44,International Conference on Data Engineering,jignesh m patel + h v jagadish + stelios paparizos,2007,SIGOPT: Using Schema to Optimize XML Query Processing,databases + schema information graph + pattern matching + constraint optimization + XML query optimization + alternate paths + space exploration + tree graphs + query processing + optimisation + query optimization + XML query processing + xml + XML + XML data + phase detection,AuthorProvided Keywords Not Found,"There has been a great deal of work in recent years on processing and optimizing queries against XML data. Typically in these previous works, schema information is not considered, so that evaluation techniques can continue to be used even in the absence of one. However, schema information is often available and, in this paper, we show that when available it can be exploited to great advantage in ways that complement ""traditional"" XML query optimization. To be usable in practice, we require that aspects of schema, essential for our purposes, be captured in a schema information graph (SIG). We exploit such meta-data knowledge with a preprocessing enumeration phase that detects potentially interchangeable evaluation units - we call such units alternate paths. We show, within an algebraic framework, methods that can break down a pattern tree into elementary paths and substitute them by one or more less costly alternate paths. This approach allows us to present various rewritten forms of the XML query to the query optimizer, and allows the DBMS to explore a larger space of query evaluation plans. We assessed the benefits of the proposed techniques experimentally with the XMark data set and show that the SIG-based optimizations can result in significant performance improvements."
81652F08,International Conference on Data Engineering,flavio rizzolo + xin gu + yaron kanza + mariano p consens,2007,"Self Managing Top-k (Summary, Keyword) Indexes in XML Retrieval",indexing + filtering + information retrieval + xml + indexation + algebra + satisfiability + structural elements,,"Retrieval queries that combine structural constraints with keyword search represent a significant challenge to XML data management systems. Queries are expected to be answered as efficiently and effectively as in traditional keyword search, while satisfying additional constraints. Several XML-retrieval systems support answering queries exhaustively by storing both structural indexes and a keyword index. Other systems answer top-k queries efficiently by constructing indexes in which keyword scores, for some structural elements, are stored in relevance order, enabling approaches such as the threshold algorithm (TA). In this paper we describe TReX, an XML retrieval system that can exploit multiple structural summaries (including newly defined ones). TReX can also self-manage small, redundant, indexes to speed up the evaluation of workloads of top-k queries. The redundant indexes are maintained to enable TReX to select an evaluation strategies among three (and potentially more) retrieval methods. We provide experimental evidence that using several strategies improves the efficiency of query evaluation, since none of the retrieval methods outperforms the others in all cases."
7EB35867,International Conference on Data Engineering,l m de campos + miguel a ruedamorales + juan f huete + juan m fernandezluna,2007,Group Recommending: A methodological Approach based on Bayesian Networks,recommender systems + bayesian methods + motion pictures + bayesian network + groupware + filtering + probability distribution + computer networks + recommender system + voting + collaboration,,"The problem of building Recommender Systems has attracted considerable attention in recent years, but most recommender systems are designed for recommending items for individuals. The aim of this paper is to automatically recommend and rank a list of new items to a group of users. The proposed model can be considered as a collaborative Bayesian network-based group recommender system, where the group's rates are computed from past voting patterns of other users with similar tastes. The use of Bayesian networks allows us to obtain an intuitive representation of the mechanisms that govern the relationships between the group members."
7FCED0A8,International Conference on Data Engineering,yannis ioannidis,2007,Emerging Open Agoras of Data and Information,business + information management + information searching + independent system + information retrieval + distributed environment + user interfaces + quality of service + informatics + uncertainty + data open Agoras + context dependent + information seeking + information systems + information open Agoras,AuthorProvided Keywords Not Found,"Open Agoras of information are distributed environments of independent systems, where seeking for information is similar to real-life searching for material goods. Interaction with these systems may occur in several unconventional modalities, user behavior may be personalized and context-dependent, system reaction may be unpredictable, and the information produced as a result may also be personalized and context-dependent, negotiable and of uncertain origin or quality. This paper explores open Agoras of information and identifies some technical challenges that they raise."
7F34E49C,International Conference on Data Engineering,amelie marian + peter j stuckey + kenneth a ross,2007,Practical Preference Relations for Large Data Sets, + databases + search engines + computer science + testing + arithmetic + database management systems + partial order + transitive closure,,"User-de ned preferences allow personalized ranking of query results. A user provides a declarative speci cation of his/her preferences, and the system is expected to use that speci cation to give more prominence to preferred answers. We study constraint formalisms for expressing user preferences as base facts in a partial order. We consider a language that allows comparison and a limited form of arithmetic, and show that the transitive closure computation required to complete the partial order terminates. We consider various ways of composing partial orders from smaller pieces, and provide results on the size of the resulting transitive closures. We introduce the notion of \covering composition,"" which solves some semantic problems apparent in previous notions of composition. Finally, we show how preference queries within our language can be supported by suitable index structures for e cient evaluation over large data sets. Our results provide guidance about when complex preferences can be e ciently evaluated, and when they cannot."
7E85DACE,International Conference on Data Engineering,sanjay agrawal + rajiv rastogi + sujay deb + k v m naidu,2007,Efficient Detection of Distributed Constraint Violations,approximation theory + NP-hard + software monitoring + distributed system + distributed environment + approximation algorithms + frequency + security of data + distributed constraint violation detection + communication-efficient schemes + np hard + global constraint checking + tcpip + system monitoring + anomalies detection + computational complexity,AuthorProvided Keywords Not Found,"In many distributed environments, the primary function of monitoring software is to detect anomalies, i.e., instances when system behavior deviates substantially from the norm. In this paper, we propose communication-efficient schemes for the anomaly detection problem, which we model as one of detecting the violation of global constraints defined over distributed system variables. Our approach eliminates the need to continuously track the global system state by decomposing global constraints into local constraints that can be checked efficiently at each site. Only in the occasional event that a local constraint is violated, do we resort to more expensive global constraint checking. We show that the problem of selecting the local constraints, based on frequency distribution of individual system variables, so as to minimize the communication cost is NP-hard. We propose approximation algorithms for computing provably near-optimal (in terms of the number of messages) local constraints. Experimental results with real-life network traffic data sets demonstrate that our technique can reduce message communication overhead by as much as 70% compared to existing data distribution-agnostic approaches."
804D2E94,International Conference on Data Engineering,kenneth salem + ye qin + anil k goel,2007,Towards Adaptive Costing of Database Access Methods,access method + cost estimation + indexes + computer science + query optimization + testing + calibration + costing + cost function + indexation + prototypes + database systems,,"Most database query optimizers use cost models to identify good query execution plans. Inaccuracies in the cost models can cause query optimizers to select poor plans. In this paper, we consider the problem of accurately estimating the I/O costs of database access methods, such as index scans. We present some experimental results which show that existing analytical I/O cost models can be very inaccurate. We also present a simple analysis which shows that larger cost estimation errors can cause the query optimizer to make larger mistakes in plan selection. We propose the use of an adaptive black-box statistical cost estimation methodology to achieve better estimates."
79EC8BE3,International Conference on Data Engineering,stanley b zdonik + tingjian ge,2007,"Fast, Secure Encryption for Indexing in a Column-Oriented DBMS",information system + data warehouse + block cipher + column stores + indexing + data security + indexation + trusted server + information security + cryptography + column-oriented DBMS indexing + networked information systems + symmetric encryption + INFO-CPA-DB + database encryption scheme + database system + fast secure encryption + access control + information systems + data warehouses + database systems,AuthorProvided Keywords Not Found,"Networked information systems require strong security guarantees because of the new threats that they face. Various forms of encryption have been proposed to deal with this problem. In a database system, there are often two contradictory goals: security of the encryption and fast performance of queries. There have been a number of proposals of database encryption schemes to facilitate queries on encrypted columns. Order-preserving encryption techniques are well-suited for databases since they support a simple, and efficient way to build indices. However, as we will show, they are insecure under straightforward attack scenarios. We propose a new light-weight database encryption scheme (called FCE) for column stores in data warehouses with trusted servers. The low decryption overhead of FCE makes comparisons of ciphertexts and hence indexing operations very fast. Since it is hard to use classical security definitions in cryptography to prove the security of any existing symmetric encryption scheme, we propose a relaxed measure of security, called INFO-CPA-DB. INFO-CPA-DB is based on a well-established security definition in cryptography and relaxes it using information theoretic concepts. Using INFO-CPA-DB, we give strong evidence that FCE is as secure as any underlying block cipher (yet more efficient than using the block cipher itself). Using the same security measure we also show the inherent insecurity of any order preserving encryption scheme under straightforward attack scenarios. We discuss indexing techniques based on FCE as well."
810BD100,International Conference on Data Engineering,akrivi vlachou + yannis kotidis + christos doulkeridis + michalis vazirgiannis,2007,SKYPEER: Efficient Subspace Skyline Computation over Distributed Data,peer-to-peer computing + computer networks + large-scale peer-to-peer networks + informatics + distributed computing + subspace skyline computation + SKYPEER + query processing + skyline query processing + super-peer architecture + optimization + file servers + computer architecture + distributed databases + information systems + distributed data,AuthorProvided Keywords Not Found,"Skyline query processing has received considerable attention in the recent past. Mainly, the skyline query is used to find a set of non dominated data points in a multidimensional dataset. While most previous work has assumed a centralized setting, in this paper we address the efficient computation of subspace skyline queries in large-scale peer-to-peer (P2P) networks, where the dataset is horizontally distributed across the peers. Relying on a super-peer architecture we propose a threshold based algorithm, called SKYPEER, which forwards the skyline query requests among peers, in such a way that the amount of transferred data is significantly reduced. For efficient subspace skyline processing, we extend the notion of domination by defining the extended skyline set, which contains all data elements that are necessary to answer a skyline query in any arbitrary subspace. We prove that our algorithm provides the exact answers and we present optimization techniques to reduce communication cost and execution time. Finally, we provide an extensive experimental evaluation showing that SKYPEER performs efficiently and provides a viable solution when a large degree of distribution is required."
80CCFF2E,International Conference on Data Engineering,anastasios kementsietsidis + floris geerts + wenfei fan + xibei jia,2007,Rewriting Regular XPath Queries on XML Views,xml document + xml + engines + prototypes + xml documents + bioinformatics + ontologies + linear time + access control + automata + query languages,,"We study the problem of answering queries posed on virtual views of XML documents, a problem commonly encountered when enforcing XML access control and integrating data. We approach the problem by rewriting queries on views into equivalent queries on the underlying document, and thus avoid the overhead of view materialization and maintenance. We consider possibly recursively defined XML views and study the rewriting of both XPath and regular XPath queries. We show that while rewriting is not always possible for XPath over recursive views, it is for regular XPath; however, the rewritten query may be of exponential size. To avoid this prohibitive cost we propose a rewriting algorithm that characterizes rewritten queries as a new form of automata, and an efficient algorithm to evaluate the automaton-represented queries. These allow us to answer queries on views in linear time. We have fully implemented a prototype system, SMOQE, which yields the first regular XPath engine and a practical solution for answering queries over possibly recursively defined XML views. •_ 2007 IEEE."
794326EE,International Conference on Data Engineering,jayavel shanmugasundaram + guy yona + lin guo,2007,Topology Search over Biological Databases,biological database + query processing + databases + transcription factor + computer science + biological databases + biology computing + technical report + data topologies + topology search + topology,Interaction,"We introduce the notion of a data topology and the problem of topology search over databases. A data topology summarizes the set of all possible relationships that connect a given set of entities. Topology search enables users to search for data topologies that relate entities in a large database, and to effectively summarize and rank these relationships. Using topology search over a biological database, users can ask, for example, how transcription factor proteins are related to DNAs in humans. However, detecting topologies in large databases is a difficult problem because entities can be connected in multiple ways. In this paper, we formalize the notion of data topologies, develop efficient algorithms for computing data topologies based on user queries, and evaluate our algorithms using a real biological database, the Biozon database (www.biozon.org)."
7F29EA63,International Conference on Data Engineering,stefanie scherzinger + christian koch + martin schmidt,2007,Combined Static and Dynamic Analysis for Effective Buffer Minimization in Streaming XQuery Evaluation,database languages + project management + buffer storage + prototypes + program diagnostics + static analysis + dynamic analysis + garbage collection + buffer minimization + query processing + memory management + buffer management + engines + xml + in-memory XQuery processing + streaming XQuery evaluation,AuthorProvided Keywords Not Found,Effective buffer management is crucial for efficient in-memory and streaming XQuery processing. We propose a buffer management scheme which combines static and dynamic analysis to keep main memory consumption low. Our approach relies on a technique that we call active garbage collection and which actively purges buffers at runtime based on the current status of query evaluation. We have built a prototype system for a practical fragment of XQuery which employs our buffer management scheme. The experimental results demonstrate the significant impact of combined static and dynamic analysis on reducing main memory consumption and running time.
7DA3F38B,International Conference on Data Engineering,roman schmidt + jessica muller + r w john + manfred hauswirth + martin richtarsky + kaiuwe sattler + marcel karnstedt,2007,UniStore: Querying a DHT-based Universal Storage,database languages + information management + weight distribution + public data management systems + universal relation model + distributed hash table overlay system + distributed universal storage + p2p + resource description framework + query processing + storage management + physical layer + distributed hash table + UniStore + cost function + energy management + relational model + Internet + internet,AuthorProvided Keywords Not Found,"The idea of collecting and combining large public data sets and services became more and more popular. The special characteristics of such systems and the requirements of the participants demand for strictly decentralized solutions. However, this comes along with several ambitious challenges a corresponding system has to overcome. In this demonstration paper, we present a lightweight distributed universal storage capable of dealing with those challenges, and providing a powerful and flexible way of building Internet-scale public data management systems. We introduce our approach based on a triple storage on top of a distributed hash table (DHT) overlay system, based on the ideas of a universal relation model and the resource description framework (RDF), and outline solved challenges as well as open issues."
80EA3946,International Conference on Data Engineering,j xu yu + bolin ding + shan wang + xuemin lin + lu qin + xiao zhang,2007,Finding Top-k Min-Cost Connected Trees in Databases,steiner tree problem + knowledge engineering + np complete problem + information retrieval + time complexity + relational database + Steiner tree problem + relational databases + NP-complete problem + weighted undirected graph + tree graphs + data engineering + query processing + Top-k Min-Cost connected trees + directed graphs + weighted directed graph + directed graph + l-keyword query + computational complexity,AuthorProvided Keywords Not Found,"It is widely realized that the integration of database and information retrieval techniques will provide users with a wide range of high quality services. In this paper, we study processing an l-keyword query, p1, p1, ..., pl, against a relational database which can be modeled as a weighted graph, G(V, E). Here V is a set of nodes (tuples) and E is a set of edges representing foreign key references between tuples. Let Vi _ V be a set of nodes that contain the keyword pi. We study finding top-k minimum cost connected trees that contain at least one node in every subset Vi, and denote our problem as GST-k When k = 1, it is known as a minimum cost group Steiner tree problem which is NP-complete. We observe that the number of keywords, l, is small, and propose a novel parameterized solution, with l as a parameter, to find the optimal GST-1, in time complexity O(3ln + 2l ((l + logn)n + m)), where n and m are the numbers of nodes and edges in graph G. Our solution can handle graphs with a large number of nodes. Our GST-1 solution can be easily extended to support GST-k, which outperforms the existing GST-k solutions over both weighted undirected/directed graphs. We conducted extensive experimental studies, and report our finding."
0020D987,International Conference on Data Engineering,patrick martin + baoning niu + paul bird + wendy powley + randy w horman,2007,Adapting Mixed Workloads to Meet SLOs in Autonomic DBMSs,service level + database management system,,"Workload adaptation allows an autonomic database management system (DBMS) to efficiently make use of its resources and meet its Service Level Objectives (SLOs) by filtering or controlling the workload presented to it. Workload adaptation has been shown to be effective for OLAP and OLTP workloads. We outline a framework of workload adaptation and explain how it can be extended to manage mixed workloads comprised of both OLAP and OLTP queries. Experiments with IBM‘ DB2‘ Universal Database_ are presented that illustrate the effectiveness of our techniques."
8038B93F,International Conference on Data Engineering,vaibhav padliya + a j singh + joseph patrao + ling liu + mahesh palekar + s li + mudhakar srivatsa + james caverlee + bhuvan bamba + t k bansal,2007,"DSphere: A Source-Centric Approach to Crawling, Indexing and Searching the World Wide Web",web spam + document handling + source-based link analysis + fault tolerance + document searching + indexing + indexation + world wide web + World Wide Web + document crawling + scalability + decentralized system + source-centric view + DSphere + document ranking + document indexing + link analysis + service oriented architecture + Internet + protocols + query formulation + internet,AuthorProvided Keywords Not Found,"We describe DSphere - a decentralized system for crawling, indexing, searching and ranking of documents in the World Wide Web. Unlike most of the existing search technologies that depend heavily on a page-centric view of the Web, we advocate a source-centric view of the Web and propose a decentralized architecture for crawling, indexing and searching the Web in a distributed source-specific fashion. A fully decentralized crawler is developed to crawl the World Wide Web where each peer is assigned the responsibility of crawling a specific set of documents referred to as a source collection. Link analysis techniques are used for ranking documents. Traditional link analysis techniques suffer from problems like slow refresh rate and vulnerabilities to Web Spam. We propose a source-based link analysis approach, which computes fast and accurate ranking scores for all crawled documents."
808ACDFA,International Conference on Data Engineering,mayssam sayyadian + hieu lekhac + anhai doan + luis gravano,2007,Efficient Keyword Search Across Heterogeneous Relational Databases,schema matching + heterogeneous relational database + human resource management + relational database + relational databases + Kite + government + lifting equipment + foreign-key join + distributed databases + structure discovery + efficient keyword search + distributed data,AuthorProvided Keywords Not Found,"Keyword search is a familiar and potentially effective way to find information of interest that is ""locked"" inside relational databases. Current work has generally assumed that answers for a keyword query reside within a single database. Many practical settings, however, require that we combine tuples from multiple databases to obtain the desired answers. Such databases are often autonomous and heterogeneous in their schemas and data. This paper describes Kite, a solution to the keyword-search problem over heterogeneous relational databases. Kite combines schema matching and structure discovery techniques to find approximate foreign-key joins across heterogeneous databases. Such joins are critical for producing query results that span multiple databases and relations. Kite then exploits the joins - discovered automatically across the databases - to enable fast and effective querying over the distributed data. Our extensive experiments over real-world data sets show that (1) our query processing algorithms are efficient and (2) our approach manages to produce high-quality query results spanning multiple heterogeneous databases, with no need for human reconciliation of the different databases."
7F70D688,International Conference on Data Engineering,carlo zaniolo,2007,Load Shedding for Window Joins on Multiple Data Streams,application software + computer science + random sampling + productivity + sampling methods + testing + frequency + robustness + bioinformatics,,"We consider the problem of semantic load shedding for continuous queries containing window joins on multiple data streams and propose a robust approach that is effective with the different semantic accuracy criteria that are required in different applications. In fact, our approach can be used to (i) maximize the number of output tuples produced by joins, and (ii) optimize the accuracy of complex aggregates estimates under uniform random sampling. We first consider the problem of computing maximal subsets of approximate window joins over multiple data streams. Previously proposed approaches are based on multiple pairwise joins and, in their load-shedding decisions, disregard the content of streams outside the joined pairs. To overcome these limitations, we optimize our load-shedding policy using various predictors of the productivity of each tuple in the window. To minimize processing costs, we use a fastand-light sketching technique to estimate the productivity of the tuples. We then show that our method can be generalized to produce statistically accurate samples, as needed in, e.g., the computation of averages, quantiles, and stream mining queries. Tests performed on both synthetic and reallife data demonstrate that our method outperforms previous approaches, while requiring comparable amounts of time and space."
7E8A969C,International Conference on Data Engineering,donghui zhang + shashi shekhar + james m kang + mohamed f mokbel + tian xia,2007,Continuous Evaluation of Monochromatic and Bichromatic Reverse Nearest Neighbors,voronoi diagram + virtual reality + Voronoi diagrams + general evaluation + voronoi diagrams + monochromatic reverse nearest neighbors + computational geometry + continuous reverse nearest neighbor queries + IGERN algorithm + strategic planning + query processing + computer science + recurrent neural networks + experimental analysis + incremental evaluation + dichromatic reverse nearest neighbors + bichromatic reverse nearest neighbor queries + information science + computational complexity,AuthorProvided Keywords Not Found,"This paper presents a novel algorithm for Incremental and General Evaluation of continuous Reverse Nearest neighbor queries (IGERN, for short). The IGERN algorithm is general as it is applicable for both the monochromatic and bichromatic reverse nearest neighbor queries. The incremental aspect of IGERN is achieved through determining only a small set of objects to be monitored. While previous algorithms for monochromatic queries rely mainly on monitoring six pie regions, IGERN takes a radical approach by monitoring only a single region around the query object. The IGERN algorithm clearly outperforms the state-of-the-art algorithms in monochromatic queries. In addition, the IGERN algorithm presents the first attempt for continuous evaluation of bichromatic reverse nearest neighbor queries. The computational complexity of IGERN is presented in comparison to the state-of-the-art algorithms in the monochromatic case and to the use of Voronoi diagrams for the bichromatic case. In addition, the correctness of IGERN in both the monochromatic and bichromatic cases are proved. Extensive experimental analysis shows that IGERN is efficient, is scalable, and outperforms previous techniques for continuous reverse nearest neighbor queries."
805C484F,International Conference on Data Engineering,anastasios kementsietsidis + floris geerts + wenfei fan + xibei jia + philip bohannon,2007,Conditional Functional Dependencies for Data Cleaning,cost function + data handling + computational fluid dynamics + data warehouses + armstrong axioms + functional dependency + business + sql + data cleaning + data mining + statistics + data quality,,"We propose a class of constraints, referred to as conditional functional dependencies (CFDs), and study their applications in data cleaning. In contrast to traditional functional dependencies (FDs) that were developed mainly for schema design, CFDs aim at capturing the consistency of data by incorporating bindings of semantically related values. For CFDs we provide an inference system analogous to Armstrong's axioms for FDs, as well as consistency analysis. Since CFDs allow data bindings, a large number of individual constraints may hold on a table, complicating detection of constraint violations. We develop techniques for detecting CFD violations in SQL as well as novel techniques for checking multiple constraints in a single query. We experimentally evaluate the performance of our CFD-based methods for inconsistency detection. This not only yields a constraint theory for CFDs but is also a step toward a practical constraint-based method for improving data quality."
7D16552B,International Conference on Data Engineering,sebastian maneth + damien k fisher,2007,Structural Selectivity Estimation for XML Documents,XPath + database languages + memory + lossy compression + XML documents + indexation + xml document + document tree + database management systems + data engineering + query processing + navigation + database system + computer science + xml + XML + structural join + xml documents + XML database + xml database + twig join + structural selectivity estimation + database systems,AuthorProvided Keywords Not Found,"Estimating the selectivity of queries is a crucial problem in database systems. Virtually all database systems rely on the use of selectivity estimates to choose amongst the many possible execution plans for a particular query. In terms of XML databases, the problem of selectivity estimation of queries presents new challenges: many evaluation operators are possible, such as simple navigation, structural joins, or twig joins, and many different indexes are possible. A new synopsis for XML documents is introduced which can be effectively used to estimate the selectivity of complex path queries. The synopsis is based on a lossy compression of the document tree that underlies the XML document, and can be computed in one pass from the document. It has several advantages over existing approaches: (1) it allows one to estimate the selectivity of queries containing all XPath axes, including the order-sensitive ones, (2) the estimator returns a range within which the actual selectivity is guaranteed to lie, with the size of this range implicitly providing a confidence measure of the estimate, and (3) the synopsis can be incrementally updated to reflect changes in the XML database."
805DD0F9,International Conference on Data Engineering,sonia delfin + miquel montaner + g gonzalez + j l de la rosa,2007,Embedding Emotional Context in Recommender Systems,adaptive systems + ambient intelligence + artificial intelligence + informatics + recommender systems + pervasive computing + appropriate technology + recommender system + decision support systems + context modeling + machine learning,,"Emotions are crucial for user's decision making in recommendation processes. We first introduce Ambient Recommender Systems, which arise from the analysis of new trends on the exploitation of the emotional context in the next generation of recommender systems. We then explain some results of these new trends in real-world applications through the Smart Prediction Assistant (SPA) platform in an Intelligent Learning Guide with more than three million users. While most approaches to recommending have focused on algorithm performance, SPA makes recommendations to users on the basis of emotional information acquired in an incremental way. This article provides a cross-disciplinary perspective to achieve this goal in such recommender systems through a SPA platform. The methodology applied in SPA is the result of a bunch of technology transfer projects for large real-world recommender systems."
7EC64296,International Conference on Data Engineering,andre madeira + s muthukrishnan + tao ye + s bhatlacharyya,2007,How to scalably and accurately skip past streams,spine + stream processing + data analysis + ip network + face detection + inspection,,"Data stream methods look at each new item of the stream, perform a small number of operations while keeping a small amount of memory, and still perform muchneeded analyses. However, in many situations, the update speed per item is extremely critical and not every item can be extensively examined. In practice, this has been addressed by only examining every N th item from the input; decreasing the input rate by a fraction 1/N , but resulting in loss of guarantees on the accuracy of the post-hoc analyses. In this paper, we present a technique of skipping past streams and looking at only a fraction of the input. Unlike traditional methods, our skipping is performed in a principled manner based on the _normî of the stream seen. Using this technique on top of well-known sketches, we show several-fold improvement in the update time for processing streams with a given guaranteed accuracy, for a number of stream processing problems including data summarization, heavy hitters detection and self-join size estimation. We present experimental results of our methods over synthetic data and integrate our methods into Sprint's Continuous Monitoring (CMON) system for live network traffic analyses. Furthermore, aiming at future scalable stream processing systems and going beyond state-of-art packet header analyses, we show how the packet contents can be analyzed at streaming speeds, a more challenging task because each packet content can result in many updates."
7D24A0DA,International Conference on Data Engineering,flavio rizzolo + jane w s liu + mariano p consens,2007,XPlainer: Visual Explanations of XPath Queries,XML processing tool + application software + visualization + web services + xml document + query languages + displays + XPath language + XML querying + development environment + query processing + XPlainer + computer languages + xml + engines + data visualisation + XML + XPlainer-Eclipse tool + XPath queries + debugging + oxygen,AuthorProvided Keywords Not Found,"The popularity of XML has motivated the development of novel XML processing tools many of which embed the XPath language for XML querying, transformation, constraint specification, etc. XPath developers (as well as less technical users) have access to commercial tools to help them use the language effectively. Example tools include debuggers that return the result of XPath subexpressions visualized in the context of the input XML document. This paper introduces XPlainer, a language that provides explanations of why XPath expressions return a specific answer. An explanation returns precisely the nodes in the input XML document that contribute to the answer. We provide a complete formalization for explanation queries based on the semantics of XPath. This enables the use of XPath engines for the evaluation of explanation queries. We describe a tool that uses XPlainer queries to provide visual explanations. The XPlainer-Eclipse tool is built on an extensible development environment that includes editors for visualizing both XML documents and XPath expressions as trees together with the explanation of the answers."
80C7E023,International Conference on Data Engineering,min wang + haixun wang + lipyeow lim,2007,Semantic Data Management: Towards Querying Data with their Meaning,concrete + data management + transitive closure + query languages + relational database management systems + ontology + ontologies + owl + xml + query language + relational databases + data model + knowledge management + taxonomy + industrial relations + domain knowledge + database languages + data models + relational data + relational database management system,,"Relational database management systems are constantly being extended and augmented to accommodate data in different domains. Recently, with the increasing use of ontology in various applications, the need to support ontology, especially the related inferencing operation, in DBMS has become more concrete and urgent. However, manipulating knowledge along with relational data in DBMSs is not a trivial undertaking due to the mismatch in data models. In this paper, we introduce a framework for managing relational data and hierarchical domain knowledge together. Our framework persists taxonomies contained in ontologies by leveraging XML support in hybrid relational-XML DBMSs (e.g., IBM's DB2 v9) and rewrites ontology-based semantic matching queries using the industry-standard query languages, SQL/XML and XQuery. Compared with previous approaches, our approach does not materialize transitive closures of ontological relationships to support inferencing. Consequently, our method has wide applicability and good performance."
7F2958CA,International Conference on Data Engineering,luis otavio alvares + bart moelans + vania bogorny,2007,Filtering Frequent Spatial Patterns with Qualitative Spatial Reasoning,association rules + common sense reasoning + lighting + geographic information systems + filtering + data mining + computer science + domain knowledge + spatial pattern + spatial reasoning,,"The full version of this paper appeared in the 2007 IEEE Workshop Proceedings of the 23rd International Conference on Data Engineering (ICDE2007)[6] The huge amount of patterns generated by frequent pattern mining algorithms has been extensively addressed in the last few years. Different measures have been proposed to evaluate how interesting association rules are. However, according to [3] it is difficult to come up with a single metric that quantifies the _interestingnessî or _goodnessî of an association rule. In most approaches, non-interesting rules are eliminated during the rule generation, i.e., a posteriori, when frequent itemsets have already been generated. In spatial frequent pattern mining the number of non-interesting association rules can increase even further than in transactional pattern mining. Geographic data have semantic dependencies and spatial properties which in many cases are well known and non-interesting for data mining. In the geographic domain, not only well known geographic dependencies (e.g., is gasStation ! touches street) generate a large number of patterns without novel and useful knowledge. Different spatial predicates may contain the same geographic object type. In transactional frequent pattern mining items have binary values, and do either participate or not in a transaction. For instance, the item milk is either present or not in a transaction t. In spatial frequent pattern mining the same spatial object (item) may have different qualitative spatial relationships with the target feature (transaction), and by consequence participate more then once in the same transaction t. For instance, a city C may contain an instance i1 of river, be crossed by an instance i2 of river, or even touch an instance i3 of river. Different spatial relationships with the same geographic object type will generate associations such as contains River ! touches River when data are considered at general granularity levels [8]. It is well known that a city does not touch a river because it also contains a river. Such kind of rule is non-interesting for most applications. An interesting association rule would be the combination of any of these two predicates with a different geographic object type or some non-spatial attribute. For instance: contains River ! W aterP ollution = high or touches River ! exportationRate = high. In [5], Apriori-KC is proposed. This method made some changes on Apriori [1] to eliminate well known geographic patterns using background knowledge. In [4] this method is extended with an additional step where not only frequent itemsets are reduced, but input space is reduced as much as possible, since this is still the most efficient way for pruning frequent patterns. In [7] the closed frequent pattern mining approach is applied to the geographic domain, eliminating both well known dependencies and redundant frequent itemsets. In this paper we extended the method presented in [5] to reduce the number of non-interesting spatial patterns using qualitative spatial reasoning. Not only the data is considered, but its semantics is taken into account, and frequent patterns that contain the same feature type are removed a priori. The method proposed in this paper is more effective and efficient then most other methods which perform pruning after the rule generation, since our method explores the anti-monotone constraint of Apriori [1] and prunes non-interesting patterns during the frequent set generation. Other transactional pattern mining"
7D45207B,International Conference on Data Engineering,mariana jbantova + elke a rundensteiner + bin liu,2007,Optimizing State-Intensive Non-Blocking Queries Using Run-time Adaptation,computational modeling + remote monitoring + decision support systems + real time systems + distributed system + intrusion detection + distributed processing + parallel processing,,"Main memory is a critical resource when processing non-blocking queries with state intensive operators that require real-time responses. While partitioned parallel processing can alleviate the stringent memory demands in some cases, in general even in a distributed system main memory remains bounded. In this work, we thus investigate the integration of two run-time adaptation techniques, namely, state spill to disk and state relocation to an alternate machine, to handle this memory shortage pmblem. We analyze the tradeoffs regarding key factors affecting these two run-time operator state adaptation techniques in a modern computecluster environment. Two strategies, lazy-disk and active-disk, are then proposed that integrate both state spill and state relocation adaptations with different emphasis on local versus global decision making. Extensive experiments of the proposed query processing system conducted on a compute-cluster (not merely a simulation) confirm the effectiveness of these strategies. © 2007 IEEE."
750D678E,International Conference on Data Engineering,xiaofang zhou + jie shao + heng tao shen + zi huang + yijun li,2007,Dynamic Batch Nearest Neighbor Search in Video Retrieval,parallel processing + search space + nearest neighbor search + data engineering + information technology + feature vector + neural networks + system testing + cost function + information retrieval + nearest neighbor,,"To retrieve similar database videos to a query clip, each video is typically represented by a sequence of highdimensional feature vectors. Given a query video containing m feature vectors, an independent Nearest Neighbor (NN) search for each feature vector is often first performed. Completing all the NN searches, an overall similarity is then computed, i.e., a single video retrieval usually involves the searches for m times. Since normally nearby feature vectors in a video are similar, a large number of expensive random disk accesses are expected to repeatedly occur, which crucially affects the overall query performance. Batch Nearest Neighbor (BNN) search is stated as a single operation that performs a batch of individual NN searches. This paper presents a novel approach to efficient high-dimensional BNN search called Dynamic Query Ordering (DQO) for advanced optimizations in both I/O and CPU cost. Observing the overlapped candidates (or search space) of a pervious query may help to further reduce the candidate sets of succeeding queries, DQO aims to progressively find a query order such that the common candidates among queries are fully utilized to maximally reduce the total number of candidates. Modelling the candidate set relationship by a Candidate Overlapping Graph (COG), DQO iteratively selects the next query to be executed based on its estimated pruning power to the rest of queries with the dynamically updated COG. The extensive experiments show its significance. © 2007 IEEE."
7E954FAC,International Conference on Data Engineering,steve madden + t m gil,2007,Scoop: An Adaptive Indexing Scheme for Stored Data in Sensor Networks,query processing + Scoop + database + wireless sensor networks + sensor network + indexing + indexation + data querying + adaptive indexing scheme + mote-based sensor network + statistics,AuthorProvided Keywords Not Found,"We present the design of Scoop, a system for indexing and querying stored data in sensor networks. Scoop works by collecting statistics about the rate of queries and distribution of sensor readings in a sensor network, and uses those statistics to build an index that tells nodes where in the network to store their data. Using this index, a queries over that stored, data can be answered, efficiently, without flooding those queries throughout the network. This approach offers a substantial advantage over other solutions that either store all data externally on a base station (requiring every reading to be collected from all nodes), or that store all data locally on the node that produced it (requiring queries to be flooded throughout the network). Our results show that Scoop offers a factor of four reduction in message transmissions relative to existing techniques in a real implementation on a 64-node mote-based sensor network. These results also show that Scoop is able to efficiently adapt to changes in the distribution of data and queries."
NE2694,International Conference on Data Engineering,Ke Deng+Xiaofang Zhou+Heng Tao,2007,Multi-source Skyline Query Processing in Road Networks,query processing + single-source skyline query + network distance + road networks + multisource skyline query processing + relatively query + user-given location,AuthorProvided Keywords Not Found,"Skyline query processing has been investigated extensively in recent years, mostly for only one query reference point. An example of a single-source skyline query is to find hotels which are cheap and close to the beach (an absolute query), or close to a user-given location (a relatively query). A multi-source skyline query considers several query points at the same time (e.g., to find hotels which are cheap and close to the University, the Botanic Garden and the China Town). In this paper, we consider the problem of efficient multi-source skyline query processing in road networks. It is not only the first effort to consider multi-source skyline query in road networks but also the first effort to process the relative skyline queries where the network distance between two locations needs to be computed on-the-fly. Three different query processing algorithms are proposed and evaluated in this paper. The Lower Bound Constraint algorithm (LBC) is proven to be an instance optimal algorithm. Extensive experiments using large real road network datasets demonstrate that LBC is four times more efficient than a straightforward algorithm."
7E1759DF,International Conference on Data Engineering,badrish chandramouli + sarath babu + c bond + jian yang,2007,On Suspending and Resuming Dataflows,database management system + rejuvenation schedule + grid computing + degradation + database management systems + dataflows + grids + query processing + rebooting + software rejuvenation + cost effectiveness + control systems + data flow computing + scheduling + database systems,AuthorProvided Keywords Not Found,"Consider a long-running, resource-intensive query Q running on a database management system (DBMS). Suppose another task T with much higher priority arrives, and we need to process T as quickly as possible and with all available resources. Ideally, the system should suspend the execution of Q, quickly release all resources held by Q, and start T using all resources. Query Q can be resumed once T finishes execution, ideally without losing any significant fraction of the work that Q had done prior to suspend. Beyond supporting mixed-priority DBMS workloads, suspend/resume of queries, or dataflows in general, is important in many other settings: Utility and grid: Dataflows now run frequently on computational utilities and grids composed of autonomous resources. When the owner of resources wants to use them, dataflows running on these resources must release control quickly, and migrate to other resources. Software rejuvenation: Benefits of software rejuvenation, the practice of rebooting enterprise computing systems regularly, are now recognized widely. Reboot is critical when performance degrades due to resource exhaustion caused by resource leaks. Suspend and resume is important in this setting because (1) the challenge of predicting completion times accurately makes it difficult to schedule task completion to match a rejuvenation schedule; (2) when performance degrades, it may not be cost-effective to wait for all dataflows to complete before rebooting."
80397468,International Conference on Data Engineering,jeonghyon hwang + stan zdonik + ugur cetintemel,2007,Fast and Reliable Stream Processing over Wide Area Networks, + application software + computer science + stream processing + distributed processing,,"We present a replication-based approach that enables both fast and reliable stream processing over wide area networks. Our approach replicates stream processing operators in a manner where operator replicas compete with each other to make the earliest impact. Therefore, any processing downstream from such replicas can proceed by relying on the fastest replica without being held back by slow or failed ones. Furthermore, our approach allows replicas to produce output in different orders so as to avoid the cost of forcing an identical execution across replicas, without sacrificing correctness. We first consider semantic issues for correct replicated stream processing and, based on a formal foundation, extend common stream-processing primitives. Next, we discuss strategies for deploying replicas. Finally, we present preliminary results obtained from experiments on PlanetLab that substantiate the potential benefits of our approach."
81560CCC,International Conference on Data Engineering,michail vaitis + xiangyuan dai + nikos mamoulis + man lung yiu,2007,Top-k Spatial Preference Queries,query processing + database + search algorithm + indexing + computer science + geography + spatial neighborhood + indexation + real estate + top-k spatial preference queries + database management systems + database systems,AuthorProvided Keywords Not Found,"A spatial preference query ranks objects based on the qualities of features in their spatial neighborhood. For example, consider a real estate agency office that holds a database with available flats for lease. A customer may want to rank the flats with respect to the appropriateness of their location, defined after aggregating the qualities of other features (e.g., restaurants, cafes, hospital, market, etc.) within a distance range from them. In this paper, we formally define spatial preference queries and propose appropriate indexing techniques and search algorithms for them. Our methods are experimentally evaluated for a wide range of problem settings."
7B4356BA,International Conference on Data Engineering,gang luo,2007,Partial Materialized Views,databases + rdbms + PostgreSQL + cache storage + relational databases + partial query results + query execution time + sql + SQL + partial materialized views + query processing + RDBMS + cached partial results + materialized views,AuthorProvided Keywords Not Found,"Early access to partial query results is highly desirable during exploration of massive data sets. However, it is challenging to provide transactionally consistent, immediate partial results without significantly increasing queries' execution time. To address this problem, this paper proposes a partial materialized view (PMV) method to cache some of the most frequently accessed results rather than all the possible results. Compared to traditional materialized views, the proposed PMVs do not require maintenance during insertion into base relations, and have much smaller storage and maintenance overhead. Upon the arrival of a query, the RDBMS first searches the PMV and returns to the user the cached partial results. Since a large portion of the PMV is cached in memory, this usually finishes within a millisecond. Then the RDBMS continues to execute the query to find the remaining results. The efficiency of our PMV method is evaluated through a simulation study, a theoretical analysis, and an initial implementation in PostgreSQL."
8010DA99,International Conference on Data Engineering,chiranjeeb buragohain + sean suri + nitisha shrivastava,2007,Space Efficient Streaming Algorithms for the Maximum Error Histogram,approximation theory + Min-Merge achieves + Min-Increment algorithm + histograms + wireless sensor networks + algorithm design and analysis + space efficient streaming algorithms + computer networks + wireless sensor network + approximation algorithms + sliding window + streaming algorithm + approximation error + optimal B-bucket histogram + query processing + maximum error histograms + 2B-bucket histogram + computational complexity + data stream model,AuthorProvided Keywords Not Found,"We propose new algorithms for constructing maximum error (Lç) histograms in the data stream model. Our first algorithm (Min-Merge) achieves the following performance guarantee: using O(B) memory, it constructs a 2B-bucket histogram whose approximation error is at most the error of the optimal B-bucket histogram. Our second algorithm (Min-Increment) achieves a (1 + _)-approximation of a B-bucket histogram using O(_-1 B log U) space, where U is the size of the domain for data values. The memory requirements of these algorithms are a significant improvement over the previous best schemes for constructing near-optimal histograms in the data stream model, making them ideal for data summary applications where memory is at a premium, such as wireless sensor networks. Our Min-Increment algorithm also extends to the sliding window model without any asymptotic increase in space. Finally, using synthetic and real-world data, we show that our algorithms are indeed as space-efficient in practice as their theoretical analysis predicts - compared to previous best algorithms, they require two or more orders of magnitude less memory for the same approximation error."
7F776549,International Conference on Data Engineering,jerome simeon + m fernandez + philippe michiels + michael stark,2007,XQuery Streaming a la Carte,XQuery streaming + algebraic optimization + web services + program diagnostics + depth-first traversal + static analysis + query languages + algebra + depth first traversal + relational databases + XML operator + physical model + data models + query processing + navigation + process algebra + XML query evaluation + xml + XML + decorrelation + XML database + xml database + automata + relational operator,AuthorProvided Keywords Not Found,"Existing work on XML query evaluation has either focused on algebraic optimization techniques suitable for XML databases, or on algorithms to efficiently process XML messages represented as a stream of parsing events. In practice, complex applications often must handle both. In this paper, we develop a physical algebra that combines streaming operators with other standard relational and XML operators. Our physical model includes marked XML streams, which permit efficient XPath evaluation, but can only be consumed once. This constraint restricts the use of streaming operators to fragments of a query plan that only access data using depth-first traversal. We develop static analysis techniques to decide which fragment of a plan can be streamed. Our experiments demonstrate the benefits of blending streaming with other evaluation techniques."
80242231,International Conference on Data Engineering,kai sattler + jorg p muller + m karnsteclt,2007,Cost-Aware Skyline Queries in Structured Overlays,query optimization + data systems + robustness + physical layer + databases + automation + scalability + computer science + cost function + distributed databases,,"Recently, systems providing access to extremely large data collections, managed in a distributed manner, gain emerging attention. A promising approach to implement the physical layer of such systems are structured overlays based on the peer-to-peer paradigm. On the level of query expressiveness, ranking queries like skyline queries are predestinated for providing a fast but concise overview of the data. The problem is that structured overlays are able to handle dynamic and unreliable environments, but usually support only limited query processing capabilities, which are very improper for processing such sophisticated queries. In this work, we propose three variants of a skyline operator and two extensions, especially suitable for efficient determination of skylines in the before mentioned overlay systems. Additionally, we back the introduced approaches on an appropriate cost model, ready for implementing adaptive costbased query optimization."
7A45E498,International Conference on Data Engineering,gerhard weikum + vagelis hristidis + nishant kapoor + goutam das + s sudarshan,2007,STAR: A System for Tuple and Attribute Ranking of Query Answers,ad-hoc search + information retrieval + relational databases + displays + query answers + structured repositories + relational data + information overload + query processing + attribute ranking + tuple ranking + ad-hoc retrieval + query formulation,AuthorProvided Keywords Not Found,"In recent years there has been a great deal of interest in developing effective techniques for ad-hoc search and retrieval in structured repositories such as relational databases - e.g., searching online databases of homes, used cars, and electronic goods. In many of these applications, the user often experiences ""information overload'', which occurs when the system responds to an under-specified user query by returning an overwhelming number of tuples, each displayed with a huge number of features (or attributes). We have developed a search and retrieval system that tackles this information overload problem from two angles. First, we show how to automatically rank and display the top-n most relevant tuples. Second, our system offers techniques for ordering the attributes of the returned tuples in decreasing order of ""usefulness"" and selects only a few of the most useful attributes to display."
7DC8077E,International Conference on Data Engineering,xiaofang zhou + ke deng + heng tao shen,2007,Multi-source Skyline Query Processing in Road Networks,lower bound + computer networks + euclidean distance + space exploration,,"Skyline query processing has been investigated extensively in recent years, mostly for only one query reference point. An example of a single-source skyline query is to find hotels which are cheap and close to the beach (an absolute query), or close to a user-given location (a relatively query). A multi-source skyline query considers several query points at the same time (e.g., to find hotels which are cheap and close to the University, the Botanic Garden and the China Town). In this paper, we consider the problem of efficient multi-source skyline query processing in road networks. It is not only the first effort to consider multi-source skyline query in road networks but also the first effort to process the relative skyline queries where the network distance between two locations needs to be computed on-the-fly. Three different query processing algorithms are proposed and evaluated in this paper. The Lower Bound Constraint algorithm (LBC) is proven to be an instance optimal algorithm. Extensive experiments using large real road network datasets demonstrate that LBC is four times more efficient than a straightforward algorithm."
80C5B7DA,International Conference on Data Engineering,fragkiskos pentaris + yannis ioannidis,2007,Autonomic Query Allocation based on Microeconomics Principles,resource management + microeconomics + query markets + automatic query workload distribution + informatics + autonomic query allocation + query processing + database system + Pareto-optimal allocation + resource allocation + fluctuations + system performance + autonomous database systems + throughput + microeconomics principles + database systems,AuthorProvided Keywords Not Found,"In large federations of autonomous database systems, automatic distribution of the query workload to those systems is a critical issue. We examine this problem under the perspective of microeconomics theory and show how the latter can be used to construct an efficient decentralized mechanism that maximizes system throughput. In particular, we introduce a solution that is based on the notion of query markets. We examine the properties of these markets and show that they result in Pareto-optimal allocations of resources to queries. An extensive set of experiments with both a simulator and an actual implementation on top of a commercial DBMS demonstrate significant improvements in the overall system throughput when our technique is used."
NE2623,International Conference on Data Engineering,Mihai Lupu+Jianzhong Li+Beng Chin Ooi+Shengfei Shi,2007,Clustering wavelets to speed-up data dissemination in structured P2P MANETs,peer-to-peer computing + peer-to-peer networks + structured P2P MANET + Hyper-Ad + wavelets clustering + mobile computing + k-means clustering + summary information indexing + pattern clustering + fast data dissemination + ad hoc networks + multiresolution analysis + nonvolatile memory,AuthorProvided Keywords Not Found,"This paper introduces a fast data dissemination method for structured peer-to-peer networks. The work is motivated on one side by the increase in non-volatile memory available on mobile devices and, on the other side, by observed behavioral patterns of the users. We envision a scenario where users come together for short periods of time (e.g. public transport, conference sessions) and wish to be able to share large collections of data. With hundreds and even thousands of data, items stored on small devices, content publication is simply too energy and time consuming. By indexing summary information obtained by a combination of multi-resolution analysis and k-means, our method (Hyper-Ad) is able to cut down the overall construction time of an overlay network such as CAN by an order of magnitude, as well as provide fast approximate similarity search on such a network. The results of our extensive experimental studies confirm that Hyper-M is both energy and time efficient, and provides good precision and recall."
7D16D1BC,International Conference on Data Engineering,sihem ameryahia + irini fundulaki + laks v s lakshmanan,2007,Personalizing XML Search in PIMENTO,design methodology + query processing + navigation + score function + algorithm design and analysis + xml + XML + Pimento project + manufacturing + XML repositories + query answers + XML search,AuthorProvided Keywords Not Found,"XML search is increasing in popularity as more and larger XML repositories are becoming available. The accuracy of XML search varies across different systems and a lot of effort is put into designing scoring functions tailored to specific users and datasets. We argue that there is no one scoring function that fits all and advocate incorporating user profiles into XML search to personalize query answers by accounting for user profiles. First, we propose a framework for defining user profiles and for enforcing them during query processing. Second, we adapt the well-known top-k pruning to account for user profiles. Finally, we present effectiveness and efficiency experiments which show that query personalization in XML search dramatically improves the accuracy of query results while incurring negligible processing overhead. This work is in the context of the Pimento project which aims at improving the relevance of searching structured and unstructured content."
80A0F7F7,International Conference on Data Engineering,kianlee tan + hock beng lim + shili xiang,2007,Multiple Query Optimization for Wireless Sensor Networks,application software + in-network optimization + design optimization + wireless sensor networks + computer networks + wireless sensor network + base station optimization + query processing + optimisation + query optimization + sensor network + aggregation queries + data access + data acquisition + intelligent sensors + base station + base stations,AuthorProvided Keywords Not Found,"Our goal is to design a light-weight but effective scheme to support multiple data acquisition and aggregation queries in a wireless sensor network, in order to minimize the number of radio transmissions. Apart from being much more powerful than sensor nodes, the base station is also the interface of a wireless sensor network. Thus, we use the base station as a filter to reduce duplicate data accesses from the sensor network, and as a screen to hide the query dynamics as much as possible. We design a two-tier optimization scheme, base station optimization and in-network optimization."
NE2672,International Conference on Data Engineering,Byung-Won On+Nick Koudas+Dongwon Lee+Divesh Srivastava,2007,Group Linkage,group linkage measure + record linkage problem + bipartite graph matching + database system + graph theory + relational records + relational databases + SQL,AuthorProvided Keywords Not Found,"Poor quality data is prevalent in databases due to a variety of reasons, including transcription errors, lack of standards for recording database fields, etc. To be able to query and integrate such data, considerable recent work has focused on the record linkage problem, i.e., determine if two entities represented as relational records are approximately the same. Often entities are represented as groups of relational records, rather than individual relational records, e.g., households in a census survey consist of a group of persons. We refer to the problem of determining if two entities represented as groups are approximately the same as group linkage. Intuitively, two groups can be linked to each other if (i) there is high enough similarity between ""matching"" pairs of individual records that constitute the two groups, and (ii) there is a large fraction of such matching record pairs. In this paper, we formalize this intuition and propose a group linkage measure based on bipartite graph matching. Given a data set consisting of a large number of groups, efficiently finding groups with a high group linkage similarity to an input query group requires quickly eliminating the many groups that are unlikely to be desired matches. To enable this task, we present simpler group similarity measures that can be used either during fast pre-processing steps or as approximations to our proposed group linkage measure. These measures can be easily instantiated using SQL, permitting our techniques to be implemented inside the database system itself. We experimentally validate the utility of our measures and techniques using a variety of real and synthetic data sets."
7D0DB8D2,International Conference on Data Engineering,julien ponge + farouk toumani + fabio casati + boualem benatallah + hamid reza motaharinezhad + regis saintpaul,2007,ServiceMosaic: Interactive Analysis and Manipulation of Service Conversations,case tool + life cycle management + web services + graphical user interfaces + message exchanges + protocol discovery + service oriented application maintenance + ServiceMosaic + Web services life-cycle management + software maintenance + web service + service conversations + model-driven CASE tool + Web services + business protocol + service oriented computing + computer aided software engineering + service oriented architecture + protocols + protocol management,AuthorProvided Keywords Not Found,"In service-oriented computing, a conversation is a sequence of message exchanges between two or more services to achieve a certain goal, for example to order and pay for goods. A business protocol of a service is a specification of the possible conversations that a service can have with its partners. Motivated by the goal of facilitating the scalable development and maintenance of service oriented applications, especially in light of the many benefits of protocols, we have developed ServiceMosaic (servicemosaic.isima.fr), a platform for Web services life-cycle management. ServiceMosaic is an interactive and model-driven CASE tool for managing Web service interactions, which consists of two broad modules: protocol discovery and protocol management."
NE2707,International Conference on Data Engineering,Sam S. Lightstone+Chris Eaton+Yun Han Lee+Adam J. Storm,2007,Optimizing Concurrency Through Automated Lock Memory Tuning in DB2,memory allocation + storage management + automated lock memory tuning + database lock memory + lock escalation + locking structures + DB2 + adaptive algorithm + lock memory consumption + database management systems + adaptive tuning + concurrency,AuthorProvided Keywords Not Found,"Lock memory consumption can be difficult to project and can vary rapidly in short amounts of time. This volatility makes lock memory tuning difficult and can result in either significant memory waste if systems are configured for peak requirements, or lock escalation and lock wait if under configured; either of which can cause significant performance penalties. This paper describes an algorithm for adaptive tuning of database lock memory. The DB2 technique adapts the locking memory in real time to mitigate the occurrence of lock escalations. The technique uses a combination of synchronous and asynchronous modification to the locking structures so that it can respond well to rapid immediate growth in locking requirements. The adaptive algorithm also relaxes the locking memory over time so that peak requirements in lock memory will not result in a permanently large allocation of memory to locks. Experimental tests have shown this technique to work well in a number of benchmark and adaptive workloads, converging almost immediately to optimal settings which avoid lock escalations and achieve optimal throughput. The solution has been implemented in DB2 9."
7F3479D7,International Conference on Data Engineering,luis gravano + anhai doan + alpa jain,2007,SQL Queries Over Unstructured Text Databases,query processing + text analysis + SQL queries + information extraction + information extraction system + data mining + text documents + relational databases + unstructured text databases + sql + SQL,AuthorProvided Keywords Not Found,"Text documents often embed data that is structured in nature. By processing a text database with information extraction systems, we can define a variety of structured ""relations"" over which we can then issue SQL queries. Processing SQL queries in this text-based scenario presents multiple challenges. One key challenge is efficiency: information extraction is a time-consuming process, so query processing strategies should pick efficient extraction systems whenever possible, and also minimize the number of documents that they process. Another key challenge is result quality: extraction systems might output erroneous information or miss information that they should capture; also, efficiency-related query processing decisions (e.g., to avoid processing large numbers of useless documents) may compromise result completeness. To address these challenges, we characterize SQL query processing strategies in terms of their efficiency and result quality, and discuss the (user-specific) tradeoff between these two properties."
NE2609,International Conference on Data Engineering,Quanzhong Li+Minglong Shao+Volker Markl+Kevin Beyer+Latha Colby+Guy Lohman,2007,Adaptively Reordering Joins during Query Execution,adaptive query processing + database management system + query execution + database management systems + pipelined join plans + adaptive systems + query processing + join order + pipelined query plans + data characteristics + static query optimization + join method + routing decision,AuthorProvided Keywords Not Found,"Traditional query processing techniques based on static query optimization are ineffective in applications where statistics about the data are unavailable at the start of query execution or where the data characteristics are skewed and change dynamically. Several adaptive query processing techniques have been proposed in recent years to overcome the limitations of static query optimizers through either explicit re-optimization of plans during execution or by using a row-routing based approach. In this paper, we present a novel method for processing pipelined join plans that dynamically arranges the join order of both inner and outer-most tables at run-time. We extend the Eddies concept of ""moments of symmetry"" to reorder indexed nested-loop joins, the join method used by all commercial DBMSs for building pipelined query plans for applications for which low latencies are crucial. Unlike row-routing techniques, our approach achieves adaptability by changing the pipeline itself which avoids the bookkeeping and routing decision associated with each row. Operator selectivities monitored during query execution are used to change the execution plan at strategic points, and the change of execution plans utilizes a novel and efficient technique for avoiding duplicates in the query results. Our prototype implementation in a commercial DBMS shows a query execution speedup of up to 8 times."
7DCFF566,International Conference on Data Engineering,hetal thakkar + yijian bai + haixun wang + carlo zaniolo,2007,A Flexible Query Graph Based Model for the Efficient Execution of Continuous Queries,relation algebra + relational algebra + time series + spectrum + scheduling + graph theory + real time systems + sql + automata + algebra + concrete,,"In this paper, we propose a simple and flexible execution model that (i) supports a wide spectrum of alternative optimization and execution strategies and their mixtures, (ii) provides for dynamic reconfiguration when adding/deleting queries and changing optimization goals, (iii) optimizes response time in idle-waiting prone operators, such as union, joins, and operators used in time series and temporal sequence queries. Thus, we introduce a flexible and concrete model of execution semantics for continuous DSMS queries, and demonstrate its many applications. Our tuple-oriented model dovetails and complements the abstract set-oriented semantics of current DSMS constructs and operators, which are often based on relational algebra and SQL enhanced with windows."
NE2784,International Conference on Data Engineering,Ashraf Aboulnaga+Kareem el Gebaly+Daniel Wong,2007,_BE: User Guided Source Selection and Schema Mediation for Internet Scale Data Integration,NonControlled Keywords Not Found + Controlled Keywords Not Found,AuthorProvided Keywords Not Found,First Page of the Article
NE2783,International Conference on Data Engineering,Ashraf Aboulnaga+Kareem el Gebaly,2007,_BE: User Guided Source Selection and Schema Mediation for Internet Scale Data Integration,_BE + schema mediation + intuitive user interface + iterative exploration + Internet scale data integration + user guided source selection + Internet + data handling + constrained nonlinear optimization + data integration system,AuthorProvided Keywords Not Found,"The typical approach to data integration is to start by defining a common mediated schema, and then to map the data sources being integrated to this schema. In Internet-scale data integration tasks, where there may be hundreds or thousands of data sources providing data of relevance to a particular domain, a better approach is to allow the user to discover the mediated schema and the set of sources to use through an iterative exploration of the space of possible schemas and sources. In this paper, we present _BE, a data integration tool that helps in this iterative exploratory process by automatically choosing the data sources to include in a data integration system and defining a mediated schema on these sources. The data integration system desired by the user may depend on several subjective and objective criteria, and the user guides _BE towards finding this system by iteratively solving a series of constrained non-linear optimization problems, and modifying the parameters and constraints of the problem in the next iteration based on the solution found in the current iteration. Our formulation of the optimization problem is designed to make it easy for the user to provide such feedback. A simple, intuitive user interface helps the user in this process. We experimentally demonstrate that _BE is efficient and finds high-quality data integration solutions."
NE2780,International Conference on Data Engineering,Nuwee Wiwatwattana+H. V. Jagadish+Laks V. S. Lakshmanan+Divesh Srivastava,2007,X^ 3: A Cube Operator for XML OLAP,XML warehouse + relational warehouse + generalized specification + data mining + marked-up text documents + XML OLAP + relational databases + X^3 + cube lattice + cube operator + XML + data cube + data warehouses + relational cube,AuthorProvided Keywords Not Found,"With increasing amounts of data being exchanged and even generated or stored in XML, a natural question is how to perform OLAP on XML data, which can be structurally heterogeneous (e.g., parse trees) and/or marked-up text documents. A core operator for OLAP is the data cube. While the relational cube can be extended in a straightforward way to XML, we argue such an extension would not address the specific issues posed by XML. While in a relational warehouse, facts are flat records and dimensions may have hierarchies, in an XML warehouse, both facts and dimensions may be hierarchical. Second, XML is flexible: (a) an element may have missing or repeated subelements; (b) different instances of the same element type may have different structure. We identify the challenges introduced by these features of XML for cube definition and computation. We propose a definition for cube adapted for XML data warehouse, including a suitably generalized specification mechanism. We define a cube lattice over the aggregates so defined. We then identify properties of this cube lattice that can be leveraged to allow optimized computation of the cube. Finally, we present the results of an extensive performance evaluation experiment gauging the behavior of alternative algorithms for cube computation."
NE2685,International Conference on Data Engineering,Theoni Pitoura+Peter Triantafillou,2007,Load Distribution Fairness in P2P Data Management Systems,sampling methods + P2P data management systems + query load distribution fairness + peer-to-peer computing + resource allocation + peer-to-peer systems + distributed algorithms + statistical metrics + distributed sampling algorithm + Gini coefficient + software metrics,AuthorProvided Keywords Not Found,"We address the issue of measuring storage, or query load distribution fairness in peer-to-peer data management systems. Existing metrics may look promising from the point of view of specific peers, while in reality being far from optimal from a global perspective. Thus, first we define the requirements and study the appropriateness of various statistical metrics for measuring load distribution fairness towards these requirements. The metric proposed as most appropriate is the Gini coefficient (G). Second, we develop novel distributed sampling algorithms to compute G on-line, with high precision, efficiently, and scalably. Third, we show how G can readily be utilized on-line by higher-level algorithms which can now know when to best intervene to correct load imbalances. Our analysis and experiments testify for the efficiency and accuracy of these algorithms, permitting the online use of a rich and reliable metric, conveying a global perspective of the distribution."
NE2711,International Conference on Data Engineering,Bogdan Butnaru+Florin Dragan+Georges Gardarin+Joana Manolescu+Benjamin Nguyen+Radu pop+Nicoleta Preda+Laurent Yeh,2007,P2PTester: a tool for measuring P2P platform performance,software architecture + P2PTester + peer-to-peer computing + program testing + P2P architectures + program diagnostics + interaction tracing + P2P platform performance measurement + distributed system + software performance evaluation + interaction analysis + software metrics,AuthorProvided Keywords Not Found,"The current abundance and complexity of P2P architectures makes it extremely difficult to assess their performance. P2PTester is the first tool devised to interface with, and measure the performance of, existing P2P data management platforms. We isolate basic components present in current P2P platforms, and insert ""hooks"" for P2PTester to capture, analyze and trace the interactions taking place in the underlying distributed system."
NE2598,International Conference on Data Engineering,Lyublena Antova+Christoph Koch+Dan Olteanu,2007,10106Worlds and Beyond: Efficient Representation and Processing of Incomplete Information,query processing + efficient representation + relational algebra + world-set decomposition + incomplete information processing + space-efficient representation system + data structures + relational algebra queries + decomposition-based approach,AuthorProvided Keywords Not Found,"We present a decomposition-based approach to managing incomplete information. We introduce world-set decompositions (WSDs), a space-efficient and complete representation system for finite sets of worlds. We study the problem of efficiently evaluating relational algebra queries on world-sets represented by WSDs. We also evaluate our technique experimentally in a large census data scenario and show that it is both scalable and efficient."
7775BA9E,International Conference on Data Engineering,qinyi wu + anant sahai + calton pu + roger barga,2007,Categorization and Optimization of Synchronization Dependencies in Business Processes,web services + scattered code + data flow analysis + parallel processing + dataflow programming + authorization + scattering + business process + engines + control systems + scheduling + cost function + synchronization dependencies + nested structure + concurrent computing + programming + business data processing + sequencing constructs,AuthorProvided Keywords Not Found,"The current approach for modeling synchronization in business processes relies on sequencing constructs, such as sequence, parallel etc. However, sequencing constructs obfuscate the true source of dependencies in a business process. Moreover, because of the nested structure and scattered code that results from using sequencing constructs, it is hard to add or delete additional constraints without over-specifying necessary constraints or invalidating existing ones. We propose a dataflow programming approach in which dependencies are explicitly modeled to guide activity scheduling. We first give a systematic categorization of dependencies: data, control, service and cooperation. Each dimension models dependency from its own point of view. Then we show that dependencies of various kinds can be first merged and then optimized to generate a minimal dependency set, which guarantees high concurrency and minimal maintenance cost for process execution."
7E9A2560,International Conference on Data Engineering,matteo golfarelli + silvio rizzi,2007,X-Time: Schema Versioning and Cross-Version Querying in Data Warehouses,data warehouse + visualization + data analysis + business + prototypes + relational data warehouses + relational databases + multidimensional systems + X-Time + relational data + query processing + tin + cross-version querying + data warehouses + schema versioning,AuthorProvided Keywords Not Found,"In this demo we present X-Time, a prototype for managing schema versioning in relational data warehouses, specifically oriented to support the formulation of cross-version queries, i.e., queries whose temporal horizon spans multiple versions. The key issue to increase querying flexibility is the introduction of augmented schemata that properly extend previous schema versions."
810C76FA,International Conference on Data Engineering,wensheng xu + romain laborde + bassem nasser + sassa otenko + david w chadwick,2007,Multi-session Separation of Duties (MSoD) for RBAC,business process + authorisation + access control + rbac + information systems + management information systems + history + privilege management infrastructure + separation of duty + computer programming + xml + information system + information security + control systems + role based access control,,"Copyright & reuse Content in the Kent Academic Repository is made available for research purposes. Unless otherwise stated all content is protected by copyright and in the absence of an open licence (eg Creative Commons), permissions for further reuse of content should be sought from the publisher, author or other copyright holder. Versions of research The version in the Kent Academic Repository may differ from the final published version. Users are advised to check http://kar.kent.ac.uk for the status of the paper. Users should always cite the published version of record. Enquiries For any further enquiries regarding the licence status of this document, please contact: researchsupport@kent.ac.uk If you believe this document infringes copyright then please contact the KAR admin team with the take-down information provided at http://kar.kent.ac.uk/contact.html"
814D95EC,International Conference on Data Engineering,marco mesiti + ismael sanz + rafael berlanga + giovanna guerrini,2007,ArHeX: Flexible Composition of Indexes and Similarity Measures for XML,terminology + merging + xml + amino acids + indexation + software measurement + proteins + work in progress,,"This work-in-progress paper describes the features of the ArHeX similarity-oriented XML processing toolkit [12]. ArHeX is designed to assist in the engineering of XML similarity-oriented applications, supporting the design and evaluation of suitable similarity measures and their associated indexes for each specific application."
7F39D7C1,International Conference on Data Engineering,jerome simeon + philippe michiels + george a mihaila,2007,Put a Tree Pattern in Your Algebra,stress + rewriting systems + robustness + algebra + tree pattern algorithm + rewritings + query processing + tuple algebra + navigation + XML algebra + process algebra + xml + engines + data intensive XML application + tree pattern operator + XQuery compiler + pattern recognition,AuthorProvided Keywords Not Found,"To address the needs of data intensive XML applications, a number of efficient tree pattern algorithms have been proposed. Still, most XQuery compilers do not support those algorithms. This is due in part to the lack of support for tree patterns in XML algebras, but also because deciding which part of a query plan should be evaluated as a tree pattern is a hard problem. In this paper, we extend a tuple algebra for XQuery with a tree pattern operator, and present rewrit-ings suitable to introduce that operator in query plans. We demonstrate the robustness of the proposed rewritings under syntactic variations commonly found in queries. The proposed tree pattern operator can be implemented using popular algorithms such as Twig joins and Staircase joins. Our experiments yield useful information to decide which algorithm should be used in a given plan."
7F126922,International Conference on Data Engineering,raymond j spiteri + s wang + jane elizabeth bailey tougas + evangelos milios + roger zhang + mahdi shafiei + bin tang,2007,Document Representation and Dimension Reduction for Text Clustering,frequency + natural languages + indexing + computer science + independent component analysis + communications technology + k means clustering + natural language + data reduction + clustering algorithms + dimension reduction + data mining + text clustering + feature extraction + three dimensions + vector space model + text analysis + data structures + latent semantic indexing + text mining + feature selection,,"Increasingly large text datasets and the high dimensionality associated with natural language create a great challenge in text mining. In this research, a systematic study is conducted, in which three different document representation methods for text are used, together with three Dimension Reduction Techniques (DRT), in the context of the text clustering problem. Several standard benchmark datasets are used. The three Document representation methods considered are based on the vector space model, and they include word, multi-word term, and character N-gram representations. The dimension reduction methods are independent component analysis (ICA), latent semantic indexing (LSI), and a feature selection technique based on Document Frequency (DF). Results are compared in terms of clustering performance, using the k-means clustering algorithm. Experiments show that ICA and LSI are clearly better than DF on all datasets. For word and N-gram representation, ICA generally gives better results compared with LSI. Experiments also show that the word representation gives better clustering results compared to term and N-gram representation. Finally, for the N-gram representation, it is demonstrated that a profile length (before dimensionality reduction) of 2000 is sufficient to capture the information and, in most cases, a 4-gram representation gives better performance than 3-gram representation."
7F62C310,International Conference on Data Engineering,deepak agarwal + flip korn + divesh srivastava + dimitrios gunopulos + neal young + dhiman barman,2007,Parsimonious Explanations of Change in Hierarchical Data,data warehouse + data mining + decision support + OLAP tools + parsimonious explanation + navigation + hierarchical data + time measurement + point of sale + clothing + data warehouses + OLAP application + dimension attribute hierarchy,AuthorProvided Keywords Not Found,"Dimension attributes in data warehouses are typically hierarchical, and a variety of OLAP applications (such as point-of-sales analysis and decision support) call for summarizing the measure attributes in fact tables along the hierarchies of these attributes. For example, the total sales at different stores can be summarized hierarchically by geographic location (e.g., state/city/zip_code/store), by time (e.g., year/month/day/hour), or by product category (e.g., clothing/outerwear/jackets/brand). Existing OLAP tools help to summarize and navigate the data at different levels of aggregation (e.g., jackets sold in each state during December 2006) via drill-down and roll-up operators. OLAP tools are also used to characterize changes in these hierarchical summaries over time (e.g., the sales in December 2006 compared to sales in December 2005 over different locations) to detect anomalies and characterize trends. When the number of changes identified is large (e.g., the total sales at many locations differed significantly from their expectations), one seeks explanations. In this paper, we are interested in parsimonious explanations of changes in measure attributes aggregated along an associated dimension attribute hierarchy. We propose a natural model of explanation that makes effective use of the dimension hierarchy and describes changes at the leaf nodes of the hierarchy (e.g., individual stores in the location hierarchy) as a composition of ""node weights"" along each node's root-to-leaf path in the dimension hierarchy; each node weight constitutes an explanatory term. For example, sales in California stores were three times expected sales; sales in San Jose stores were higher by a factor of two (six times expected sales), whereas sales in Los Angeles stores were lower than the statewide increase by a factor of 1.5 (two times expected sales)."